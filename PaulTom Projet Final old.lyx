#LyX 2.2 created this file. For more info see http://www.lyx.org/
\lyxformat 508
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass extarticle
\begin_preamble
\usepackage{lastpage}

%\usepackage{tocbibind}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\minimise}{minimise}
\DeclareMathOperator*{\maximise}{maximise}
\DeclareMathOperator*{\sign}{sign}
%\usepackage{babel}

\usepackage[T1]{fontenc}
\usepackage{accanthis}
\usepackage{indentfirst}

\fancyhead{}
\fancyhead[LE,RO]{\textsl{\rightmark}}
\fancyhead[LO,RE]{\textsl{\leftmark}}
\fancyfoot{}
\fancyfoot[LE,RO]{Page \thepage\ of \pageref{LastPage}}
\fancyfoot[LO,RE]{\includegraphics[width=4cm]{NYU_logo.png}}
\renewcommand{\headrulewidth}{0.5pt}
\renewcommand{\footrulewidth}{0.5pt}

%\usepackage{fancyhdr}
%\usepackage{lastpage}
%\fancyhead{}
%\renewcommand{\headrulewidth}{0.4pt}
%\rhead{LINTILHAC AND LI}
%\lhead{CO-INTEGRATION KERNEL} 
%\fancyfoot{}
%\renewcommand{\footrulewidth}{0.4pt}
%\rfoot{\includegraphics[width=4cm]{NYU_logo.png}}
%\lfoot{Page \thepage\ of \pageref{LastPage}} 
%\renewcommand{\footrulewidth}{0.4pt}

%\usepackage{fancyhdr}
%\let\ps@plain\ps@fancy
%\fancyhf{}
%\renewcommand{\headrulewidth}{0pt}
%\rfoot{\includegraphics[width=4cm]{NYU_logo.png}}
%\fancyfoot[R]{Page \thepage\ of \pageref{LastPage}}
%\fancyfoot[L]{\includegraphics[width=4cm]{NYU_logo.png}}

\usepackage{color}
\definecolor{nyupurple}{RGB}{82,46,145}
\definecolor{matlabcomment}{rgb}{0.13,0.54,0.13}
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=nyupurple,
    filecolor=nyupurple,      
    urlcolor=nyupurple,
    citecolor=nyupurple,
    hyperfootnotes=true,
}

%\usepackage{chngcntr}
%\usepackage{footmisc}
%\counterwithout{footnote}{chapter}
%\renewcommand{\thefootnote}{\fnsymbol{footnote}}
%\footnote[num]{text}
%\renewcommand\thefootnote{\textcolor{red}{\arabic{footnote}}}
%\usepackage[dvipsnames]{xcolor}
%\usepackage{alltt}
%\definecolor{string}{rgb}{0.7,0.0,0.0}
%\definecolor{comment}{rgb}{0.13,0.54,0.13}
%\definecolor{keyword}{rgb}{0.0,0.0,1.0}

\usepackage[minbibnames=1, maxbibnames=99, backend=biber,style=authoryear,dashed=false,natbib=true,url=true,bibencoding=utf8]{biblatex}
% add bibliography database
\addbibresource{bibliography.bib}
%more options
\ExecuteBibliographyOptions{sorting=nty,backref=true,doi=true}
\DeclareFieldFormat[article]{volume}{\bibstring{jourvol}\addspace #1}
\DeclareFieldFormat[article]{number}{\bibstring{number}\addspace #1}
%\renewcommand*{\newunitpunct}{\addcomma\space}
\renewbibmacro*{volume+number+eid}{%
  \printfield{volume}%
  \setunit{\adddot\space}%<---- was \setunit*{\addcomma}%
  \printfield{number}%
  \setunit{\addspace}%
  \printfield{eid}}

\usepackage[section]{algorithm}
\usepackage{algorithmicx}
\usepackage{algpseudocode}
\renewcommand{\thealgorithm}{\arabic{section}.\arabic{algorithm}} 
\end_preamble
\use_default_options true
\begin_modules
eqs-within-sections
figs-within-sections
tabs-within-sections
shapepar
enumitem
theorems-bytype
theorems-sec-bytype
\end_modules
\maintain_unincluded_children false
\begin_local_layout
Format 60
Provides natbib 1
\end_local_layout
\language british
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command biber
\index_command default
\float_placement h
\paperfontsize 9
\spacing onehalf
\use_hyperref true
\pdf_bookmarks true
\pdf_bookmarksnumbered true
\pdf_bookmarksopen true
\pdf_bookmarksopenlevel 5
\pdf_breaklinks true
\pdf_pdfborder false
\pdf_colorlinks false
\pdf_backref false
\pdf_pdfusetitle true
\papersize a4paper
\use_geometry true
\use_package amsmath 2
\use_package amssymb 2
\use_package cancel 2
\use_package esint 2
\use_package mathdots 2
\use_package mathtools 2
\use_package mhchem 2
\use_package stackrel 2
\use_package stmaryrd 2
\use_package undertilde 2
\cite_engine natbib
\cite_engine_type authoryear
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 3cm
\topmargin 3cm
\rightmargin 3cm
\bottommargin 3cm
\footskip 1.25cm
\secnumdepth 5
\tocdepth 5
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 2
\paperpagestyle fancy
\listings_params "language=Matlab,basicstyle={\fontfamily{pcr}\selectfont\small},keywordstyle={\color{blue}},commentstyle={\color{matlabcomment}\itshape},emphstyle={\color{red}},stringstyle={\color{magenta}},identifierstyle={\color{black}},numbers=left,numberstyle={\scriptsize},breaklines=true"
\bullet 0 1 31 -1
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title

\series bold
FINAL PROJECT FOR THE COURSE ADVANCED FOUNDATIONS OF MACHINE LEARNING: ONLINE
 LEARNING OVER GRAPHS
\begin_inset Foot
status open

\begin_layout Plain Layout

\size small
\begin_inset CommandInset href
LatexCommand href
name "New York University"
target "http://www.nyu.edu"

\end_inset

 
\begin_inset CommandInset href
LatexCommand href
name "Courant Institute of Mathematical Sciences"
target "https://www.cs.nyu.edu/home/index.html"

\end_inset

.
 
\begin_inset CommandInset href
LatexCommand href
name "Spring 2017"
target "http://www.cs.nyu.edu/~mohri/aml17/"

\end_inset

.
 Professor 
\begin_inset CommandInset href
LatexCommand href
name "Mehryar Mohri"
target "http://www.cs.nyu.edu/~mohri/"

\end_inset

, Ph.D.
\end_layout

\end_inset


\end_layout

\begin_layout Author
PAUL SOPHER LINTILHAC
\begin_inset Foot
status open

\begin_layout Plain Layout
New York University 
\begin_inset CommandInset href
LatexCommand href
name "School of Engineering"
target "http://engineering.nyu.edu/academics/departments/mathematics"

\end_inset

.
 psl274@nyu.edu
\end_layout

\end_inset

 
\begin_inset Formula $\;\;$
\end_inset

AND 
\begin_inset Formula $\;$
\end_inset

THOMAS NANFENG L
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
v{I}
\end_layout

\end_inset


\begin_inset Foot
status open

\begin_layout Plain Layout
New York University 
\begin_inset CommandInset href
LatexCommand href
name "School of Engineering"
target "http://engineering.nyu.edu/academics/departments/mathematics"

\end_inset

.
 nl747@nyu.edu
\end_layout

\end_inset

 
\end_layout

\begin_layout Date
MAY 9
\begin_inset Formula $^{\text{TH}}$
\end_inset

, 2017
\end_layout

\begin_layout Section
Learning over graphs
\end_layout

\begin_layout Subsection
Concepts and Definitions
\end_layout

\begin_layout Standard
Graphs are mathematical structures that appear in many different applications
 of machine learning, and especially in the context of semi-supervised learning.
 In semi-supervised learning, we hope to exploit not only the information
 in the labeled examples, but also the information in the unlabeled examples
 by incorporating some notion of similarity between labeled and unlabeled
 examples.
 Even if our examples do not lie in a metric space, we can naturally define
 a notion of similarity between the examples by considering them as the
 vertices of a graph.
 One application of semi-supervised learning over graphs is in marketing
 and ad-tech, where we may model the flow of web traffic as a Markov Random
 Field.
 In this case, certain sites are known to be of interest to a consumer,
 while other sites may be of interest based on their proximity to sies previousl
y visited on the web.
 Another application might be to identify suspiscious accounts on a transactiona
l network such as the block chain.
 In this paper we detail several different graphical models, including basic
 diffusion models, label propagation, regression, semi-supervised learning,
 and finally online learning.
 First, we introduce some definitions.
\end_layout

\begin_layout Standard
Let 
\begin_inset Formula $G\left(V,\,E\right)$
\end_inset

 be a graph with 
\begin_inset Formula $V(G)=\left\{ v_{1},\,v_{2},\,\cdots,\,v_{n}\right\} $
\end_inset

 and 
\begin_inset Formula $E(G)=\left\{ e_{1},\,e_{2},\,\cdots,\,e_{m}\right\} $
\end_inset

.
 The 
\begin_inset Formula $\boldsymbol{adjacency}$
\end_inset

 
\begin_inset Formula $\boldsymbol{matrix}$
\end_inset

 of 
\begin_inset Formula $G\left(V,\,E\right)$
\end_inset

, denoted by 
\begin_inset Formula $\boldsymbol{A}(G)$
\end_inset

, is the 
\begin_inset Formula $n\times n$
\end_inset

 matrix defined as follows.
 The rows and the columns of 
\begin_inset Formula $\boldsymbol{A}(G)$
\end_inset

 are indexed by 
\begin_inset Formula $V(G)$
\end_inset

.
 If 
\begin_inset Formula $i\neq j$
\end_inset

 then the 
\begin_inset Formula $\left(i,\,j\right)$
\end_inset

 entry of 
\begin_inset Formula $\boldsymbol{A}(G)$
\end_inset

 is 
\begin_inset Formula $0$
\end_inset

 for vertices 
\begin_inset Formula $i$
\end_inset

 and 
\begin_inset Formula $j$
\end_inset

 non-adjacent, and the 
\begin_inset Formula $\left(i,\,j\right)$
\end_inset

 entry is 1 for 
\begin_inset Formula $i$
\end_inset

 and 
\begin_inset Formula $j$
\end_inset

 adjacent.
 The 
\begin_inset Formula $\left(i,\,j\right)$
\end_inset

 entry of 
\begin_inset Formula $\boldsymbol{A}(G)$
\end_inset

 is 
\begin_inset Formula $0$
\end_inset

 for 
\begin_inset Formula $i=j=1,\,\cdots\,,n$
\end_inset

.
 The 
\begin_inset Formula $\boldsymbol{degree}$
\end_inset

 
\begin_inset Formula $\boldsymbol{matrix}$
\end_inset

 
\begin_inset Formula $\boldsymbol{D}(G)$
\end_inset

 for 
\begin_inset Formula $G\left(V,\,E\right)$
\end_inset

 is a 
\begin_inset Formula $n\times n$
\end_inset

 diagonal matrix defined as
\begin_inset Formula 
\[
\begin{aligned}\boldsymbol{D}\left(G\right)_{i,j} & \coloneqq\begin{cases}
d\left(v_{i}\right) & \text{if }i=j\\
0 & \text{otherwise}.
\end{cases}\end{aligned}
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula $\boldsymbol{Weighted}$
\end_inset

 
\begin_inset Formula $\boldsymbol{graph}$
\end_inset

 
\begin_inset Formula $G\left(V,\,E,\,W\right)$
\end_inset

 is a graph with real edge weights given by 
\begin_inset Formula $w\,:\,E\rightarrow\mathbb{R}$
\end_inset

.
 Here, the weight 
\begin_inset Formula $w\left(e\right)$
\end_inset

 of an edge 
\emph on

\begin_inset Formula $e$
\end_inset


\emph default
 indicates the similarity of the incident vertices, and a missing edge correspon
ds to zero similarity.
 The 
\begin_inset Formula $\boldsymbol{weighted}$
\end_inset

 
\begin_inset Formula $\boldsymbol{adjacency}$
\end_inset

 
\begin_inset Formula $\boldsymbol{matrix}$
\end_inset

 
\begin_inset Formula $\boldsymbol{W}\left(G\right)$
\end_inset

 of the graph 
\begin_inset Formula $G\left(V,\,E,\,W\right)$
\end_inset

 is defined by
\begin_inset Formula 
\begin{equation}
\begin{aligned}\boldsymbol{W}_{ij} & \coloneqq\begin{cases}
w\left(e\right) & \text{if }e=\left(i,\,j\right)\in E\\
0 & \text{otherwise}.
\end{cases}\end{aligned}
.
\end{equation}

\end_inset

The weight matrix 
\begin_inset Formula $\boldsymbol{W}\left(G\right)$
\end_inset

 can be, for instance, the k-nearest neighbour matrix 
\begin_inset Formula $\boldsymbol{W}\left(G\right)_{ij}=1$
\end_inset

 if and only if vertex 
\begin_inset Formula $v_{i}$
\end_inset

 is among the 
\begin_inset Formula $k$
\end_inset

-nearest neighbours of 
\begin_inset Formula $v_{j}$
\end_inset

 or vice versa, and is 0 otherwise.
 Another typical weight matrix is given by the Gaussian kernel of width
 
\begin_inset Formula $\sigma$
\end_inset


\begin_inset Formula 
\begin{equation}
\boldsymbol{W}\left(G\right)_{ij}=e^{-\frac{\left\Vert v_{i}-v_{j}\right\Vert ^{2}}{2\sigma^{2}}}.\label{eq:1.5}
\end{equation}

\end_inset

 Then the 
\begin_inset Formula $\boldsymbol{degree}$
\end_inset

 
\begin_inset Formula $\boldsymbol{matrix}$
\end_inset

 
\begin_inset Formula $\boldsymbol{D}\left(G\right)$
\end_inset

 for weighted graph 
\begin_inset Formula $G\left(V,\,E,\,W\right)$
\end_inset

 is defined by 
\begin_inset Formula 
\begin{equation}
\boldsymbol{D}\left(G\right)_{i,i}\coloneqq\sum_{j}\boldsymbol{W}\left(G\right)_{ij}
\end{equation}

\end_inset


\end_layout

\begin_layout Subsection
Graph Laplacian and the Heat Equation
\end_layout

\begin_layout Standard
The graph Laplacian 
\begin_inset Formula $\boldsymbol{L}\left(G\right)$
\end_inset

 is defined in two different ways.
 The 
\begin_inset Formula $\boldsymbol{normalized}$
\end_inset

 
\begin_inset Formula $\boldsymbol{graph}$
\end_inset

 
\begin_inset Formula $\boldsymbol{Laplacian}$
\end_inset

 is 
\begin_inset Formula 
\begin{equation}
\boldsymbol{L}\left(G\right)\coloneqq\boldsymbol{I}-\boldsymbol{D}^{-\frac{1}{2}}\boldsymbol{W}\boldsymbol{D}^{-\frac{1}{2}},
\end{equation}

\end_inset

and the 
\begin_inset Formula $\boldsymbol{unnormalized}$
\end_inset

 
\begin_inset Formula $\boldsymbol{graph}$
\end_inset

 
\begin_inset Formula $\boldsymbol{Laplacian}$
\end_inset

 is 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\boldsymbol{L}\left(G\right)\coloneqq\boldsymbol{D}-\boldsymbol{W}.
\end{equation}

\end_inset

 The Laplacian matrix 
\begin_inset Formula $\boldsymbol{L}$
\end_inset

 is symmetric and positive semi-definite, and it has 
\begin_inset Formula $n$
\end_inset

 non-negative, real-valued eigenvalues 
\begin_inset Formula $0=\lambda_{1}\leq\lambda_{2}\leq\cdots\leq\lambda_{n}$
\end_inset

.
 The number of 0 eigenvalues of the Laplacian matrix 
\begin_inset Formula $\boldsymbol{L}$
\end_inset

 is the number of connected components.
\end_layout

\begin_layout Standard
We can get more intuition about 
\begin_inset Formula $L$
\end_inset

 by using the concept of conductance and borrowing intuition from the continous
 version of this operator, for example in the heat equation.
 Let 
\begin_inset Formula $\boldsymbol{f}$
\end_inset

 be some real-valued function defined over the vertices of the graph.
 If we think of 
\begin_inset Formula $\boldsymbol{f}$
\end_inset

 as a heat distribution, then we can think of 
\begin_inset Formula $\boldsymbol{L}\boldsymbol{f}$
\end_inset

 roughly as the flux induced at each of the vertices by that distribution
 over the graph, based on the graph structure.
 Then the form 
\begin_inset Formula $\boldsymbol{g}^{T}\boldsymbol{L}\boldsymbol{f}$
\end_inset

 represents some weighted measurement of this flux with the weights given
 by 
\begin_inset Formula $\boldsymbol{g}$
\end_inset

.
 For example, if f is a binary vector representing the boundary of some
 open set on the graph, then 
\begin_inset Formula $\left\langle \boldsymbol{f},\,\boldsymbol{g}\right\rangle \coloneqq\boldsymbol{f}^{T}\boldsymbol{L}\boldsymbol{g}$
\end_inset

 would be the flux produced by 
\begin_inset Formula $\boldsymbol{g}$
\end_inset

 as measured on the boundary represented by 
\begin_inset Formula $\boldsymbol{f}$
\end_inset

.
 In particular, the quadratic form 
\begin_inset Formula $\boldsymbol{f}^{T}\boldsymbol{L}\boldsymbol{f}$
\end_inset

 can be thought of as a standard metric for the 
\begin_inset Quotes eld
\end_inset

total flux
\begin_inset Quotes erd
\end_inset

 induced over the graph by some initial heat distribution 
\begin_inset Formula $\boldsymbol{f}$
\end_inset

.
 In other words, 
\begin_inset Formula $\boldsymbol{f}^{T}\boldsymbol{L}\boldsymbol{f}$
\end_inset

 is a measure of the smoothness of the function 
\begin_inset Formula $\boldsymbol{f}$
\end_inset

 relative to the structure of the graph defined by 
\begin_inset Formula $\boldsymbol{L}.$
\end_inset


\end_layout

\begin_layout Subsection
Label Propagation
\end_layout

\begin_layout Standard
Given the graph 
\begin_inset Formula $G\left(V,\,E,\,W\right)$
\end_inset

, and an initial state of some real-valued function 
\begin_inset Formula $\boldsymbol{f}\,:\,V\rightarrow\mathbb{R}$
\end_inset

 defined over the vertices of the graph, we may be interested in propagating
 this function over the graph at time steps 
\begin_inset Formula $t=0,\,1,\,2,\,\cdots,\,T$
\end_inset

 that is consistent with the graph structure.
 In the simplest possible case, we might be provided with the initial state
 
\begin_inset Formula $\boldsymbol{f}_{0}$
\end_inset

 and wish to see how these values propagate over the graph, as in a heat
 diffusion.
 While very simple, this scenario could be useful in settings such as fraud
 detection over a transactional network of accounts that are modeled as
 a graph.
 For example, we may have received knowledge that a small number of vertices
 in the graph are fraudulent, and we want to perform a quick analysis to
 see to what degree other vertices in the graph might be exposed to the
 risk emanating from these fraudulent accounts.
 
\end_layout

\begin_layout Standard
Rather than taking the discrete version of the diffusion equation as given,
 we frame the equation as the solution to an optimization problem over the
 graph.
 We proceed by deriving an update equation starting with two basic assumptions:
 First, we want our function 
\begin_inset Formula $\boldsymbol{f}_{t+1}$
\end_inset

 at each time step to be 
\begin_inset Quotes eld
\end_inset

close
\begin_inset Quotes erd
\end_inset

 to 
\begin_inset Formula $\boldsymbol{f}_{t}$
\end_inset

.
 Second, we want to impose the condition that 
\begin_inset Formula $\boldsymbol{f}_{t+1}$
\end_inset

 be a 
\begin_inset Quotes eld
\end_inset

smooth
\begin_inset Quotes erd
\end_inset

 function over the graph, in the sense of minimal total flux described above.
 Thus we seek to minimize
\begin_inset Formula 
\begin{equation}
C\left(\boldsymbol{f}_{t+1}\Bigl|\boldsymbol{f}_{t}\right)=\left(\boldsymbol{f}_{t+1}-\boldsymbol{f}_{t}\right){}^{T}\left(\boldsymbol{f}_{t+1}-\boldsymbol{f}_{t}\right)+\alpha\boldsymbol{f}_{t+1}^{T}\boldsymbol{L}\boldsymbol{f}_{t+1},
\end{equation}

\end_inset

where 
\begin_inset Formula $\boldsymbol{L}$
\end_inset

 is the non-normalized Laplacian, and 
\begin_inset Formula $\alpha$
\end_inset

 is a constant measuring the conductivity of the graph.
 Taking the derivative and setting it to zero yields
\begin_inset Formula 
\begin{equation}
\boldsymbol{f}_{t+1}=\boldsymbol{f}_{t}-\alpha\boldsymbol{L}\boldsymbol{f}_{t}.
\end{equation}

\end_inset

As an example, if we were to begin with the vector of initial values 
\begin_inset Formula $\hat{\boldsymbol{Y}}^{0}=e_{1}$
\end_inset

 representing a unit point density, the above recurrence relation would
 show how that density propagates across the graph.
 We could alternatively consider the update equation to be a discretization
 of a system evolving in continuous-time.
 Note that the above equation can be viewed as a discrete difference equation
\begin_inset Formula 
\begin{equation}
\boldsymbol{f}_{t+1}-\boldsymbol{f}_{t}=\delta\boldsymbol{f}_{t}=-\alpha\boldsymbol{L}\boldsymbol{f}_{t}.
\end{equation}

\end_inset

Taking the continuous limit of this diffusion yields
\begin_inset Formula 
\begin{equation}
\frac{d\boldsymbol{f}_{t}}{dt}=-\alpha\boldsymbol{L}\boldsymbol{f}_{t},\qquad\boldsymbol{f}_{t}(0)=\boldsymbol{f}_{0}.
\end{equation}

\end_inset

The solution to this ODE is given by
\begin_inset Formula 
\begin{equation}
\boldsymbol{f}_{t}=e^{-\alpha\boldsymbol{L}t}\boldsymbol{f}_{0}.
\end{equation}

\end_inset

We can now use this continuous system in our discretization in order to
 derive a discrete update formula
\begin_inset Formula 
\begin{equation}
\boldsymbol{f}_{t}=e^{-\alpha\boldsymbol{L}t}\boldsymbol{f}_{0}.
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
This system differs from the one described by the original difference equation
 in several ways.
 First, there is a closed-form solution for the value of each node at any
 point in time, rather than a recurrence relation, which should make the
 time-complexity much more efficient.
 But it turns out that what is gained in time efficiency is more than lost
 inamuch higher space complexity, and thus is not practical in applications
 with very large graphs.
 Consider the case where our graph contains 
\begin_inset Formula $100000$
\end_inset

 vertices, and all vertices are connected by a single path, with the degree
 of each node exactly equal to 
\begin_inset Formula $2$
\end_inset

.
 In this case the regular Laplacian matrix is extremely sparse, as only
 
\begin_inset Formula $0.00001$
\end_inset

 of the entries are non-zero.
 Using data structures designed specifically for extremely sparse matrices,
 storing this matrix and performing basic computations would be quite easy.
 On the other hand, the matrix 
\begin_inset Formula $e^{-\alpha\boldsymbol{L}t}$
\end_inset

 has strictly positive entries for all t, and therefore would require 
\begin_inset Formula $100000$
\end_inset

 times as much memory to store.
\end_layout

\begin_layout Standard
The underlying reason for this discrepancy is that whereas the continuous
 Laplacian establishes a relationship among all vertices within an entire
 connected component of the graph, the discrete Laplacian 
\begin_inset Formula $\boldsymbol{L}\left(G\right)$
\end_inset

 only propagates to immediate neighbors, as any vertices 
\begin_inset Formula $v_{i}$
\end_inset

 and 
\begin_inset Formula $v_{j}$
\end_inset

, which are not connected by an edge will receive no flux contribution from
 each other.
 To gain more intuition about this discrepancy, we recall that the matrix
 exponential is defined by its Taylor series
\begin_inset Formula 
\begin{equation}
e^{-\alpha\boldsymbol{L}t}=\sum_{\{i=0\}}^{\infty}\frac{\left(-\alpha\boldsymbol{L}t\right)^{i}}{i!}.
\end{equation}

\end_inset

Replacing our initial condition with 
\begin_inset Formula $\boldsymbol{f}_{t}$
\end_inset

 and final state with 
\begin_inset Formula $\boldsymbol{f}_{t+1}$
\end_inset

 and taking the first order approximation to this series gives
\begin_inset Formula 
\begin{equation}
e^{-\alpha\boldsymbol{L}t}\approx\boldsymbol{1}-\boldsymbol{L}.
\end{equation}

\end_inset

Plugging this approximation into our update formula for the continuous case
 yields
\begin_inset Formula 
\begin{equation}
\boldsymbol{f}_{t+1}=e^{-\alpha\boldsymbol{L}}\boldsymbol{f}_{t}\approx(\boldsymbol{1}-\boldsymbol{L})\boldsymbol{f}_{t}=\boldsymbol{f}_{t}-\alpha\boldsymbol{L}\boldsymbol{f}_{t},
\end{equation}

\end_inset

which matches exactly our formula in the discrete case.
 If we were to take the second order approximation, each step in our iteration
 would take into account not only immediate neighbors, but also neighbors
 of neighbors and so on as more terms from the Taylor series are kept.
 This in turn would reduce the total number of iterations needed to simulate
 the evolution of the system, so that we have achieved some of the time-efficien
cy of the continuous model.
 However, we need to pay for this more favourable time-complexity by computing
 and storing the matrix 
\begin_inset Formula $\boldsymbol{L}^{2}$
\end_inset

.
 Even if 
\begin_inset Formula $\boldsymbol{L}$
\end_inset

 is sparse, 
\begin_inset Formula $\boldsymbol{L}^{2}$
\end_inset

will have exponentially more non-zero entries, and will thus be much larger,
 and will also take much longer to compute.
 Therefore, if we want to optimize label propagation in practice, we may
 want to take as many term of the Taylor expansion as we can without exceeding
 our memory.
\end_layout

\begin_layout Subsection
Sustained Effects of Initial State
\end_layout

\begin_layout Standard
Now suppose that we wish to maintain the effects of our initial function
 over time.
 Then we may add a term to the objective function of the form 
\begin_inset Formula $\boldsymbol{f}_{t+1}^{T}\boldsymbol{f}_{0}.$
\end_inset

 This inner product measures the similarity between the updated function
 
\begin_inset Formula $\boldsymbol{f}_{t+1}$
\end_inset

 and the initial state vector 
\begin_inset Formula $\boldsymbol{f}_{0}.$
\end_inset

 For this problem, we will also replace the non-normalized Laplacian 
\begin_inset Formula $L$
\end_inset

 with the normalized Laplacian 
\begin_inset Formula $\hat{\boldsymbol{L}}$
\end_inset

, which will become useful when deriving the steady state solution below.
 Thus our objective function now has the form
\begin_inset Formula 
\begin{equation}
C\left(\boldsymbol{f}_{t+1}\Bigl|\boldsymbol{f}_{t}\right)=\left(\boldsymbol{f}_{t+1}-\boldsymbol{f}_{t}\right)^{T}\left(\boldsymbol{f}_{t+1}-\boldsymbol{f}_{t}\right)+\boldsymbol{f}_{t+1}^{T}\left(\boldsymbol{I}-\alpha\hat{\boldsymbol{L}}\right)\boldsymbol{f}_{t+1}-2\beta\boldsymbol{f}_{t+1}^{T}\boldsymbol{f}_{0}.
\end{equation}

\end_inset

Straightforward calculus and linear algebra give the solution
\begin_inset Formula 
\begin{equation}
\boldsymbol{f}_{t+1}=\alpha\hat{\boldsymbol{L}}\boldsymbol{f}_{t}+\beta\boldsymbol{f}_{0}.
\end{equation}

\end_inset

The update equation derived hence agrees with that of chapter 11 of 
\begin_inset CommandInset citation
LatexCommand citet
key "BengioDelalleauRoux2006"

\end_inset

.
 Following 
\begin_inset CommandInset citation
LatexCommand citet
key "BengioDelalleauRoux2006"

\end_inset

, this recurrence relation can equivalently write the entire recurrence
 relation in terms of 
\begin_inset Formula $\hat{\boldsymbol{Y}}^{0}$
\end_inset


\begin_inset Formula 
\begin{equation}
\boldsymbol{f}_{t+1}=\left(\alpha\hat{\boldsymbol{L}}\right)^{t}\boldsymbol{f}_{0}=\beta\sum_{i=0}^{t}\left(\alpha\hat{\boldsymbol{L}}\right)^{i}\boldsymbol{f}_{0}.
\end{equation}

\end_inset

Note that in order to write this equation in absolute form, reducing the
 number of computations, we need to pay for this by having to store powers
 of the matrix 
\begin_inset Formula $\boldsymbol{L}$
\end_inset

.
 It can be easily shown that the normalized Laplacian has an operator norm
 less than unity, 
\begin_inset Formula $\left\Vert \hat{\boldsymbol{L}}\right\Vert <1$
\end_inset

.
 Therefore, the power series 
\begin_inset Formula $\sum_{i=0}^{t}\left(\alpha\hat{\boldsymbol{L}}\right)^{i}$
\end_inset

 converges, and is equal to 
\begin_inset Formula $\left(\boldsymbol{I}-\alpha\hat{\boldsymbol{L}}\right)^{-1}$
\end_inset

.
 In addition, this implies that 
\begin_inset Formula $\lim_{t\to\infty}\left(\alpha\hat{\boldsymbol{L}}\right)^{t}=0$
\end_inset

.
 Thus 
\begin_inset Formula 
\begin{equation}
\boldsymbol{f}_{\infty}=\beta\left(\boldsymbol{I}-\alpha\hat{\boldsymbol{L}}\right)^{-1}.\boldsymbol{f}_{0}.
\end{equation}

\end_inset

This matches the update formula for the 
\begin_inset Quotes eld
\end_inset

label spreading
\begin_inset Quotes erd
\end_inset

 algorithm in (Begnio 2006), though we derive the strategy here starting
 with an objective function for the sake of consistency with other algorithms
 discussed herein.
 We will see in the sequel that the inverse or generalized inverse of the
 Laplacian and closely related matrices play a key role for online learning
 over graphs.
 The above is a linear system of equations which can be solved iteratively,
 as explained in section 2.
\end_layout

\begin_layout Subsection
Regression on Graphs
\end_layout

\begin_layout Standard
Now that we have introduced the basic building blocks for graphical learning,
 we can begin to analyse graphical learning algorithms that are used in
 practice.
 Like in the previous sections, we begin by defining some cost function
 for our function 
\begin_inset Formula $f$
\end_inset

, and then we proceed to derive an update equation that minimizes that cost
 function.
 The sequel deals with algorithms that are actually used by machine learning
 practitioners.
 In this setting, we are given some set of labels some set of labels 
\begin_inset Formula $Y$
\end_inset

 defined over the 
\begin_inset Formula $V$
\end_inset

, and we are asked to make predictions of those labels, 
\begin_inset Formula $\hat{\boldsymbol{Y}}$
\end_inset

.
 
\end_layout

\begin_layout Standard
Given a labelling 
\begin_inset Formula $\boldsymbol{Y}$
\end_inset

, we wish to find a set of predicted labels 
\begin_inset Formula $\hat{\boldsymbol{Y}}$
\end_inset

 that closely approximate the true labels, with some additional constraints
 that are familiar from the previous section.
 consistency with the initial labelling can be measured by
\begin_inset Formula 
\begin{equation}
\sum_{i=1}^{l}\left(\hat{y}_{i}-y_{i}\right)^{2}=\left\Vert \hat{\boldsymbol{Y}}_{l}-\boldsymbol{Y}_{l}\right\Vert ^{2}.\label{eq:1.37}
\end{equation}

\end_inset

By following the smoothness assumption, if two points 
\begin_inset Formula $x_{1}$
\end_inset

 and 
\begin_inset Formula $x_{2}$
\end_inset

 in a high-density region are close, then so should be the corresponding
 outputs 
\begin_inset Formula $y_{1}$
\end_inset

 and 
\begin_inset Formula $y_{2}$
\end_inset

, we consider a penalty term of the form
\begin_inset Formula 
\begin{equation}
\frac{1}{2}\sum_{i,j=1}^{n}\boldsymbol{W}_{ij}\left(\hat{y}_{i}-\hat{y}_{j}\right)^{2}.\label{eq:1.38}
\end{equation}

\end_inset

This means we penalize rapid changes in 
\begin_inset Formula $\hat{\boldsymbol{Y}}$
\end_inset

 between points that are close, which is given by the similarity matrix
 
\begin_inset Formula $\boldsymbol{W}$
\end_inset

.
 However, if there is noise in the available labels, it may be beneficial
 to allow the algorithm to relabel the labelled data.
 This could also help generalization in a noise-free setting where, for
 instance, a positive sample had been drawn from a region of space mainly
 filled with negative samples.
 
\end_layout

\begin_layout Standard
Based on this observation, 
\begin_inset CommandInset citation
LatexCommand citet
key "BelkinMatveevaNiyogi2004"

\end_inset

 proposed a more general cost criterion involving a trade-off between formula
 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:1.37"

\end_inset

 and formula 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:1.38"

\end_inset

, which is shown in algorithm 
\begin_inset CommandInset ref
LatexCommand ref
reference "alg:1.1"

\end_inset

 and algorithm 
\begin_inset CommandInset ref
LatexCommand ref
reference "alg:1.2"

\end_inset

.
 The paper assumed that 
\begin_inset Formula $G\left(V,\,E,\,W\right)$
\end_inset

 is connected and that the vertices of the graph are numbered, and only
 partial information, 
\begin_inset Formula $f\left(\boldsymbol{x}_{i}\right)=y_{i},\:1\leq i\leq k$
\end_inset

, is given.
 The labels can potentially be noisy.
 They also allow data points to have multiplicities, i.e.
 each vertex of the graph may appear more than once with same or different
 
\begin_inset Formula $y$
\end_inset

 value.
 They precondition the data by mean subtracting first.
 That is we take
\begin_inset Formula 
\begin{equation}
\tilde{\boldsymbol{y}}=\left(y_{1}-\bar{y},\,y_{2}-\bar{y},\,\cdots,\,y_{k}-\bar{y}\right),
\end{equation}

\end_inset

where
\begin_inset Formula $\bar{y}=\frac{1}{k}\sum y_{i}$
\end_inset

.
 
\end_layout

\begin_layout Standard
In the algorithm 
\begin_inset CommandInset ref
LatexCommand ref
reference "alg:1.1"

\end_inset

, 
\begin_inset Formula $\boldsymbol{S}$
\end_inset

 is a smoothness matrix, e.g.
 
\begin_inset Formula $S=L$
\end_inset

 or 
\begin_inset Formula $S=L^{p}$
\end_inset

, 
\begin_inset Formula $p\in\mathbb{N}$
\end_inset

.
 The condition 
\begin_inset Formula $\sum f_{i}=0$
\end_inset

 is needed to make the algorithm stable.
 The solution to the quadratic problem above is not hard to obtain by standard
 linear algebra considerations.
 We have the objective function 
\begin_inset Formula $\frac{1}{k}\sum_{i}\left(f_{i}-\tilde{y}_{i}\right)^{2}+\gamma\boldsymbol{f}^{T}\boldsymbol{S}\boldsymbol{f}$
\end_inset

 and constraint 
\begin_inset Formula $\sum f_{i}-0$
\end_inset

.
 Therefore, the Lagrangian is 
\begin_inset Formula 
\begin{equation}
\begin{aligned}\mathcal{L} & =\frac{1}{k}\sum_{i}\left(f_{i}-\tilde{y}_{i}\right)^{2}+\gamma\boldsymbol{f}^{T}\boldsymbol{S}\boldsymbol{f}-\mu\left(\sum f_{i}-0\right)\\
 & =\frac{1}{k}\boldsymbol{f}^{T}\boldsymbol{f}-\frac{2}{k}\boldsymbol{f}^{T}\tilde{\boldsymbol{y}}+\frac{1}{k}\tilde{\boldsymbol{y}}^{T}\tilde{\boldsymbol{y}}+\gamma\boldsymbol{f}^{T}\boldsymbol{S}\boldsymbol{f}-\mu\boldsymbol{f}^{T}\boldsymbol{1}\\
 & =\boldsymbol{f}^{T}\left(\frac{1}{k}\boldsymbol{I}+\gamma\boldsymbol{S}\right)\boldsymbol{f}-\frac{2}{k}\boldsymbol{f}^{T}\left(\tilde{\boldsymbol{y}}+\mu\boldsymbol{1}\right)+\frac{1}{k}\tilde{\boldsymbol{y}}^{T}\tilde{\boldsymbol{y}}.
\end{aligned}
\end{equation}

\end_inset

Then by taking 
\begin_inset Formula $\frac{\partial\mathcal{L}}{\partial\boldsymbol{f}}=0$
\end_inset

, we have 
\begin_inset Formula 
\begin{equation}
\tilde{\boldsymbol{f}}=\left(\boldsymbol{I}+k\gamma\boldsymbol{S}\right)^{-1}\left(\tilde{\boldsymbol{y}}+\mu\boldsymbol{1}\right).
\end{equation}

\end_inset

Here, 
\begin_inset Formula $\mu$
\end_inset

 is chosen so that the resulting vector 
\begin_inset Formula $\boldsymbol{f}$
\end_inset

 is orthogonal to 
\begin_inset Formula $\boldsymbol{1}$
\end_inset

.
 Denote by 
\begin_inset Formula $s\left(\boldsymbol{f}\right)$
\end_inset

 the functional
\begin_inset Formula 
\begin{equation}
s\,:\,\boldsymbol{f}\rightarrow\sum_{i}f_{i}.
\end{equation}

\end_inset

Since 
\begin_inset Formula $s$
\end_inset

 is linear, we obtain
\begin_inset Formula 
\begin{equation}
0=s\left(\tilde{\boldsymbol{f}}\right)=s\left(\left(\boldsymbol{I}+k\gamma\boldsymbol{S}\right)^{-1}\tilde{\boldsymbol{y}}\right)+s\left(\left(\boldsymbol{I}+k\gamma\boldsymbol{S}\right)^{-1}\boldsymbol{1}\right).
\end{equation}

\end_inset

Therefore we can write
\begin_inset Formula 
\begin{equation}
\mu=-\frac{s\left(\left(\boldsymbol{I}+k\gamma\boldsymbol{S}\right)^{-1}\tilde{\boldsymbol{y}}\right)}{s\left(\left(\boldsymbol{I}+k\gamma\boldsymbol{S}\right)^{-1}\boldsymbol{1}\right)}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float algorithm
wide false
sideways false
status open

\begin_layout Plain Layout
The objective is to minimize the square loss function plus the smoothness
 penalty.
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\tilde{\boldsymbol{f}}=\argmin{}_{\substack{\boldsymbol{f}=\left(f_{1},\,\cdots,\,f_{n}\right)\\
\sum f_{i}=0
}
}\frac{1}{k}\sum_{i}\left(f_{i}-\tilde{y}_{i}\right)^{2}+\gamma\boldsymbol{f}^{T}\boldsymbol{S}\boldsymbol{f}$
\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Tickhonov Regularization with Parameter 
\begin_inset Formula $\gamma\in\mathbb{R}$
\end_inset


\begin_inset CommandInset label
LatexCommand label
name "alg:1.1"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset

 
\end_layout

\begin_layout Standard
In the algorithm 
\begin_inset CommandInset ref
LatexCommand ref
reference "alg:1.2"

\end_inset

, 
\begin_inset Formula $\boldsymbol{S}$
\end_inset

 is a smoothness matrix, e.g.
 
\begin_inset Formula $S=L$
\end_inset

 or 
\begin_inset Formula $S=L^{p}$
\end_inset

, 
\begin_inset Formula $p\in\mathbb{N}$
\end_inset

.
 However, here we are not allowing multiple vertices in the sample.
 We partition 
\begin_inset Formula $\boldsymbol{S}$
\end_inset

 as 
\begin_inset Formula 
\begin{equation}
\begin{aligned}\boldsymbol{S} & =\left[\begin{array}{cc}
\boldsymbol{S}_{1} & \boldsymbol{S}_{2}\\
\boldsymbol{S}_{2}^{T} & \boldsymbol{S}_{3}
\end{array}\right]\end{aligned}
,
\end{equation}

\end_inset

where 
\begin_inset Formula $\boldsymbol{S}_{1}$
\end_inset

 is a 
\begin_inset Formula $k\times k$
\end_inset

 matrix, 
\begin_inset Formula $\boldsymbol{S}_{2}$
\end_inset

 is a 
\begin_inset Formula $k\times\left(n-k\right)$
\end_inset

 matrix, and 
\begin_inset Formula $\boldsymbol{S}_{3}$
\end_inset

 is a 
\begin_inset Formula $\left(n-k\right)\times\left(n-k\right)$
\end_inset

 matrix.
 Let 
\begin_inset Formula $\tilde{\boldsymbol{f}}$
\end_inset

 be the values of 
\begin_inset Formula $\boldsymbol{f}$
\end_inset

, where the function is unknown, 
\begin_inset Formula $\tilde{\boldsymbol{f}}=\left(f_{k+1},\,\cdots,\,f_{n}\right)$
\end_inset

.
 By applying the similar Lagrangian multiplier method, we have 
\begin_inset Formula 
\begin{equation}
\tilde{\boldsymbol{f}}=\boldsymbol{S}_{3}^{-1}\boldsymbol{S}_{2}^{T}\left(\left(\tilde{y}_{1},\,\cdots,\,\tilde{y}_{k}\right)^{T}+\mu\boldsymbol{1}\right),
\end{equation}

\end_inset

and 
\begin_inset Formula 
\begin{equation}
\mu=-\frac{s\left(\boldsymbol{S}_{3}^{-1}\boldsymbol{S}_{2}^{T}\tilde{\boldsymbol{y}}\right)}{s\left(\boldsymbol{S}_{3}^{-1}\boldsymbol{S}_{2}^{T}\boldsymbol{1}\right)}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float algorithm
wide false
sideways false
status open

\begin_layout Plain Layout
Here we assume that the values 
\begin_inset Formula $y_{1},\,y_{2},\,\cdots,\,y_{k}$
\end_inset

 have no noise.
\end_layout

\begin_layout Plain Layout
Thus the optimization problem is to find a function of maximum smoothness
 satisfying 
\begin_inset Formula $f\left(\boldsymbol{x}_{i}\right)=\tilde{y}_{i},\:1\leq i\leq k$
\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\tilde{\boldsymbol{f}}=\argmin{}_{\substack{\boldsymbol{f}=\left(\tilde{y}_{1},\,\tilde{y}_{2},\,\cdots,\,\tilde{y_{k}},\,f_{k+1},\,\cdots,\,f_{n}\right)\\
\sum f_{i}=0
}
}\boldsymbol{f}^{T}\boldsymbol{S}\boldsymbol{f}$
\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Interpolated Regularization without Parameter
\begin_inset CommandInset label
LatexCommand label
name "alg:1.2"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
We now investigate generalization bounds for this graph semi-supervised
 learning regularization through algorithmic stability.
 
\begin_inset CommandInset citation
LatexCommand citet
key "BelkinMatveevaNiyogi2004"

\end_inset

 defined the the empirical error 
\begin_inset Formula $R_{k}\left(f\right)$
\end_inset

, which is a measure of how well we do on the training set, and the generalizati
on error 
\begin_inset Formula $R\left(f\right)$
\end_inset

, which is the expectation of how well we do on all labelled or unlabelled
 points, in the following way
\begin_inset Formula 
\begin{equation}
\begin{aligned}R_{k}\left(f\right) & =\frac{1}{k}\sum_{1}^{k}\left(f\left(\boldsymbol{x}_{i}\right)-y_{i}\right)^{2}\\
R\left(f\right) & =\mathbb{E}_{\mu}\left[\left(f\left(\boldsymbol{x}\right)-y\left(\boldsymbol{x}\right)\right)^{2}\right],
\end{aligned}
\end{equation}

\end_inset

where 
\begin_inset Formula $f_{T}$
\end_inset

 maps a given set of examples 
\begin_inset Formula $T$
\end_inset

 to 
\begin_inset Formula $\mathbb{R}$
\end_inset

, i.e.
 
\begin_inset Formula $f_{T}\,:\,V\rightarrow\mathbb{R}$
\end_inset

, the expectation is taken over an underlying distribution 
\begin_inset Formula $\mu$
\end_inset

.
 In theorem 5, they showed that for data samples of size 
\begin_inset Formula $k\geq4$
\end_inset

 with multiplicity of at most 
\begin_inset Formula $t$
\end_inset

, 
\begin_inset Formula $\gamma$
\end_inset

-regularization using the smoothness functional 
\begin_inset Formula $S$
\end_inset

 is a 
\begin_inset Formula $\left(\frac{3M\sqrt{tk}}{\left(k\gamma\lambda_{1}-t\right)^{2}}+\frac{4M}{k\gamma\lambda_{1}-t}\right)$
\end_inset

-stable algorithm, assuming that the denominator 
\begin_inset Formula $k\gamma\lambda_{1}-t$
\end_inset

 is positive.
 
\begin_inset CommandInset citation
LatexCommand citet
key "BousquetElisseeff2001"

\end_inset

 showed that for a 
\begin_inset Formula $\beta$
\end_inset

-stable algorithm 
\begin_inset Formula $T\rightarrow f_{T}$
\end_inset

 we have
\begin_inset Formula 
\begin{equation}
\mathbb{P}\left(\left|R_{k}\left(f_{T}\right)-R\left(f_{T}\right)\right|>\epsilon+\beta\right)\leq2\exp\left(-\frac{k\epsilon^{2}}{2\left(k\beta+K+M\right)^{2}}\right),\qquad\forall\epsilon>0.
\end{equation}

\end_inset

Therefore, by following the derivation in the theorem 11.1 in 
\begin_inset CommandInset citation
LatexCommand citet
key "MohriRosTalw2012"

\end_inset

, we have with probability 
\begin_inset Formula $1-\delta$
\end_inset

 
\begin_inset Formula 
\begin{equation}
\begin{aligned}\left|R_{k}\left(f_{T}\right)-R\left(f_{T}\right)\right| & <\epsilon+\beta\end{aligned}
\label{eq:2.14}
\end{equation}

\end_inset

where 
\begin_inset Formula $\delta=2\exp\left(-\frac{k\epsilon^{2}}{2\left(k\beta+K+M\right)^{2}}\right)$
\end_inset

.
 We then solve for 
\begin_inset Formula $\epsilon$
\end_inset

 in this expression for 
\begin_inset Formula $\delta$
\end_inset

 and get 
\begin_inset Formula 
\begin{equation}
\epsilon=\sqrt{\frac{2\ln\frac{2}{\delta}}{k}}\left(k\beta+K+M\right),
\end{equation}

\end_inset

then plug into inequality 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:2.14"

\end_inset

 and rearrange terms, then, with probability 
\begin_inset Formula $1-\delta$
\end_inset

, we have
\begin_inset Formula 
\begin{equation}
\left|R_{k}\left(f_{T}\right)-R\left(f_{T}\right)\right|\leq\beta+\sqrt{\frac{2\ln\frac{2}{\delta}}{k}}\left(k\beta+K+M\right),
\end{equation}

\end_inset

where 
\begin_inset Formula 
\begin{equation}
\beta=\left(\frac{3M\sqrt{tk}}{\left(k\gamma\lambda_{1}-t\right)^{2}}+\frac{4M}{k\gamma\lambda_{1}-t}\right).
\end{equation}

\end_inset

This is the theorem 1 of 
\begin_inset CommandInset citation
LatexCommand citet
key "BelkinMatveevaNiyogi2004"

\end_inset

.
\end_layout

\begin_layout Section
Semi-Supervised Learning and Regularization over Graphs
\end_layout

\begin_layout Standard
\begin_inset Formula $\boldsymbol{Semi-supervised}$
\end_inset

 
\begin_inset Formula $\boldsymbol{learning}$
\end_inset

 is halfway between supervised and unsupervised learning.
 In addition to unlabelled data, the algorithm is provided with some supervision
 information, but not necessarily for all examples.
 Often, this information will be the targets associated with some of the
 examples.
 In this case, the data set 
\begin_inset Formula $\boldsymbol{X}=\left\{ x_{1},\,x_{2},\,\cdots,\,x_{n}\right\} $
\end_inset

 can be divided into two parts: the points 
\begin_inset Formula $\boldsymbol{X}_{l}=\left\{ x_{1},\,x_{2},\,\cdots,\,x_{l}\right\} $
\end_inset

, for which labels 
\begin_inset Formula $\boldsymbol{Y}_{l}=\left(y_{1},\,y_{2},\,\cdots,\,y_{l}\right)$
\end_inset

 are provided, and the points 
\begin_inset Formula $\boldsymbol{X}_{u}=\left\{ x_{l+1},\,x_{l+2},\,\cdots,\,x_{l+u}\right\} $
\end_inset

, the labels of which are not known.
\end_layout

\begin_layout Standard
Building upon the previous ideas introduced in this section, we now describe
 a setting for semi-supervised learning over a weighted graph.
 We start with vertices 
\begin_inset Formula $1,\,2,\,\cdots,\,l$
\end_inset

 labelled 
\begin_inset Formula $l$
\end_inset

 with their known label
\begin_inset Formula $1$
\end_inset

 or 
\begin_inset Formula $−1$
\end_inset

 and nodes 
\begin_inset Formula $l+1,\,\cdots,\,n$
\end_inset

 labelled with 
\begin_inset Formula $0$
\end_inset

.
 Estimated labels on both labelled and unlabelled data are denoted by 
\begin_inset Formula $\hat{\boldsymbol{Y}}=\left(\hat{\boldsymbol{Y}}_{l},\,\hat{\boldsymbol{Y}}_{u}\right)$
\end_inset

, where where 
\begin_inset Formula $\hat{\boldsymbol{Y}}_{l}$
\end_inset

 may be allowed to differ from the given labels 
\begin_inset Formula $\boldsymbol{Y}_{l}=\left(y_{1},\,y_{2},\,\cdots,\,y_{l}\right)$
\end_inset

.
 At each step a vertex 
\begin_inset Formula $i$
\end_inset

 receives a contribution from its neighbours 
\begin_inset Formula $j$
\end_inset

 and an additional small contribution given by its initial value.
 In addition, we wish to take into consideration that the correct labels
 
\begin_inset Formula $\boldsymbol{Y}_{l}$
\end_inset

 are known for some subset of vertices.
 
\end_layout

\begin_layout Standard
The new semi-supervised learning objective – that we wish to minimize the
 difference between predicted labels and those that are known in the labelled
 set – can be reflected in terms of the cost function 
\begin_inset Formula $||S\hat{Y_{v}}-SY_{v}||_{2}^{2}$
\end_inset

, where 
\begin_inset Formula $S=I_{[l]}$
\end_inset

 is a selection matrix that is equal to the identity for the first 
\begin_inset Formula $l$
\end_inset

 rows and columns, and equal to 0 everywhere else.
 In addition, since we no longer have access to the true labels for every
 vertex, we add a regularization term, so that we can deal with degenerate
 situations, such as connected components of the graph that have no labelled
 vertices.
 Thus we seek to minimize the objective function 
\begin_inset CommandInset citation
LatexCommand citet
key "ZhouBousquetLalWS2004"

\end_inset

 
\begin_inset Formula 
\begin{equation}
C(\hat{Y)}=||S\hat{Y}-SY||_{2}^{2}+\mu\hat{Y}L_{w}\hat{Y}+\epsilon\mu||\hat{Y}||_{2}^{2}.
\end{equation}

\end_inset

We can use straightforward calculus techniques to minimize this expression
\begin_inset Formula 
\begin{equation}
\frac{\partial C(\hat{Y})}{\partial\hat{Y}}=2S(\hat{Y}-Y)+2\mu L\hat{Y}+2\epsilon\mu I\hat{Y}=0.
\end{equation}

\end_inset


\begin_inset CommandInset citation
LatexCommand citet
key "BengioDelalleauRoux2006"

\end_inset

 proposed a solution to this system of linear equations based on the Jacobi
 iterative method for linear systems.
 The Jacobi iteration algorithm gives a solution to the linear system 
\begin_inset Formula 
\begin{equation}
Mx=b.
\end{equation}

\end_inset

By expanding the system as 
\begin_inset Formula $\sum_{j=1}^{n}m_{ij}x_{j}=b_{j}.$
\end_inset

 Suppose we wish to solve for 
\begin_inset Formula $x_{j}$
\end_inset

 while assuming that all other values of x remain fixed.
 We decompose the sum as
\begin_inset Formula 
\begin{equation}
\sum_{j=1}^{n}m_{ij}x_{j}=m_{ij}x_{j}+\sum_{k\neq j}^{n}m_{ij}x_{j}=b_{i}\to x_{j}=b_{i}-\frac{\sum_{k\neq j}^{n}m_{ij}x_{j}}{m_{ij}}.
\end{equation}

\end_inset

We can write this in matrix form as 
\begin_inset Formula 
\begin{equation}
x^{(k)}=D^{-1}Rx^{k-1}+D^{-1}b,
\end{equation}

\end_inset

where D is the diagonal component of M, and R is the remainder, (i.e.
 D+R=M).
 In our case, 
\begin_inset Formula $M=S+\mu L+\epsilon\mu I$
\end_inset

, and 
\begin_inset Formula $b=SY$
\end_inset

.
 The Jacobi method is particularly well suited for the graphical setting,
 since 
\begin_inset Formula $M=S+\mu D+\epsilon\mu I$
\end_inset

, and 
\begin_inset Formula $R=\mu W.$
\end_inset

 A sufficient condition for the convergence of the Jacobi method is when
 the matrix M is strictly diangonally dominany, meaning that the absolute
 value of each diagonal entry is greater than any other entry in the same
 row or column.
 Since the diagonal entries of the Laplacian are by definition positive
 and at least as great as any entry in the same row or column, adding the
 positive diagonal matrix 
\begin_inset Formula $S+\epsilon\mu I$
\end_inset

 to 
\begin_inset Formula $\mu L$
\end_inset

 makes 
\begin_inset Formula $M=S+\mu L+\epsilon\mu I$
\end_inset

 strictly diagonally dominant, as desired.
\end_layout

\begin_layout Standard
\begin_inset Float algorithm
wide false
sideways false
status open

\begin_layout Plain Layout
Compute weight matrix 
\begin_inset Formula $\boldsymbol{W}$
\end_inset

 from formula 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:1.5"

\end_inset

 such that 
\begin_inset Formula $\boldsymbol{W}_{ii}=0$
\end_inset


\end_layout

\begin_layout Plain Layout
Compute the diagonal degree matrix 
\begin_inset Formula $\boldsymbol{D}$
\end_inset

 by 
\begin_inset Formula $\boldsymbol{D}_{ii}=\sum_{j}\boldsymbol{W}_{ij}$
\end_inset

 
\end_layout

\begin_layout Plain Layout
Choose a parameter 
\begin_inset Formula $\alpha\in\left(0,\,1\right)$
\end_inset

 and a small 
\begin_inset Formula $\epsilon>0$
\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\mu=\frac{\alpha}{1-\alpha}\in\left(0,\,+\infty\right)$
\end_inset


\end_layout

\begin_layout Plain Layout
Compute the diagonal matrix 
\begin_inset Formula $\boldsymbol{A}$
\end_inset

 by 
\begin_inset Formula $\boldsymbol{A}_{ii}=\boldsymbol{I}_{l}\left(i\right)+\mu\boldsymbol{D}_{ii}+\mu\epsilon$
\end_inset


\end_layout

\begin_layout Plain Layout
Initialize 
\begin_inset Formula $\hat{\boldsymbol{Y}}^{\left(0\right)}=\left(y_{1},\,y_{2},\,\cdots,\,y_{l},\,0,\,0,\,\cdots,\,0\right)$
\end_inset


\end_layout

\begin_layout Plain Layout
Iterate
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\qquad$
\end_inset


\begin_inset Formula $\hat{\boldsymbol{Y}}^{\left(t+1\right)}=\boldsymbol{A}^{-1}\left(\mu\boldsymbol{W}\hat{\boldsymbol{Y}}^{\left(t\right)}+\hat{\boldsymbol{Y}}^{\left(0\right)}\right)$
\end_inset


\end_layout

\begin_layout Plain Layout
until convergence to 
\begin_inset Formula $\hat{\boldsymbol{Y}}^{\left(\infty\right)}$
\end_inset


\end_layout

\begin_layout Plain Layout
Label point 
\begin_inset Formula $v_{i}$
\end_inset

 by the sign of 
\begin_inset Formula $\hat{y}_{i}^{\left(\infty\right)}$
\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Jacobi Iterative Label Propagation Algorithm
\begin_inset CommandInset label
LatexCommand label
name "alg:2.3"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Section
Projection Algorithm for Graph Online Learning
\end_layout

\begin_layout Standard
A different approach was proposed by 
\begin_inset CommandInset citation
LatexCommand citet
key "HerbsterPontilWainer2005"

\end_inset

 using projection and kernel methods.
 
\end_layout

\begin_layout Subsection
Pseudo-Inverse of Graph Laplacian as the Reproducing Kernel of a Hilbert
 Space
\end_layout

\begin_layout Standard
As in the previous examples, we consider real-valued functions over the
 vertices of the graph
\begin_inset Formula 
\begin{equation}
\boldsymbol{f}\,:V\rightarrow\mathbb{R}^{n}
\end{equation}

\end_inset

We define the function 
\begin_inset Formula $\left\langle \boldsymbol{},\,\boldsymbol{}\right\rangle $
\end_inset

 as
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\left\langle \boldsymbol{f},\,\boldsymbol{g}\right\rangle =\boldsymbol{f}^{T}\boldsymbol{L}\boldsymbol{g}.
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula $\boldsymbol{L}$
\end_inset

is symmetric and positive semi-definite, which suggests that by appropriately
 constraining our vector space containing containing 
\begin_inset Formula $f$
\end_inset

, we could ensure the strict positiveity of 
\begin_inset Formula $\left\langle \boldsymbol{f},\,\boldsymbol{g}\right\rangle $
\end_inset

, which would imply that that our space is in fact a Hilbert space.
 Over the entire Vector space of real-valued functions on the graph, 
\begin_inset Formula $\left\langle \boldsymbol{f},\,\boldsymbol{g}\right\rangle $
\end_inset

 is only a semi-inner product, and therefore induces the semi-norm 
\begin_inset Formula $\left\Vert \boldsymbol{g}\right\Vert $
\end_inset

.
 If 
\begin_inset Formula $\boldsymbol{f}^{*}$
\end_inset

 is an eigenvector of 
\begin_inset Formula $\boldsymbol{L}$
\end_inset

 corresponding to an eigenvalue of 
\begin_inset Formula $0$
\end_inset

, then 
\begin_inset Formula $\left\langle \boldsymbol{f}^{*},\,\boldsymbol{f}^{*}\right\rangle =0$
\end_inset

.
 These eigenvectors with eigenvalues of zero are precisely the eigenvectors
 that are piecewise constant on each connected component.
 We can see this by noting that 
\begin_inset Formula 
\begin{equation}
\begin{aligned}\left\Vert \boldsymbol{f}\right\Vert ^{2} & =\sum_{\left(i,\,j\right)\in E\left(G\right)}(\boldsymbol{f}_{i}-\boldsymbol{f}_{j})^{2}\end{aligned}
.
\end{equation}

\end_inset

This is because 
\begin_inset Formula 
\begin{equation}
\begin{aligned}\left\Vert \boldsymbol{f}\right\Vert ^{2} & =\left\langle \boldsymbol{f},\,\boldsymbol{f}\right\rangle \\
 & =\sum_{i,\,j=1}^{n}\boldsymbol{f}_{i}\boldsymbol{L}_{ij}\boldsymbol{f}_{j}\\
 & =\sum_{i=j}^{n}\boldsymbol{f}_{i}\boldsymbol{L}_{ij}\boldsymbol{f}_{j}+\sum_{i\neq j}^{n}\boldsymbol{f}_{i}\boldsymbol{L}_{ij}\boldsymbol{f}_{j}\\
 & =\sum_{i=1}^{n}\boldsymbol{f}_{i}^{2}\boldsymbol{D}_{ii}+\sum_{i\neq j}^{n}\boldsymbol{f}_{i}\boldsymbol{A}_{ij}\boldsymbol{f}_{j}\\
 & =\sum_{i,\,j=1}^{n}\boldsymbol{f}_{i}{}_{i}^{2}\boldsymbol{A}_{ij}+\sum_{i\neq j}^{n}\boldsymbol{f}_{i}\boldsymbol{L}_{ij}\boldsymbol{f}_{j}\\
 & =\sum_{i,j\in E\left(G\right)}^{n}\boldsymbol{f}_{i}^{2}-2\boldsymbol{f}_{i}\boldsymbol{f}_{j}\\
 & =\sum_{\left(i,\,j\right)\in E\left(G\right)}(\boldsymbol{f}_{i}-\boldsymbol{f}_{j})^{2}.
\end{aligned}
\end{equation}

\end_inset

Note the diagonal component of 
\begin_inset Formula $\boldsymbol{L}$
\end_inset

 is 
\begin_inset Formula $\boldsymbol{D}$
\end_inset

, the off-diagonal component of 
\begin_inset Formula $\boldsymbol{L}$
\end_inset

 is 
\begin_inset Formula $-\boldsymbol{A}$
\end_inset

, 
\begin_inset Formula $\boldsymbol{D}_{ii}=\sum_{j=1}^{n}\boldsymbol{A}_{ij}$
\end_inset

, and in the last line of the proof we drop a factor of 
\begin_inset Formula $2$
\end_inset

 from the second summation since 
\begin_inset Formula $E$
\end_inset

 is the set of unordered pairs in the edge set, and therefore each edge
 gets counted twice.
\end_layout

\begin_layout Standard
As we can see, the sum vanishes if 
\begin_inset Formula $\boldsymbol{f}_{i}=\boldsymbol{f}_{j}$
\end_inset

 for all vertices connected by an edge, i.e.
 when g is piecewise constant on each connected component.
 Therefore we must restrict our functions to be in the subspace of 
\begin_inset Formula $H$
\end_inset

 spanned by the eigenvectors that have non-zero eigenvalues.
 Restricted to this subspace, 
\begin_inset Formula $\left\langle \boldsymbol{f},\,\boldsymbol{g}\right\rangle $
\end_inset

 is indeed an inner product, and therefore induces a true norm 
\begin_inset Formula $\left\Vert \boldsymbol{f}\right\Vert $
\end_inset

.
 
\end_layout

\begin_layout Standard
Now we can define the objective function 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\min_{f\in H}\{||f||:f_{i}=y_{i},i=1,...,l\}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
Note that this is very similar to the objective function as that of section
 2, but our norm is being induced by a different inner product.
 Once 
\begin_inset Formula $L^{+}$
\end_inset

has been computed, this formulation will allow us to solve a system of 
\begin_inset Formula $l$
\end_inset

 equations rather than a system of 
\begin_inset Formula $n$
\end_inset

 equations, as in the formulation of section 2.
 Since 
\begin_inset Formula $H$
\end_inset

 is a Hilbert space, we can use the Riesz Representation theorem to conclude
 that the evaluation functional of 
\begin_inset Formula $\boldsymbol{f}$
\end_inset

 at any vertex, being a linear functional, can be represented as an inner
 product 
\begin_inset Formula 
\begin{equation}
f_{i}=f(v_{i})=\left\langle \boldsymbol{K}_{i},\,\boldsymbol{f}\right\rangle =\boldsymbol{K}_{i}\boldsymbol{L}\boldsymbol{f}.
\end{equation}

\end_inset

The pseudo-inverse 
\begin_inset Formula $\boldsymbol{L}^{+}$
\end_inset

 satisfies this property for the reproducing kernel 
\begin_inset Formula $\boldsymbol{K}$
\end_inset

, which can be verified by noting that 
\begin_inset Formula 
\begin{equation}
e_{i}^{T}\boldsymbol{\boldsymbol{L}^{+}}\boldsymbol{L}\boldsymbol{f}=\boldsymbol{L}_{i}^{+}\boldsymbol{L}\boldsymbol{f}=f_{i}
\end{equation}

\end_inset

We call 
\begin_inset Formula $\boldsymbol{L}^{+}=K$
\end_inset

 the reproducing kernel of the hilbert space
\begin_inset Formula $H$
\end_inset

.
\end_layout

\begin_layout Standard
Now, we return to the optimization problem (3.5).
 Since Herbster does not give a proof for this in the paper, we give our
 own derivation of the minimizer of (3.5), following the arguments of (Aronszajn
 1950) and (Wahba 1990).
 Recall that the reproducing kernel of the direct sum of RKHS is equal to
 the sum of the reproducing kernels of the two spaces (Aronszajn 1950).
 In our case, we consider the two subspaces (both Hilbert spaces) of H,
 
\begin_inset Formula $H_{1}$
\end_inset

and 
\begin_inset Formula $H_{2}$
\end_inset

 where 
\begin_inset Formula $H_{1}$
\end_inset

 is the span of the first 
\begin_inset Formula $l$
\end_inset

 basis vectors of 
\begin_inset Formula $H$
\end_inset

, corresponding to the labeled vertices, while 
\begin_inset Formula $H_{2}$
\end_inset

 is the span of the last 
\begin_inset Formula $n-l$
\end_inset

 basis vectors of 
\begin_inset Formula $H$
\end_inset

, so that 
\begin_inset Formula $H=H_{1}\oplus H_{2}$
\end_inset

.
 Then we can write and 
\begin_inset Formula $f\in H$
\end_inset

 as a unique decomposition 
\begin_inset Formula $f=f_{1}+f_{2}$
\end_inset

 with 
\begin_inset Formula $f_{1}\in H_{1},f_{2}\in H_{2}.$
\end_inset

 By the reproducing kernel property, we can express any 
\begin_inset Formula $f_{i}$
\end_inset

 as 
\begin_inset Formula $f_{i}=\left\langle \boldsymbol{K}_{i},\,\boldsymbol{f}\right\rangle =\left\langle \boldsymbol{K}_{i},\,\boldsymbol{f_{1}}\right\rangle +\left\langle \boldsymbol{K}_{i},\,\boldsymbol{f_{2}}\right\rangle $
\end_inset

.
 Since the last 
\begin_inset Formula $n-l$
\end_inset

 terms of 
\begin_inset Formula $\boldsymbol{f_{1}}$
\end_inset

and the first 
\begin_inset Formula $l$
\end_inset

 terms of 
\begin_inset Formula $f_{2}$
\end_inset

are 0, we can equivalently write this as 
\begin_inset Formula $f_{i}=\left\langle \boldsymbol{K}_{i},\,\boldsymbol{f}\right\rangle =\left\langle \boldsymbol{K}_{i}^{1},\,\boldsymbol{f_{1}}\right\rangle +\left\langle \boldsymbol{K}_{i}^{2},\,\boldsymbol{f_{2}}\right\rangle $
\end_inset

, where 
\begin_inset Formula $\boldsymbol{K_{i}}^{1}=(K_{i})_{j=1}^{l}$
\end_inset

and 
\begin_inset Formula $K_{i}^{2}=(K_{i})_{j=l+1}^{n}$
\end_inset

.
 Thus 
\begin_inset Formula $K^{1}$
\end_inset

 and 
\begin_inset Formula $K^{2}$
\end_inset

are the reproducing kernels of 
\begin_inset Formula $H_{1}$
\end_inset

and 
\begin_inset Formula $H_{2},$
\end_inset

respectively.
 
\end_layout

\begin_layout Standard
Since 
\series bold

\begin_inset Formula $K^{1}$
\end_inset


\series default
and 
\begin_inset Formula $K^{2}$
\end_inset

 are positive definite, the columns of 
\series bold

\begin_inset Formula $K^{1}$
\end_inset


\series default
and 
\begin_inset Formula $K^{2}$
\end_inset

form a basis for 
\begin_inset Formula $H_{1}$
\end_inset

and 
\begin_inset Formula $H_{2}$
\end_inset

respectively.
 It follows that we can express any 
\begin_inset Formula $f$
\end_inset

 as 
\begin_inset Formula $f=f_{1}+f_{2}=\sum_{i=1}^{l}c_{j}K_{j}^{1}+\sum_{i=l+1}^{n}d_{j}K_{j}^{2}$
\end_inset

.
 Note that the vectors 
\begin_inset Formula $K_{j}^{2}$
\end_inset

 in the second sum are all perpendicular to the 
\begin_inset Formula $K_{j}^{1},$
\end_inset

 and therefore for all 
\begin_inset Formula $1\leq i\leq l$
\end_inset

, we have 
\begin_inset Formula $f_{i}=$
\end_inset

 
\begin_inset Formula $\left\langle \boldsymbol{K}_{i},\,\boldsymbol{f}\right\rangle =\left\langle \boldsymbol{K}_{i},\,\sum_{i=1}^{l}c_{j}K_{j}^{1}+\sum_{i=l+1}^{n}d_{j}K_{j}^{2}\right\rangle =\left\langle \boldsymbol{K}_{i},\,\sum_{i=1}^{l}c_{j}K_{j}^{1}\right\rangle $
\end_inset

.
\end_layout

\begin_layout Standard
It follows that for all 
\begin_inset Formula $1\leq i\leq l$
\end_inset

, 
\begin_inset Formula $f=\sum_{i=1}^{l}c_{j}K_{j}^{1}=\sum_{i=1}^{n}e_{i}\sum_{j=1}^{l}c_{j}K_{ij}^{1}$
\end_inset

, or equivalently 
\begin_inset Formula $f_{i}=\sum_{j=1}^{l}c_{j}K_{ij}^{1}$
\end_inset

, which matches the result of Herbster for 
\begin_inset Formula $1\leq i\leq l.$
\end_inset

 Herbster's paper does not mention that this only holds true for 
\begin_inset Formula $1\leq i\leq l$
\end_inset

 but it will turn out not to matter for the sake of minimizing (3.5), as
 we only need to represent the first 
\begin_inset Formula $l$
\end_inset

 components of 
\begin_inset Formula $f$
\end_inset

 anyways.
 We adopt our notation slightly to vector form: we have 
\begin_inset Formula $f=K^{1}\boldsymbol{c}$
\end_inset

, where we are now referring to 
\begin_inset Formula $K^{1}$
\end_inset

as the upper left nxr submatrix of 
\begin_inset Formula $K^{1}$
\end_inset

 (the last 
\begin_inset Formula $n-l$
\end_inset

 columns are all 0 an were therefore not .
 We can now write the lagrange dual of (3.5) as
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\min_{f\in H}\{||f||+\lambda\sum_{i=1}^{l}(y_{i}-f_{i})\}=\min_{c\in H_{1}}\{||K^{1}\boldsymbol{c}||+\lambda\sum_{i=1}^{l}(y_{i}-K_{i}^{1}\boldsymbol{c})\}=\min_{c\in H_{1}}\{c^{T}(K^{1})^{T}LK^{1}\boldsymbol{c}+\lambda\boldsymbol{1^{T}}(\boldsymbol{y}-\tilde{K}c)\}
\]

\end_inset


\end_layout

\begin_layout Standard
Where 
\begin_inset Formula $\tilde{K}$
\end_inset

 is the upper left block of 
\begin_inset Formula $K^{1}$
\end_inset

 of dimension 
\begin_inset Formula $l$
\end_inset

x
\begin_inset Formula $l$
\end_inset

.
 Taking the gradient with resepect to c, we have 
\begin_inset Formula $(K^{1})^{T}LK^{1}\boldsymbol{c}=\tilde{K}c=\lambda\boldsymbol{1^{T}}\tilde{K}.$
\end_inset

 Pluggin in our constraint that 
\begin_inset Formula $\boldsymbol{y}=\tilde{K}c,$
\end_inset

 we have
\begin_inset Formula $y=\lambda\boldsymbol{1^{T}}\tilde{K}.$
\end_inset

 This implies that 
\begin_inset Formula $\tilde{K}c=y,$
\end_inset

 and finally that 
\begin_inset Formula $c=(\tilde{K})^{+}y$
\end_inset

.
 Finally, we can express the minimizer of the original objective as 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
f=K^{1}\boldsymbol{c}=K^{1}(\tilde{K})^{+}y
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
To gain intuition about 
\begin_inset Formula $K=L^{+}$
\end_inset

, we turn back to the representation theorem, namely that 
\begin_inset Formula $\boldsymbol{f}\left(v_{i}\right)=\boldsymbol{L}_{i}^{+}\boldsymbol{L}\boldsymbol{f}$
\end_inset

.
 In the heat analogy, this could be interpreted as saying that the heat
 at each vertex can be expressed in terms of, or derived from, the flux
 through every vertex.
 So in this heat analogy, 
\begin_inset Formula $\boldsymbol{L}$
\end_inset

 sends a heat distribution 
\begin_inset Formula $\boldsymbol{f}$
\end_inset

 over each node to a flux through each vertex.
 Conversely, 
\begin_inset Formula $\boldsymbol{L}_{i}^{+}$
\end_inset

 sends some vector of fluxes through vertex graph back to some heat distribution
 that would have induced it.
 This relationship is shown in figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:3.1"

\end_inset

.
 
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename 3-1.png
	lyxscale 70
	scale 60

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Mapping Between Heat and Flux
\begin_inset CommandInset label
LatexCommand label
name "fig:3.1"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset

 Returning to equation (3.8), we can now interpret the solution to this minimizat
ion problem roughly as two steps: first we compute the flux through each
 label generated by each other label as 
\begin_inset Formula $(\tilde{K})^{+}y$
\end_inset

, ignoring the presence of any unlabeled nodes.
 Then we apply the mapping 
\begin_inset Formula $K^{1}$
\end_inset

to determine the heat distribution 
\begin_inset Formula $\boldsymbol{f}$
\end_inset

 over all vertices that remains consistent with this flux, by taking 
\begin_inset Formula $K^{1}(\tilde{K})^{+}y.$
\end_inset

 
\end_layout

\begin_layout Standard
In the Herbster paper this result is stated (but not proven), and then the
 authors proceed to describe the projection algorithm described in the next
 section, gurantee its convergence, and derive a bound on the number of
 mistakes using that algorithm.
 However, one might ask how this projection algorithm and the solution of
 Wahba are related.
 Briefly, I will say that we can think of the optimal weight given by (3.8)
 as a projection of the vetor 
\begin_inset Formula $c$
\end_inset

 onto the first 
\begin_inset Formula $l$
\end_inset

 columns of K.
 
\end_layout

\begin_layout Subsection
Online Projections over Graphs
\end_layout

\begin_layout Standard
With that in mind, we can introduce the mathematical framework for the online
 projection algorithm of 
\begin_inset CommandInset citation
LatexCommand citet
key "HerbsterPontilWainer2005"

\end_inset

.
 Herbster seeks to minimize the number of mistakes (cumulative error) on
 the sequence of pattern-label pairs over a series of trials.
 The key tool for this algorithm is projection, which is defined by definition
 
\begin_inset CommandInset ref
LatexCommand ref
reference "def:3.1"

\end_inset

.
 In the sequel, we will refer to the weight vector of the genetic projection
 algorithm as 
\begin_inset Formula $\boldsymbol{w}$
\end_inset

 rather than 
\begin_inset Formula $\boldsymbol{f,}$
\end_inset

which refers specifically to the definition of the minimizer of (3.5).
 While it can be shown that the following algorithm converges to the solution
 of (3.5), we will keep separate notation for the solution obtained through
 the two different algorithms.
\end_layout

\begin_layout Definition
\begin_inset CommandInset label
LatexCommand label
name "def:3.1"

\end_inset

The projection of a point 
\begin_inset Formula $\boldsymbol{w}\in\mathcal{H}$
\end_inset

, which is a Hilbert space with inner product 
\begin_inset Formula $\left\langle \cdot,\cdot\right\rangle $
\end_inset

 and norm 
\begin_inset Formula $\left\Vert \cdot\right\Vert =\sqrt{\left\langle \cdot,\cdot\right\rangle }$
\end_inset

, onto a closed convex non-empty set 
\begin_inset Formula $\mathcal{N}\subseteq\mathcal{H}$
\end_inset

 is defined by 
\begin_inset Formula 
\begin{equation}
P\left(\mathcal{N};\,\boldsymbol{w}\right)\coloneqq\argmin_{\boldsymbol{u}\in\mathcal{N}}\left\Vert \boldsymbol{u}-\boldsymbol{w}\right\Vert .\label{eq:3.15}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Standard
From the definition, we can see that 
\begin_inset Formula $\boldsymbol{u}$
\end_inset

 any point in 
\begin_inset Formula $\mathcal{N}$
\end_inset

 that is closest to 
\begin_inset Formula $\boldsymbol{w}$
\end_inset

.
 If we assume 
\begin_inset Formula $\boldsymbol{u}^{*}=P\left(\mathcal{N};\,\boldsymbol{w}\right)=\argmin_{\boldsymbol{u}\in\mathcal{N}}\left\Vert \boldsymbol{u}-\boldsymbol{w}\right\Vert $
\end_inset

, then the solution is characterized by an orthogonality condition: 
\begin_inset Formula $\boldsymbol{u}^{*}$
\end_inset

 is the solution if and only if 
\begin_inset Formula 
\begin{equation}
\left\langle \boldsymbol{w}-\boldsymbol{u}^{*},\,\boldsymbol{u}-\boldsymbol{u}^{*}\right\rangle =0.
\end{equation}

\end_inset

Therefore by the Pythagorean Theorem, we have 
\begin_inset Formula 
\begin{equation}
\left\Vert \boldsymbol{u}-\boldsymbol{w}\right\Vert ^{2}\ge\min_{\boldsymbol{u}\in\mathcal{N}}\left\Vert \boldsymbol{u}-\boldsymbol{w}\right\Vert =\left\Vert \boldsymbol{u^{*}}-\boldsymbol{w}\right\Vert ^{2}=\left\Vert \boldsymbol{u}-\boldsymbol{u}^{*}\right\Vert ^{2}+\left\Vert \boldsymbol{u}^{*}-\boldsymbol{w}\right\Vert ^{2}.
\end{equation}

\end_inset

Hence we have the following theorem.
 
\end_layout

\begin_layout Theorem
\begin_inset CommandInset label
LatexCommand label
name "thm:3.1"

\end_inset

If 
\begin_inset Formula $\mathcal{N}$
\end_inset

 is a closed convex subset of 
\begin_inset Formula $\mathcal{H}$
\end_inset

, then, for every 
\begin_inset Formula $\boldsymbol{u}\in\mathcal{N}$
\end_inset

 and 
\begin_inset Formula $\boldsymbol{w}\in\mathcal{H}$
\end_inset

, we have that
\begin_inset Formula 
\begin{equation}
\left\Vert \boldsymbol{u}-\boldsymbol{w}\right\Vert ^{2}\geq\left\Vert \boldsymbol{u}-P\left(\mathcal{N};\,\boldsymbol{w}\right)\right\Vert ^{2}+\left\Vert P\left(\mathcal{N};\,\boldsymbol{w}\right)-\boldsymbol{w}\right\Vert ^{2}.
\end{equation}

\end_inset

In particular, if 
\begin_inset Formula $\mathcal{N}$
\end_inset

 is an affine set the equality holds.
\end_layout

\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Standard
Our learning scenario consists of a set of an example set 
\begin_inset Formula $\mathcal{D}=\left\{ \left(x_{i,}\,y_{i}\right)\right\} _{t=1}^{l}\subseteq\mathcal{X}\times\left\{ -1,\,+1\right\} $
\end_inset

 .
 let 
\begin_inset Formula $\mathcal{D}_{t}$
\end_inset

denote the first 
\begin_inset Formula $t$
\end_inset

 examples of 
\begin_inset Formula $\mathcal{D}$
\end_inset

.
 We consider an online learning algorithm (algorithm 
\begin_inset CommandInset ref
LatexCommand ref
reference "alg:3.4"

\end_inset

) such that at each iteration, 
\begin_inset Formula $\boldsymbol{w}_{t}$
\end_inset

 is updated by the projection 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:3.15"

\end_inset

, then it is convergent.
 Because on any trial 
\begin_inset Formula $t=1,\,2,\,\cdots,\,l$
\end_inset

, the theorem 
\begin_inset CommandInset ref
LatexCommand ref
reference "thm:3.1"

\end_inset

 implies, for all 
\begin_inset Formula $\boldsymbol{u}\in\mathcal{U}$
\end_inset

, that 
\begin_inset Formula $\left\Vert \boldsymbol{u}-\boldsymbol{w}_{t}\right\Vert ^{2}\geq\left\Vert \boldsymbol{u}-\boldsymbol{w}_{t+1}\right\Vert ^{2}+\left\Vert \boldsymbol{w}_{t+1}-\boldsymbol{w}_{t}\right\Vert ^{2}$
\end_inset

, which is 
\begin_inset Formula $\left\Vert \boldsymbol{w}_{t+1}-\boldsymbol{w}_{t}\right\Vert ^{2}\leq\left\Vert \boldsymbol{u}-\boldsymbol{w}_{t}\right\Vert ^{2}+\left\Vert \boldsymbol{u}-\boldsymbol{w}_{t+1}\right\Vert ^{2}$
\end_inset

.
 Therefore, by summing it from 
\begin_inset Formula $t=1$
\end_inset

 to 
\begin_inset Formula $t=l$
\end_inset

, we have 
\begin_inset Formula $\sum_{t=1}^{l}\left\Vert \boldsymbol{w}_{t+1}-\boldsymbol{w}_{t}\right\Vert ^{2}\leq\left\Vert \boldsymbol{u}-\boldsymbol{w}_{t}\right\Vert ^{2}-\left\Vert \boldsymbol{u}-\boldsymbol{w}_{l+1}\right\Vert ^{2}$
\end_inset

, where 
\begin_inset Formula $\mathcal{U}_{t}\in\mathcal{H}$
\end_inset

, and 
\begin_inset Formula $\boldsymbol{u}\in\cap_{t=1}^{l}\mathcal{U}_{t}$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float algorithm
wide false
sideways false
status open

\begin_layout Plain Layout

\series bold
Input
\series default
: A sequence of closed convex sets 
\begin_inset Formula $\left\{ \mathcal{U}_{t}\right\} _{t=1}^{l}\subset\mathcal{H}$
\end_inset


\end_layout

\begin_layout Plain Layout

\series bold
Initialization
\series default
: 
\begin_inset Formula $\boldsymbol{w}_{1}\in\mathcal{H}$
\end_inset


\end_layout

\begin_layout Plain Layout

\series bold
for
\series default
 
\begin_inset Formula $t=1,\,2,\,\cdots,\,l$
\end_inset

 
\series bold
do
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\qquad$
\end_inset


\begin_inset Formula $\boldsymbol{w}_{t+1}=P\left(\mathcal{U}_{t};\,\boldsymbol{w}_{t}\right)$
\end_inset


\end_layout

\begin_layout Plain Layout

\series bold
end
\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Prototypical Projection Algorithm
\begin_inset CommandInset label
LatexCommand label
name "alg:3.4"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Lemma
\begin_inset CommandInset label
LatexCommand label
name "lem:3.1"

\end_inset

If 
\begin_inset Formula $\left(x,\,y\right)\in\mathcal{H}\times\left\{ -1,\,+1\right\} $
\end_inset

 and 
\begin_inset Formula $\boldsymbol{w}\in\mathcal{H}$
\end_inset

, then 
\begin_inset Formula 
\begin{equation}
P\left(\left\{ \boldsymbol{u}:\,\left\langle \boldsymbol{u},\,\boldsymbol{x}\right\rangle =y\right\} ;\,\boldsymbol{w}\right)=\boldsymbol{w}+\frac{\left(y-\left\langle \boldsymbol{w},\,\boldsymbol{x}\right\rangle \right)}{\left\Vert \boldsymbol{x}\right\Vert ^{2}}\boldsymbol{x}
\end{equation}

\end_inset


\begin_inset Formula 
\begin{equation}
P\left(\left\{ \boldsymbol{u}:\,y\left\langle \boldsymbol{u},\,\boldsymbol{x}\right\rangle \geq1\right\} ;\,\boldsymbol{w}\right)=\boldsymbol{w}+\frac{y\max\left(0,\,1-y\left\langle \boldsymbol{w},\,\boldsymbol{x}\right\rangle \right)}{\left\Vert \boldsymbol{x}\right\Vert ^{2}}\boldsymbol{x}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Standard
Proof:
\end_layout

\begin_layout Standard
We give a proof of (3.11), which is almost exactly the same as the proof
 of (3.12).
 
\end_layout

\begin_layout Standard
\begin_inset Formula $\left\{ \boldsymbol{u}:\,\left\langle \boldsymbol{u},\,\boldsymbol{x}\right\rangle =y\right\} =\left\{ \boldsymbol{u}:\,\left\langle \boldsymbol{u-\frac{xy}{||x||^{2}}},\,\boldsymbol{x}\right\rangle =0\right\} $
\end_inset


\end_layout

\begin_layout Standard
Now let 
\begin_inset Formula $v(\boldsymbol{u})=\boldsymbol{u-\frac{xy}{||x||^{2}}}$
\end_inset

.
 Then we can find 
\begin_inset Formula $P\left(\left\{ \boldsymbol{u}:\,\left\langle \boldsymbol{u},\,\boldsymbol{x}\right\rangle =y\right\} ;\,\boldsymbol{w}\right)$
\end_inset

 by first finding 
\begin_inset Formula $v^{*}=Proj(\left\{ \boldsymbol{u}:\,\left\langle \boldsymbol{u},\,\boldsymbol{x}\right\rangle =0\right\} ;v(\boldsymbol{w}))$
\end_inset

 and then taking 
\begin_inset Formula $w^{*}=v^{-1}(v^{*}).$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula $v^{*}=Proj(\left\{ \boldsymbol{u}:\,\left\langle \boldsymbol{u},\,\boldsymbol{x}\right\rangle =0\right\} ;v(\boldsymbol{w}))=v(\boldsymbol{w})-Proj_{x}(v(\boldsymbol{w}))=v(\boldsymbol{w})-\boldsymbol{\frac{\left\langle \boldsymbol{v(w)},\,\boldsymbol{x}\right\rangle }{\left\langle \boldsymbol{x},\,\boldsymbol{x}\right\rangle }x=v(\boldsymbol{w})+\boldsymbol{\frac{\left\langle \boldsymbol{w-\frac{xy}{||x||^{2}}},\,\boldsymbol{x}\right\rangle }{\left\langle \boldsymbol{x},\,\boldsymbol{x}\right\rangle }x=v(\boldsymbol{w})+\boldsymbol{\frac{y-\left\langle \boldsymbol{w},\,\boldsymbol{x}\right\rangle }{\left\langle \boldsymbol{x},\,\boldsymbol{x}\right\rangle }x}}}$
\end_inset


\end_layout

\begin_layout Standard
Thus
\end_layout

\begin_layout Standard
\begin_inset Formula $\boldsymbol{w}^{*}=\boldsymbol{w+}\frac{y-\left\langle \boldsymbol{w},\,\boldsymbol{x}\right\rangle }{\left\langle \boldsymbol{x},\,\boldsymbol{x}\right\rangle }x$
\end_inset


\end_layout

\begin_layout Standard
Based on the prototypical projection algorithm 
\begin_inset CommandInset ref
LatexCommand ref
reference "alg:3.4"

\end_inset

 and lemma 
\begin_inset CommandInset ref
LatexCommand ref
reference "lem:3.1"

\end_inset

, the main algorithm of 
\begin_inset CommandInset citation
LatexCommand citet
key "HerbsterPontilWainer2005"

\end_inset

 is given in algorithm 
\begin_inset CommandInset ref
LatexCommand ref
reference "alg:3.5"

\end_inset

.
 In the algorithm, 
\begin_inset Formula $M$
\end_inset

 is the set of trials in which the algorithm predicts incorrectly, 
\begin_inset Formula $U_{t}$
\end_inset

 is the index set, which is determines by the 
\begin_inset Formula $\boldsymbol{strategy}$
\end_inset

 function on trial 
\begin_inset Formula $t$
\end_inset

.
 While the full specification in 
\begin_inset CommandInset ref
LatexCommand ref
reference "alg:3.5"

\end_inset

 allows for a function 
\begin_inset Quotes eld
\end_inset

strategy
\begin_inset Quotes erd
\end_inset

 to determine 
\begin_inset Formula $U_{t}$
\end_inset

, in this paper we will assume that
\begin_inset Formula $U_{t}=\{1,...,l\}$
\end_inset

, i.e.
 the set of labeled vertices.
 The author introduces two strategies: one called 
\begin_inset Quotes eld
\end_inset

non-cyclic
\begin_inset Quotes erd
\end_inset

 and the other called 
\begin_inset Quotes eld
\end_inset

cyclic
\begin_inset Quotes erd
\end_inset

.
 The boolean 
\begin_inset Formula $\boldsymbol{aggressive}$
\end_inset

 controls whether an update is made on every trial (aggressive) or only
 on those trials when a mistake occurs (non-aggressive).
\end_layout

\begin_layout Standard
The non-cyclic strategy projects the current hypothesis 
\begin_inset Formula $\boldsymbol{w}_{t}$
\end_inset

 to a single affine set 
\begin_inset Formula $af\left(U_{t}\right)\coloneqq\cap_{i\in U}\left\{ \boldsymbol{u}\in\mathcal{H}:\,\left\langle \boldsymbol{u},\,\boldsymbol{x}_{i}\right\rangle =y_{i}\right\} $
\end_inset

 .
 This ensures that all labeled points are accrately predicted.
 In contrast, the cyclic strategy repeatedly projects 
\series bold

\begin_inset Formula $\boldsymbol{w}$
\end_inset


\series default
 onto the sequence of half-spaces 
\begin_inset Formula $\left\{ \boldsymbol{u}\in\mathcal{H}:\,y_{t}\left\langle \boldsymbol{u},\,\boldsymbol{x}_{t}\right\rangle \geq1,\right\} $
\end_inset

 for each 
\begin_inset Formula $t\in M$
\end_inset

.
 Note that the strategy continues to loop through the mislabeled vertices
 until they are all predicting correctly.
 The terminal condition of this algorithm is ensured by the fact that a
 sequence of projections between closed convex sets converges to a point
 in their intersection.
 In other words, we can be sure that after completing the while loop, 
\begin_inset Formula $w_{t+1}\in hs\left(U_{t}\right)\coloneqq\cap_{i\in U_{t}}\left\{ \boldsymbol{u}\in\mathcal{H}:\,y_{i}\left\langle \boldsymbol{u},\,\boldsymbol{x}_{i}\right\rangle \geq1\right\} $
\end_inset

.
 The reason for this alternative projection algorithm is that it is more
 efficient: rather than having to perfom t projections on each trial, we
 only need to perform 
\begin_inset Formula $|M|$
\end_inset

 projections.
\end_layout

\begin_layout Standard
\begin_inset Float algorithm
wide false
sideways false
status open

\begin_layout Plain Layout
\align left

\series bold
\begin_inset Graphics
	filename Algorithm 3.5.png
	lyxscale 70
	scale 60

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Online Projection Algorithm
\begin_inset CommandInset label
LatexCommand label
name "alg:3.5"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Theorem
\begin_inset CommandInset label
LatexCommand label
name "thm:3.2"

\end_inset

If 
\begin_inset Formula $\left\{ \left(\boldsymbol{x}_{i},\,y_{i}\right)\right\} _{i=1}^{l}\subseteq\mathcal{H}\times\left\{ -1,\,+1\right\} $
\end_inset

 is a sequence of examples, 
\begin_inset Formula $\boldsymbol{w}_{1}\in\mathcal{H}$
\end_inset

 a start vector and 
\begin_inset Formula $M$
\end_inset

 the set of trials in which Algorithm 
\begin_inset CommandInset ref
LatexCommand ref
reference "alg:3.5"

\end_inset

 predicted incorrectly, then the cumulative number of mistakes 
\begin_inset Formula $\left|M\right|$
\end_inset

 of the algorithm is bounded by
\begin_inset Formula 
\begin{equation}
\left|M\right|\leq\left\Vert \boldsymbol{u}-\boldsymbol{w}_{1}\right\Vert ^{2}B\label{eq:3.21}
\end{equation}

\end_inset

for all 
\begin_inset Formula $\boldsymbol{u}\in hs\left(\left\{ 1,\,\cdots,\,l\right\} \right)$
\end_inset

 with cyclic updating and for all 
\begin_inset Formula $\boldsymbol{u}\in af\left(\left\{ 1,\,\cdots,\,l\right\} \right)$
\end_inset

 with non-cyclic updating, where
\begin_inset Formula 
\begin{equation}
B=\left(\frac{1}{\left|M\right|}\sum_{i}\left(\left\Vert \boldsymbol{x}_{i}\right\Vert ^{2}\right)^{-1}\right)^{-1}.
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Standard
Theorem 
\begin_inset CommandInset ref
LatexCommand ref
reference "thm:3.2"

\end_inset

 bounds the number of mistakes as proportional to the squared norm (“complexity”
) of the predictor 
\begin_inset Formula $\boldsymbol{u}$
\end_inset

.
 We can sketch the proof.
 For instance, let us consider the non-cyclic case.
 On each trial t there is a projection to a single affine set 
\begin_inset Formula $\mathcal{U}_{t}=af\left(U_{t}\right)$
\end_inset

.
 Since we showed that for all 
\begin_inset Formula $\boldsymbol{u}\in af\left(\left\{ 1,\,\cdots,\,l\right\} \right)$
\end_inset

, we have 
\begin_inset Formula $\sum_{t=1}^{l}\left\Vert \boldsymbol{w}_{t+1}-\boldsymbol{w}_{t}\right\Vert ^{2}\leq\left\Vert \boldsymbol{u}-\boldsymbol{w}_{t}\right\Vert ^{2}-\left\Vert \boldsymbol{u}-\boldsymbol{w}_{l+1}\right\Vert ^{2}$
\end_inset

.
 If a mistake has occurred at trial 
\begin_inset Formula $t$
\end_inset

, 
\begin_inset Formula $y_{y}=\left\langle \boldsymbol{w}_{t+1},\,\boldsymbol{x}_{t}\right\rangle $
\end_inset

, then 
\begin_inset Formula $1\leq\left|\left\langle \boldsymbol{w}_{t},\,\boldsymbol{x}_{t}\right\rangle -y_{t}\right|\leq\left|\left\langle \boldsymbol{w}_{t}-\boldsymbol{w}_{t+1},\,\boldsymbol{x}_{t}\right\rangle -y_{t}\right|\leq\left\Vert \boldsymbol{w}_{t}-\boldsymbol{w}_{t+1}\right\Vert \left\Vert \boldsymbol{x}_{t}\right\Vert $
\end_inset

.
 Therefore, 
\begin_inset Formula $1\leq\left\Vert \boldsymbol{w}_{t}-\boldsymbol{w}_{t+1}\right\Vert \left\Vert \boldsymbol{x}_{t}\right\Vert $
\end_inset

, which implies 
\begin_inset Formula $\left\Vert \boldsymbol{x}_{t}\right\Vert ^{-1}\leq\left\Vert \boldsymbol{w}_{t}-\boldsymbol{w}_{t+1}\right\Vert $
\end_inset

.
 Hence, 
\begin_inset Formula $\sum_{t\in M}\left\Vert \boldsymbol{x}_{t}\right\Vert ^{-2}\leq\sum_{t=1}^{l}\left\Vert \boldsymbol{w}_{t}-\boldsymbol{w}_{t+1}\right\Vert ^{2}$
\end_inset

.
 Combining them, we have 
\begin_inset Formula $\sum_{t\in M}\left\Vert \boldsymbol{x}_{t}\right\Vert ^{-2}\leq\left\Vert \boldsymbol{u}-\boldsymbol{w}_{t}\right\Vert ^{2}+\left\Vert \boldsymbol{u}-\boldsymbol{w}_{l+1}\right\Vert ^{2}$
\end_inset

, which gives us 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\left|M\right|\leq\left(\left\Vert \boldsymbol{u}-\boldsymbol{w}_{t}\right\Vert ^{2}-\left\Vert \boldsymbol{u}-\boldsymbol{w}_{l+1}\right\Vert ^{2}\right)\left(\frac{1}{\left|M\right|}\sum_{i}\left(\left\Vert \boldsymbol{x}_{i}\right\Vert ^{2}\right)^{-1}\right)^{-1}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
Since the 
\begin_inset Formula $\left\Vert \boldsymbol{u}-\boldsymbol{w}_{l+1}\right\Vert ^{2}>0$
\end_inset

 we can neglect the second term, yielding the formula 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:3.21"

\end_inset

.
 The argument for the cyclic case follows the above argument except that
 the left hand side of the analogous inequality to 
\begin_inset Formula $\sum_{t=1}^{l}\left\Vert \boldsymbol{w}_{t+1}-\boldsymbol{w}_{t}\right\Vert ^{2}\leq\left\Vert \boldsymbol{u}-\boldsymbol{w}_{t}\right\Vert ^{2}-\left\Vert \boldsymbol{u}-\boldsymbol{w}_{l+1}\right\Vert ^{2}$
\end_inset

 contains sub-terms due to repeatedly cycling through the past examples,
 which is line 2 in algorithm 
\begin_inset CommandInset ref
LatexCommand ref
reference "alg:3.5"

\end_inset

.
 These terms may all be lower-bounded by zero except the term corresponding
 to the first projection, which is line 1 in algorithm 
\begin_inset CommandInset ref
LatexCommand ref
reference "alg:3.5"

\end_inset

.
 Note that 
\begin_inset Formula $af\left(U\right)\in hs\left(U\right)$
\end_inset

, which implies that the bound is tighter using the non-cyclic strategy.
\end_layout

\begin_layout Standard
The arguments in Herbster show that the above algorithm converges to an
 element in the feasible set, and that we can obtain an explicit bound on
 the number of mistakes.
 However, there is no guarantee that this algorithm indeed converges to
 the minimizer of (3.5).
 We claim this is the case.
 In order to make a heuristic argument for why this is the case, we argue
 that at each step of the iteration, we have 
\begin_inset Formula $||w_{t+1}||^{2}\leq||w_{t}||^{2}.$
\end_inset

 This is very straightforward using the Pythagorean Inequality, since we
 have for all 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\forall u\in af(U_{t}),||u-w_{t}||^{2}\ge||u-P(U_{t},w_{t})||^{2}+||P(U_{t},w_{t})-w_{t}||^{2}
\]

\end_inset


\end_layout

\begin_layout Standard
Letting 
\begin_inset Formula $u=\boldsymbol{0},$
\end_inset

we obtain 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
||w_{t}||^{2}\ge||P(U_{t},w_{t})||^{2}+||P(U_{t},w_{t})-w_{t}||^{2}=||w_{t+1}||^{2}+||w_{t+1}-w_{t}||^{2}\ge||w_{t+1}||^{2}
\]

\end_inset


\end_layout

\begin_layout Standard
Thus the objective function is always non-decreasing, and strictly decreasing
 whenever there is an mistake made, i.e.
 when there is an update and 
\begin_inset Formula $w_{t+1}\neq w_{t}$
\end_inset

.
 Since the convergence of the algorithm is guaranteeed, this means that
 the algorithm will converge to a local minimum.
 To complete the full proof, we would need to prove th subtle but non-trivial
 fact that the objective will not become stuck in a local minimum.
 The proof of this fact is much more in-depth, so we don't summarize it
 here.
\end_layout

\begin_layout Subsection
Online learning over graphs
\end_layout

\begin_layout Standard
As shown in section 3.1, the reproducing kernel of the Hilbert space defined
 over the graph is given by the pseudoinverse of the Laplacian matrix, 
\begin_inset Formula $\boldsymbol{L}^{+}.$
\end_inset

 Ths suggests that we can apply the above projection algorithm, with its
 associated guarantees, by letting the input vector 
\begin_inset Formula $x_{t}$
\end_inset

be replaced by the t-th row of 
\begin_inset Formula $\boldsymbol{L}^{+},$
\end_inset

 
\begin_inset Formula $K_{t}$
\end_inset

.
 We saw in the previous section that the number of cumulative mistakes is
 upper bounded by the product of two terms: one involving 
\begin_inset Formula $\left\Vert \boldsymbol{u}-\boldsymbol{w}_{1}\right\Vert ^{2}$
\end_inset

, and the other involving the harmonic mean of the input patterns that were
 misclassified.
 Since the initial weight vector 
\begin_inset Formula $\boldsymbol{w}$
\end_inset

 is arbitrary, we will consider the first term to be bounded more generally
 by 
\begin_inset Formula $\left\Vert \boldsymbol{u}\right\Vert ^{2}$
\end_inset

.
 In the following analysis, we will derive upper bounds for each of these
 terms using the properties of our graph, and of the labelings given to
 us.
\end_layout

\begin_layout Standard
Claim:
\end_layout

\begin_layout Standard
Let 
\begin_inset Formula $g^{+}$
\end_inset

 be the positively labeled vertices, and let 
\begin_inset Formula $g^{-}$
\end_inset

be the negatively labeled vertices.
 Let 
\begin_inset Formula $|g|^{+}=n^{+},|g^{-}|=n^{-},$
\end_inset

 and assume WLOG that 
\begin_inset Formula $n^{+}>=n^{-}.$
\end_inset

 then there exists a 
\begin_inset Formula $u\in af(U_{t})$
\end_inset

 such that
\end_layout

\begin_layout Standard
\begin_inset Formula $||u||^{2}\leq\partial(w^{+},g^{-})*(1+\frac{n_{+}}{n_{-}})^{2}$
\end_inset


\end_layout

\begin_layout Standard
Proof.
 To show this, we consider the vector 
\begin_inset Formula $f$
\end_inset

, with 
\begin_inset Formula $f_{i}=1$
\end_inset

 if 
\begin_inset Formula $g_{i}=1$
\end_inset

, and 
\begin_inset Formula $f_{i}=-\frac{n^{+}}{n^{-}}$
\end_inset

 if 
\begin_inset Formula $g_{i}=-1.$
\end_inset

Now suppose we order our vertices so that the first 
\begin_inset Formula $n^{+}$
\end_inset

vertices are those in 
\begin_inset Formula $g^{+},$
\end_inset

 while the last 
\begin_inset Formula $n_{-}$
\end_inset

vertices are those in 
\begin_inset Formula $g^{-}.$
\end_inset

 Now consider the Laplacian 
\begin_inset Formula $L$
\end_inset

 of this matrix with the vertices ordered in such a way.
 We are interested in the sums of the entries in the off-diagonal blocks,
 the sums of whose entries are each the negative of the number of intra-partitio
n edges.
 We will denote this sum by S, as opposed to the sub-matrix 
\series bold
S.
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\begin{aligned}L & =\left[\begin{array}{cc}
\boldsymbol{A} & -\boldsymbol{S}\\
\boldsymbol{-S}^{T} & \boldsymbol{B}
\end{array}\right]\end{aligned}
,
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
Note that the sum of entries in the left partition of the matrix for any
 given row is always the negative of the sum to the right of the partition.
 Using this and the fact that the sum of entries in both 
\series bold
S
\series default
 and 
\begin_inset Formula $\boldsymbol{\boldsymbol{S}^{T}}$
\end_inset

 are both equal to S, we find after some algebra that
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
||f||^{2}=<f,f>=f^{T}Lf=(1+\frac{n^{+}}{n^{-}})^{2}\partial(g^{+},g^{-})
\]

\end_inset


\end_layout

\begin_layout Standard
Since this equality holds for at least one 
\begin_inset Formula $u\in U_{t}$
\end_inset

 since the bound in (3.17) holds for all 
\begin_inset Formula $u\in U_{t}$
\end_inset

we have 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\left|M\right|\leq(1+\frac{n^{+}}{n^{-}})^{2}\partial(g^{+},g^{-})\left(\frac{1}{\left|M\right|}\sum_{i}\left(\left\Vert \boldsymbol{x}_{i}\right\Vert ^{2}\right)^{-1}\right)^{-1}.
\]

\end_inset


\end_layout

\begin_layout Standard
While this bound is the tightest given by the authors, the third term on
 the RHS is not very intuitive.
 However, we can bound this term in terms of the more intuitive quantity
 
\begin_inset Formula $\max_{1\leq t\leq l}||x_{t}||^{2}$
\end_inset

.
 Note that the function 
\begin_inset Formula $\frac{1}{x}$
\end_inset

 is convex for 
\begin_inset Formula $x>0$
\end_inset

, so by Jensen's Inequality it follows that
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\left(\sum_{i}\frac{1}{\left|M\right|}\left(\left\Vert \boldsymbol{x}_{i}\right\Vert ^{2}\right)^{-1}\right)^{-1}\leq\sum_{i}((\frac{1}{\left|M\right|}\left\Vert \boldsymbol{x}_{i}\right\Vert ^{2})^{-1})^{^{-1}}=\sum_{i}\frac{1}{\left|M\right|}\left\Vert \boldsymbol{x}_{i}\right\Vert ^{2}\leq\max_{1\leq t\leq l}||x_{t}||^{2}
\]

\end_inset


\end_layout

\begin_layout Standard
To simplify notation, let 
\begin_inset Formula $\beta=(1+\frac{n^{+}}{n^{-}})^{2}$
\end_inset

 be called the 
\begin_inset Quotes eld
\end_inset

imbalance
\begin_inset Quotes erd
\end_inset

 of the graph, and let 
\begin_inset Formula $\Phi(G)=\partial(g^{+},g^{-})$
\end_inset

 be called the 
\begin_inset Quotes eld
\end_inset

cut-size
\begin_inset Quotes erd
\end_inset

 of the graph.
 Asuming that there exists some 
\begin_inset Formula $r$
\end_inset

 such that 
\begin_inset Formula $||x_{t}||^{2}\leq r^{2}$
\end_inset

for all 
\begin_inset Formula $1\leq t\leq l,$
\end_inset

 we have a bound on thh mistakes:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\left|M\right|\leq\beta\Phi(G)r^{2}
\]

\end_inset


\end_layout

\begin_layout Standard
Note that the dependence on 
\begin_inset Formula $r^{2}$
\end_inset

 also appears on the mistake bound for the perceptron algorithm.
 However we can still make the bound more explicit.
 Recall thatby the reproducing kernel property, we have that 
\begin_inset Formula $K_{ii}=\left\langle \boldsymbol{K}_{i},\,\boldsymbol{K_{i}}\right\rangle =||K_{i}||^{2}.$
\end_inset

 Therefore we seek an upper bound on 
\begin_inset Formula $K_{ii}$
\end_inset

 in terms of the properties of the graph.
 By the Rayleigh-Reisz characterization of eigenvalues, we have that for
 all 
\begin_inset Formula $f\in H$
\end_inset

, 
\begin_inset Formula $f^{T}Lf\ge\min_{f\in H}\left\langle f,\,f\right\rangle =min_{1\leq i\leq n}|\lambda_{i}|\left\langle f,\,f\right\rangle =\lambda_{2}\left\langle f,\,f\right\rangle $
\end_inset

.
 The smallest eigenvalue 
\begin_inset Formula $\lambda_{2}>0$
\end_inset

 is often referred to a the 
\begin_inset Quotes eld
\end_inset

Fielder Value
\begin_inset Quotes erd
\end_inset

 or 
\begin_inset Quotes eld
\end_inset

Algebraic Connectivity
\begin_inset Quotes erd
\end_inset

 of the graph G.
 Thus we have 
\begin_inset Formula $K_{ii}\ge\lambda_{2}K_{p}\,\cdot K_{p}=\lambda_{2}\sum_{j=1}^{n}K_{jj}^{2}\ge\lambda_{2}K_{ii}^{2}.$
\end_inset

In all, this implies that 
\begin_inset Formula $K_{jj}\le\frac{1}{\lambda_{2}}$
\end_inset

, and our inequality becomes 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\left|M\right|\leq\beta\Phi(G)\frac{1}{\lambda_{2}}
\]

\end_inset


\end_layout

\begin_layout Subsection
Drawbacks with the above approach
\end_layout

\begin_layout Standard
Large diameter gaphs
\end_layout

\begin_layout Standard
With that in mind, we can introduce the mathematical framework for the online
 projection algorithm of 
\begin_inset CommandInset citation
LatexCommand citet
key "HerbsterPontilWainer2005"

\end_inset

.
 Herbster seeks to minimize
\end_layout

\begin_layout Standard
–projection description (tom)
\end_layout

\begin_layout Standard
–learning bounds (Paul)
\end_layout

\begin_layout Standard
Extension of the herbster paper – use different laplacians simulataneously
 by considering them each as different kernels in some set of kernels, and
 then applying the objective function for multiple kernels (need to flesh
 out this idea)
\end_layout

\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage clearpage
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
renewcommand
\backslash
refname{Bibliography}
\end_layout

\end_inset


\begin_inset Note Comment
status open

\begin_layout Plain Layout
Leave this inside the comment:
\begin_inset CommandInset bibtex
LatexCommand bibtex
bibfiles "bibliography"
options "plain"

\end_inset


\end_layout

\begin_layout Plain Layout
LyX still provides its citation dialogs, but does not export bibtex commands
 to LaTeX.
 Bibliography database has to be loaded twice: in the document preamble
 for biblatex, and through the above button for LyX support.
\end_layout

\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
nocite{*}
\end_layout

\begin_layout Plain Layout


\backslash
printbibliography
\end_layout

\begin_layout Plain Layout


\backslash
addcontentsline{toc}{chapter}{Bibliography}
\end_layout

\end_inset


\end_layout

\end_body
\end_document
