#LyX 2.2 created this file. For more info see http://www.lyx.org/
\lyxformat 508
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass extarticle
\begin_preamble
\usepackage{lastpage}

%\usepackage{tocbibind}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\minimise}{minimise}
\DeclareMathOperator*{\maximise}{maximise}
\DeclareMathOperator*{\sign}{sign}
%\usepackage{babel}

\usepackage[T1]{fontenc}
\usepackage{accanthis}
\usepackage{indentfirst}

\fancyhead{}
\fancyhead[LE,RO]{\textsl{\rightmark}}
\fancyhead[LO,RE]{\textsl{\leftmark}}
\fancyfoot{}
\fancyfoot[LE,RO]{Page \thepage\ of \pageref{LastPage}}
\fancyfoot[LO,RE]{\includegraphics[width=4cm]{NYU_logo.png}}
\renewcommand{\headrulewidth}{0.5pt}
\renewcommand{\footrulewidth}{0.5pt}

%\usepackage{fancyhdr}
%\usepackage{lastpage}
%\fancyhead{}
%\renewcommand{\headrulewidth}{0.4pt}
%\rhead{LINTILHAC AND LI}
%\lhead{CO-INTEGRATION KERNEL} 
%\fancyfoot{}
%\renewcommand{\footrulewidth}{0.4pt}
%\rfoot{\includegraphics[width=4cm]{NYU_logo.png}}
%\lfoot{Page \thepage\ of \pageref{LastPage}} 
%\renewcommand{\footrulewidth}{0.4pt}

%\usepackage{fancyhdr}
%\let\ps@plain\ps@fancy
%\fancyhf{}
%\renewcommand{\headrulewidth}{0pt}
%\rfoot{\includegraphics[width=4cm]{NYU_logo.png}}
%\fancyfoot[R]{Page \thepage\ of \pageref{LastPage}}
%\fancyfoot[L]{\includegraphics[width=4cm]{NYU_logo.png}}

\usepackage{color}
\definecolor{nyupurple}{RGB}{82,46,145}
\definecolor{matlabcomment}{rgb}{0.13,0.54,0.13}
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=nyupurple,
    filecolor=nyupurple,      
    urlcolor=nyupurple,
    citecolor=nyupurple,
    hyperfootnotes=true,
}

%\usepackage{chngcntr}
%\usepackage{footmisc}
%\counterwithout{footnote}{chapter}
%\renewcommand{\thefootnote}{\fnsymbol{footnote}}
%\footnote[num]{text}
%\renewcommand\thefootnote{\textcolor{red}{\arabic{footnote}}}
%\usepackage[dvipsnames]{xcolor}
%\usepackage{alltt}
%\definecolor{string}{rgb}{0.7,0.0,0.0}
%\definecolor{comment}{rgb}{0.13,0.54,0.13}
%\definecolor{keyword}{rgb}{0.0,0.0,1.0}

\usepackage[minbibnames=1, maxbibnames=99, backend=biber,style=authoryear,dashed=false,natbib=true,url=true,bibencoding=utf8]{biblatex}
% add bibliography database
\addbibresource{bibliography.bib}
%more options
\ExecuteBibliographyOptions{sorting=nty,backref=true,doi=true}

\usepackage[section]{algorithm}
\usepackage{algorithmicx}
\usepackage{algpseudocode}
\renewcommand{\thealgorithm}{\arabic{section}.\arabic{algorithm}} 
\end_preamble
\use_default_options true
\begin_modules
theorems-ams
eqs-within-sections
figs-within-sections
tabs-within-sections
theorems-sec
shapepar
enumitem
\end_modules
\maintain_unincluded_children false
\begin_local_layout
Format 60
Provides natbib 1
\end_local_layout
\language british
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command biber
\index_command default
\float_placement h
\paperfontsize 9
\spacing onehalf
\use_hyperref true
\pdf_bookmarks true
\pdf_bookmarksnumbered true
\pdf_bookmarksopen true
\pdf_bookmarksopenlevel 5
\pdf_breaklinks true
\pdf_pdfborder false
\pdf_colorlinks false
\pdf_backref false
\pdf_pdfusetitle true
\papersize a4paper
\use_geometry true
\use_package amsmath 2
\use_package amssymb 2
\use_package cancel 2
\use_package esint 2
\use_package mathdots 2
\use_package mathtools 2
\use_package mhchem 2
\use_package stackrel 2
\use_package stmaryrd 2
\use_package undertilde 2
\cite_engine natbib
\cite_engine_type authoryear
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 3cm
\topmargin 3cm
\rightmargin 3cm
\bottommargin 3cm
\footskip 1.25cm
\secnumdepth 5
\tocdepth 5
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 2
\paperpagestyle fancy
\listings_params "language=Matlab,basicstyle={\fontfamily{pcr}\selectfont\small},keywordstyle={\color{blue}},commentstyle={\color{matlabcomment}\itshape},emphstyle={\color{red}},stringstyle={\color{magenta}},identifierstyle={\color{black}},numbers=left,numberstyle={\scriptsize},breaklines=true"
\bullet 0 1 31 -1
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title

\series bold
FINAL PROJECT FOR THE COURSE ADVANCED FOUNDATIONS OF MACHINE LEARNING: ONLINE
 LEARNING OVER GRAPHS
\begin_inset Foot
status open

\begin_layout Plain Layout

\size small
\begin_inset CommandInset href
LatexCommand href
name "New York University"
target "http://www.nyu.edu"

\end_inset

 
\begin_inset CommandInset href
LatexCommand href
name "Courant Institute of Mathematical Sciences"
target "https://www.cs.nyu.edu/home/index.html"

\end_inset

.
 
\begin_inset CommandInset href
LatexCommand href
name "Spring 2017"
target "http://www.cs.nyu.edu/~mohri/aml17/"

\end_inset

.
 Professor 
\begin_inset CommandInset href
LatexCommand href
name "Mehryar Mohri"
target "http://www.cs.nyu.edu/~mohri/"

\end_inset

, Ph.D.
\end_layout

\end_inset


\end_layout

\begin_layout Author
PAUL SOPHER LINTILHAC
\begin_inset Foot
status open

\begin_layout Plain Layout
New York University 
\begin_inset CommandInset href
LatexCommand href
name "School of Engineering"
target "http://engineering.nyu.edu/academics/departments/mathematics"

\end_inset

.
 psl274@nyu.edu
\end_layout

\end_inset

 
\begin_inset Formula $\;\;$
\end_inset

AND 
\begin_inset Formula $\;$
\end_inset

THOMAS NANFENG LI
\begin_inset Foot
status open

\begin_layout Plain Layout
New York University 
\begin_inset CommandInset href
LatexCommand href
name "School of Engineering"
target "http://engineering.nyu.edu/academics/departments/mathematics"

\end_inset

.
 nl747@nyu.edu
\end_layout

\end_inset

 
\end_layout

\begin_layout Date
MAY 9
\begin_inset Formula $^{\text{TH}}$
\end_inset

, 2017
\end_layout

\begin_layout Section
Introduction to Graph
\end_layout

\begin_layout Subsection
Concepts and Definitions
\end_layout

\begin_layout Standard
We first summarise some key concepts of graph theory, for more detailed
 knowledge, we refer to 
\begin_inset CommandInset citation
LatexCommand citet
key "Bapat2014"

\end_inset

.
 A 
\begin_inset Formula $\boldsymbol{simple}$
\end_inset

 
\begin_inset Formula $\boldsymbol{graph}$
\end_inset

, that is, graph without loops and parallel edges, 
\begin_inset Formula $G\left(V,\,E\right)$
\end_inset

 consists of a finite set of 
\begin_inset Formula $\boldsymbol{vertices}$
\end_inset

 
\begin_inset Formula $V(G)$
\end_inset

 and a set of 
\begin_inset Formula $\boldsymbol{edges}$
\end_inset

 
\begin_inset Formula $E(G)$
\end_inset

 consisting of distinct, unordered pairs of vertices.
 
\begin_inset Formula $V(G)=\left\{ v_{1},\,v_{2},\,\cdots,\,v_{n}\right\} $
\end_inset

 is called the vertex set with 
\begin_inset Formula $n=\left|V(G\right|$
\end_inset

, 
\begin_inset Formula $E(G)=\left\{ e_{ij}\right\} $
\end_inset

 is called the edge set with 
\begin_inset Formula $m=\left|E(G)\right|$
\end_inset

.
 An edge 
\begin_inset Formula $e_{ij}$
\end_inset

 connects vertices 
\begin_inset Formula $v_{i}$
\end_inset

 and 
\begin_inset Formula $v_{j}$
\end_inset

 if they are 
\begin_inset Formula $\boldsymbol{adjacent}$
\end_inset

 or neighbours, which is denoted by 
\begin_inset Formula $v_{i}\sim v_{j}$
\end_inset

.
 The number of neighbours of a vertex 
\begin_inset Formula $v$
\end_inset

 is called the 
\begin_inset Formula $\boldsymbol{degree}$
\end_inset

 of 
\begin_inset Formula $v$
\end_inset

 and is denoted by 
\begin_inset Formula $d\left(v\right)$
\end_inset

, therefore, for each vertex, 
\begin_inset Formula $d\left(v_{i}\right)=\sum_{v_{i}\sim v_{j}}e_{ij}$
\end_inset

.
 If all the vertices of a graph have the same degree, the graph is 
\begin_inset Formula $\boldsymbol{regula}$
\end_inset

r, the vertices of an 
\begin_inset Formula $\boldsymbol{Eulerian}$
\end_inset

 
\begin_inset Formula $\boldsymbol{Graph}$
\end_inset

 have even degree.
 A graph is 
\begin_inset Formula $\boldsymbol{complete}$
\end_inset

 if there is an edge between every pair of vertices.
 
\end_layout

\begin_layout Standard
\begin_inset Formula $H\left(G\right)$
\end_inset

 is a 
\begin_inset Formula $\boldsymbol{sub-graph}$
\end_inset

 of 
\begin_inset Formula $G$
\end_inset

 if 
\begin_inset Formula $V(H)\subseteq V(G)$
\end_inset

 and 
\begin_inset Formula $E(H)\subseteq E(G)$
\end_inset

.
 A sub-graph 
\begin_inset Formula $H\left(G\right)$
\end_inset

 is an 
\begin_inset Formula $\boldsymbol{induced}$
\end_inset

 
\begin_inset Formula $\boldsymbol{sub-graph}$
\end_inset

 of G if two vertices of 
\begin_inset Formula $V(H)$
\end_inset

 are adjacent if and only if they are adjacent in 
\begin_inset Formula $G$
\end_inset

.
 A 
\begin_inset Formula $\boldsymbol{clique}$
\end_inset

 is a complete sub-graph of a graph.
 A 
\begin_inset Formula $\boldsymbol{path}$
\end_inset

 of 
\begin_inset Formula $k$
\end_inset

 vertices is a sequence of 
\begin_inset Formula $k$
\end_inset

 distinct vertices such that consecutive vertices are adjacent.
 A 
\begin_inset Formula $\boldsymbol{cycle}$
\end_inset

 is a connected sub-graph where every vertex has exactly two neighbours.
 A graph containing no cycles is a 
\begin_inset Formula $\boldsymbol{forest}$
\end_inset

.
 A connected forest is a 
\begin_inset Formula $\boldsymbol{tree}$
\end_inset

.
\end_layout

\begin_layout Standard
We define incidence matrix of graph.
 Let 
\begin_inset Formula $G\left(V,\,E\right)$
\end_inset

 be a graph with 
\begin_inset Formula $V(G)=\left\{ v_{1},\,v_{2},\,\cdots,\,v_{n}\right\} $
\end_inset

 and 
\begin_inset Formula $E(G)=\left\{ e_{1},\,e_{2},\,\cdots,\,e_{m}\right\} $
\end_inset

.
 Suppose each edge of 
\begin_inset Formula $G\left(V,\,E\right)$
\end_inset

 is assigned an orientation, which is arbitrary but fixed.
 The vertex-edge 
\begin_inset Formula $\boldsymbol{incidence}$
\end_inset

 
\begin_inset Formula $\boldsymbol{matrix}$
\end_inset

 of 
\begin_inset Formula $G\left(V,\,E\right)$
\end_inset

, denoted by 
\begin_inset Formula $\boldsymbol{Q}(G)$
\end_inset

, is the 
\begin_inset Formula $n\times m$
\end_inset

 matrix defined as follows.
 The rows and the columns of 
\begin_inset Formula $\boldsymbol{Q}(G)$
\end_inset

 are indexed by 
\begin_inset Formula $V(G)$
\end_inset

 and 
\begin_inset Formula $E(G)$
\end_inset

, respectively.
 The 
\begin_inset Formula $\left(i,\,j\right)$
\end_inset

 entry of 
\begin_inset Formula $\boldsymbol{Q}(G)$
\end_inset

 is 
\begin_inset Formula $0$
\end_inset

 if vertex 
\begin_inset Formula $i$
\end_inset

 and edge 
\begin_inset Formula $e_{j}$
\end_inset

 are not incident, and otherwise it is 
\begin_inset Formula $-1$
\end_inset

 or 
\begin_inset Formula $1$
\end_inset

 according as 
\begin_inset Formula $e_{j}$
\end_inset

 originates or terminates at 
\begin_inset Formula $i$
\end_inset

, respectively.
 For instance, the incidence matrix 
\begin_inset Formula $\boldsymbol{Q}(G)$
\end_inset

 of the graph that is shown in figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:1.1"

\end_inset

 is 
\begin_inset Formula 
\begin{equation}
\begin{aligned}\boldsymbol{Q}(G) & =\left[\begin{array}{cccccc}
1 & -1 & 1 & 0 & 0 & 0\\
-1 & 0 & 0 & 1 & 0 & 0\\
0 & 1 & 0 & 0 & -1 & 0\\
0 & 0 & -1 & 0 & 0 & 1\\
0 & 0 & 0 & -1 & 1 & -1
\end{array}\right]\end{aligned}
.
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename 1-1.png
	scale 61

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Example of Incidence Matrix of Graph
\begin_inset CommandInset label
LatexCommand label
name "fig:1.1"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
We introduce adjacency matrix of graph.
 Let 
\begin_inset Formula $G\left(V,\,E\right)$
\end_inset

 be a graph with 
\begin_inset Formula $V(G)=\left\{ v_{1},\,v_{2},\,\cdots,\,v_{n}\right\} $
\end_inset

 and 
\begin_inset Formula $E(G)=\left\{ e_{1},\,e_{2},\,\cdots,\,e_{m}\right\} $
\end_inset

.
 The 
\begin_inset Formula $\boldsymbol{adjacency}$
\end_inset

 
\begin_inset Formula $\boldsymbol{matrix}$
\end_inset

 of 
\begin_inset Formula $G\left(V,\,E\right)$
\end_inset

, denoted by 
\begin_inset Formula $\boldsymbol{A}(G)$
\end_inset

, is the 
\begin_inset Formula $n\times n$
\end_inset

 matrix defined as follows.
 The rows and the columns of 
\begin_inset Formula $\boldsymbol{A}(G)$
\end_inset

 are indexed by 
\begin_inset Formula $V(G)$
\end_inset

.
 If 
\begin_inset Formula $i\neq j$
\end_inset

 then the 
\begin_inset Formula $\left(i,\,j\right)$
\end_inset

 entry of 
\begin_inset Formula $\boldsymbol{A}(G)$
\end_inset

 is 
\begin_inset Formula $0$
\end_inset

 for vertices 
\begin_inset Formula $i$
\end_inset

 and 
\begin_inset Formula $j$
\end_inset

 non-adjacent, and the 
\begin_inset Formula $\left(i,\,j\right)$
\end_inset

 entry is 1 for 
\begin_inset Formula $i$
\end_inset

 and 
\begin_inset Formula $j$
\end_inset

 adjacent.
 The 
\begin_inset Formula $\left(i,\,j\right)$
\end_inset

 entry of 
\begin_inset Formula $\boldsymbol{A}(G)$
\end_inset

 is 
\begin_inset Formula $0$
\end_inset

 for 
\begin_inset Formula $i=j=1,\,\cdots\,,n$
\end_inset

.
 For instance, the adjacency matrix 
\begin_inset Formula $\boldsymbol{A}(G)$
\end_inset

 of the graph that is shown in figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:1.2"

\end_inset

 is 
\begin_inset Formula 
\begin{equation}
\begin{aligned}\boldsymbol{A}\left(G\right) & =\left[\begin{array}{ccccc}
0 & 1 & 1 & 1 & 0\\
1 & 0 & 1 & 0 & 0\\
1 & 1 & 0 & 1 & 1\\
1 & 0 & 1 & 0 & 1\\
0 & 0 & 1 & 1 & 0
\end{array}\right]\end{aligned}
.
\end{equation}

\end_inset

Clearly 
\begin_inset Formula $\boldsymbol{A}$
\end_inset

 is a symmetric matrix with zeros on the diagonal.
 The 
\begin_inset Formula $\left(i,\,j\right)$
\end_inset

 entry of 
\begin_inset Formula $\boldsymbol{A}^{k}$
\end_inset

 is the number of walks of length 
\begin_inset Formula $k$
\end_inset

 from 
\begin_inset Formula $i$
\end_inset

 to 
\begin_inset Formula $j$
\end_inset

.
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename 1-2.png
	scale 70

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Example of Adjacency Matrix of Graph
\begin_inset CommandInset label
LatexCommand label
name "fig:1.2"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
We define degree matrix of graph.
 Let 
\begin_inset Formula $G\left(V,\,E\right)$
\end_inset

 be a graph with 
\begin_inset Formula $V(G)=\left\{ v_{1},\,v_{2},\,\cdots,\,v_{n}\right\} $
\end_inset

 and 
\begin_inset Formula $E(G)=\left\{ e_{1},\,e_{2},\,\cdots,\,e_{m}\right\} $
\end_inset

.
 The 
\begin_inset Formula $\boldsymbol{degree}$
\end_inset

 
\begin_inset Formula $\boldsymbol{matrix}$
\end_inset

 
\begin_inset Formula $\boldsymbol{D}(G)$
\end_inset

 for 
\begin_inset Formula $G\left(V,\,E\right)$
\end_inset

 is a 
\begin_inset Formula $n\times n$
\end_inset

 diagonal matrix defined as
\begin_inset Formula 
\[
\begin{aligned}\boldsymbol{D}\left(G\right)_{i,j} & \coloneqq\begin{cases}
d\left(v_{i}\right) & \text{if }i=j\\
0 & \text{otherwise}.
\end{cases}\end{aligned}
\]

\end_inset

According to this definition, the degree matrix of figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:1.2"

\end_inset

 is 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\begin{aligned}\boldsymbol{A}\left(G\right) & =\left[\begin{array}{ccccc}
3 & 0 & 0 & 0 & 0\\
0 & 2 & 0 & 0 & 0\\
0 & 0 & 4 & 0 & 0\\
0 & 0 & 0 & 3 & 0\\
0 & 0 & 0 & 0 & 2
\end{array}\right]\end{aligned}
.
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula $\boldsymbol{Weighted}$
\end_inset

 
\begin_inset Formula $\boldsymbol{graph}$
\end_inset

 
\begin_inset Formula $G\left(V,\,E,\,W\right)$
\end_inset

 is a graph with real edge weights given by 
\begin_inset Formula $w\,:\,E\rightarrow\mathbb{R}$
\end_inset

.
 Here, the weight 
\begin_inset Formula $w\left(e\right)$
\end_inset

 of an edge 
\emph on

\begin_inset Formula $e$
\end_inset


\emph default
 indicates the similarity of the incident vertices, and a missing edge correspon
ds to zero similarity.
 The 
\begin_inset Formula $\boldsymbol{weighted}$
\end_inset

 
\begin_inset Formula $\boldsymbol{adjacency}$
\end_inset

 
\begin_inset Formula $\boldsymbol{matrix}$
\end_inset

 
\begin_inset Formula $\boldsymbol{W}\left(G\right)$
\end_inset

 of the graph 
\begin_inset Formula $G\left(V,\,E,\,W\right)$
\end_inset

 is defined by
\begin_inset Formula 
\begin{equation}
\begin{aligned}\boldsymbol{W}_{ij} & \coloneqq\begin{cases}
w\left(e\right) & \text{if }e=\left(i,\,j\right)\in E\\
0 & \text{otherwise}.
\end{cases}\end{aligned}
.
\end{equation}

\end_inset

The weight matrix 
\begin_inset Formula $\boldsymbol{W}\left(G\right)$
\end_inset

 can be, for instance, the k-nearest neighbour matrix 
\begin_inset Formula $\boldsymbol{W}\left(G\right)_{ij}=1$
\end_inset

 if and only if vertex 
\begin_inset Formula $v_{i}$
\end_inset

 is among the 
\begin_inset Formula $k$
\end_inset

-nearest neighbours of 
\begin_inset Formula $v_{j}$
\end_inset

 or vice versa, and is 0 otherwise.
 Another typical weight matrix is given by the Gaussian kernel of width
 
\begin_inset Formula $\sigma$
\end_inset


\begin_inset Formula 
\begin{equation}
\boldsymbol{W}\left(G\right)_{ij}=e^{-\frac{\left\Vert v_{i}-v_{j}\right\Vert ^{2}}{2\sigma^{2}}}.\label{eq:1.5}
\end{equation}

\end_inset

 Then the 
\begin_inset Formula $\boldsymbol{degree}$
\end_inset

 
\begin_inset Formula $\boldsymbol{matrix}$
\end_inset

 
\begin_inset Formula $\boldsymbol{D}\left(G\right)$
\end_inset

 for weighted graph 
\begin_inset Formula $G\left(V,\,E,\,W\right)$
\end_inset

 is defined by 
\begin_inset Formula 
\begin{equation}
\boldsymbol{D}\left(G\right)_{i,i}\coloneqq\sum_{j}\boldsymbol{W}\left(G\right)_{ij}
\end{equation}

\end_inset


\end_layout

\begin_layout Subsection
Graph Laplacian
\end_layout

\begin_layout Standard
The graph Laplacian 
\begin_inset Formula $\boldsymbol{L}\left(G\right)$
\end_inset

 is defined in two different ways.
 The 
\begin_inset Formula $\boldsymbol{normalized}$
\end_inset

 
\begin_inset Formula $\boldsymbol{graph}$
\end_inset

 
\begin_inset Formula $\boldsymbol{Laplacian}$
\end_inset

 is 
\begin_inset Formula 
\begin{equation}
\boldsymbol{L}\left(G\right)\coloneqq\boldsymbol{I}-\boldsymbol{D}^{-\frac{1}{2}}\boldsymbol{W}\boldsymbol{D}^{-\frac{1}{2}},
\end{equation}

\end_inset

and the 
\begin_inset Formula $\boldsymbol{unnormalized}$
\end_inset

 
\begin_inset Formula $\boldsymbol{graph}$
\end_inset

 
\begin_inset Formula $\boldsymbol{Laplacian}$
\end_inset

 is 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\boldsymbol{L}\left(G\right)\coloneqq\boldsymbol{D}-\boldsymbol{W}.
\end{equation}

\end_inset

Let us consider an example to understand the graph Laplacian of the graph
 that is shown in figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:1.3"

\end_inset

.
 
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename 1-3.png
	lyxscale 50
	scale 50

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Real-Valued Functions on a Graph
\begin_inset CommandInset label
LatexCommand label
name "fig:1.3"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset

Suppose 
\begin_inset Formula $\boldsymbol{f}\,:\,V\rightarrow\mathbb{R}$
\end_inset

 is a real-valued function on the set of the vertices of graph 
\begin_inset Formula $G\left(V,\,E\right)$
\end_inset

 such that it assigns a real number to each graph vertex.
 Therefore, 
\begin_inset Formula $\boldsymbol{f}=\left(f\left(v_{1}\right),\,f\left(v_{2}\right),\,\cdots,\,f\left(v_{n}\right)\right)^{T}\in\mathbb{R}^{n}$
\end_inset

 is a vector indexed by the vertices of graph.
 Its adjacency matrix is 
\begin_inset Formula 
\begin{equation}
\begin{aligned}\boldsymbol{A} & =\left[\begin{array}{cccc}
0 & 1 & 1 & 0\\
1 & 0 & 1 & 1\\
1 & 1 & 0 & 0\\
0 & 1 & 0 & 0
\end{array}\right].\end{aligned}
\end{equation}

\end_inset

Hence, the eigenvectors of the adjacency matrix, 
\begin_inset Formula $\boldsymbol{A}\boldsymbol{x}=\lambda\boldsymbol{x}$
\end_inset

, can be viewed as eigenfunctions 
\begin_inset Formula $\boldsymbol{A}\boldsymbol{f}=\lambda\boldsymbol{f}$
\end_inset

.
 The adjacency matrix can be viewed as an operator
\begin_inset Formula 
\begin{equation}
\begin{aligned}\boldsymbol{g} & =\boldsymbol{A}\boldsymbol{f}\\
g\left(i\right) & =\sum_{i\sim j}f\left(j\right),
\end{aligned}
\end{equation}

\end_inset

and it can also be viewed as a quadratic form
\begin_inset Formula 
\begin{equation}
\boldsymbol{f}^{T}\boldsymbol{A}\boldsymbol{f}=\sum_{e_{ij}}f\left(i\right)f\left(j\right).
\end{equation}

\end_inset

Assume that each edge in the graph have an arbitrary but fixed orientation,
 which is shown in figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:1.4"

\end_inset

.
 Then the incidence matrix of the graph is
\begin_inset Formula 
\begin{equation}
\begin{aligned}\boldsymbol{Q} & =\left[\begin{array}{cccc}
-1 & 1 & 0 & 0\\
1 & 0 & -1 & 0\\
0 & -1 & 1 & 0\\
0 & -1 & 0 & 1
\end{array}\right]\end{aligned}
.
\end{equation}

\end_inset


\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename 1-4.png
	lyxscale 50
	scale 50

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Orientation of the Graph
\begin_inset CommandInset label
LatexCommand label
name "fig:1.4"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset

 Therefore the co-boundary mapping of the graph 
\begin_inset Formula $\boldsymbol{f}\rightarrow\boldsymbol{Q}\boldsymbol{f}$
\end_inset

 implies 
\begin_inset Formula $\left(\boldsymbol{Q}\boldsymbol{f}\right)\left(e_{ij}\right)=f\left(v_{j}\right)-f\left(v_{i}\right)$
\end_inset

 is
\begin_inset Formula 
\begin{equation}
\begin{aligned}\left[\begin{array}{c}
f\left(2\right)-f\left(1\right)\\
f\left(1\right)-f\left(3\right)\\
f\left(3\right)-f\left(2\right)\\
f\left(4\right)-f\left(2\right)
\end{array}\right] & =\left[\begin{array}{cccc}
-1 & 1 & 0 & 0\\
1 & 0 & -1 & 0\\
0 & -1 & 1 & 0\\
0 & -1 & 0 & 1
\end{array}\right]\left[\begin{array}{c}
f\left(1\right)\\
f\left(2\right)\\
f\left(3\right)\\
f\left(4\right)
\end{array}\right]\end{aligned}
.
\end{equation}

\end_inset

If we let 
\begin_inset Formula 
\begin{equation}
\boldsymbol{L}=\boldsymbol{Q}^{T}\boldsymbol{Q},
\end{equation}

\end_inset

then we have 
\begin_inset Formula 
\begin{equation}
\left(\boldsymbol{L}\boldsymbol{f}\right)\left(v_{i}\right)=\sum_{v_{i}\sim v_{j}}\left[f\left(v_{i}\right)-f\left(v_{j}\right)\right].
\end{equation}

\end_inset

Hence, the connection between the Laplacian and the adjacency matrices is
 
\begin_inset Formula 
\begin{equation}
\boldsymbol{L}=\boldsymbol{D}-\boldsymbol{A}=\left[\begin{array}{cccc}
2 & -1 & -1 & 0\\
-1 & 3 & -1 & -1\\
-1 & -1 & 2 & 0\\
0 & -1 & 0 & 1
\end{array}\right],\label{eq:1.16}
\end{equation}

\end_inset

where the degree matrix 
\begin_inset Formula $\boldsymbol{D}$
\end_inset

 is 
\begin_inset Formula 
\begin{equation}
\boldsymbol{D}=\left[\begin{array}{cccc}
2 & 0 & 0 & 0\\
0 & 3 & 0 & 0\\
0 & 0 & 2 & 0\\
0 & 0 & 0 & 1
\end{array}\right].
\end{equation}

\end_inset

If we consider undirected weighted graphs, which is each edge 
\begin_inset Formula $e_{ij}$
\end_inset

 is weighted by 
\begin_inset Formula $w_{ij}$
\end_inset

, then the Laplacian as an operator is 
\begin_inset Formula 
\begin{equation}
\left(\boldsymbol{L}\boldsymbol{f}\right)\left(v_{i}\right)=\sum_{v_{i}\sim v_{j}}w_{ij}\left[f\left(v_{i}\right)-f\left(v_{j}\right)\right].
\end{equation}

\end_inset

Its quadratic form is 
\begin_inset Formula 
\begin{equation}
\boldsymbol{f}^{T}\boldsymbol{L}\boldsymbol{f}=\frac{1}{2}\sum_{e_{ij}}w_{ij}\left[f\left(v_{i}\right)-f\left(v_{j}\right)\right]^{2}.
\end{equation}

\end_inset

The intuition behind a Laplacian matrix is the following.
 If, for instance, we apply the Laplacian operator of formula 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:1.16"

\end_inset

 to the real-valued functions 
\begin_inset Formula $\boldsymbol{f}=\left(f\left(v_{1}\right),\,f\left(v_{2}\right),\,f\left(v_{3}\right),\,f\left(v_{4}\right)\right)^{T}$
\end_inset

 of the set of the vertices of graph 
\begin_inset Formula $G\left(V,\,E\right)$
\end_inset

, we have
\begin_inset Formula 
\begin{equation}
\begin{aligned}\left(\boldsymbol{L}\boldsymbol{f}\right)\left(v_{i}\right) & =\left[\begin{array}{cccc}
2 & -1 & -1 & 0\\
-1 & 3 & -1 & -1\\
-1 & -1 & 2 & 0\\
0 & -1 & 0 & 1
\end{array}\right]\left[\begin{array}{c}
f\left(v_{1}\right)\\
f\left(v_{2}\right)\\
f\left(v_{3}\right)\\
f\left(v_{4}\right)
\end{array}\right].\end{aligned}
\end{equation}

\end_inset

For simplicity, let us only look at the first element
\begin_inset Formula 
\begin{equation}
\begin{aligned}\left(\boldsymbol{L}\boldsymbol{f}\right)\left(v_{i}\right)_{1} & =\left[\begin{array}{cccc}
2 & -1 & -1 & 0\end{array}\right]\left[\begin{array}{c}
f\left(v_{1}\right)\\
f\left(v_{2}\right)\\
f\left(v_{3}\right)\\
f\left(v_{4}\right)
\end{array}\right]\\
 & =2f\left(v_{1}\right)-f\left(v_{2}\right)-f\left(v_{3}\right)\\
 & =-\left[f\left(v_{2}\right)-2f\left(v_{1}\right)+f\left(v_{3}\right)\right]\\
 & =-\left[f\left(v_{2}\right)-f\left(v_{1}\right)-f\left(v_{1}\right)+f\left(v_{3}\right)\right]
\end{aligned}
\end{equation}

\end_inset

If we label 
\begin_inset Formula $f\left(v_{1}\right)=f_{k}$
\end_inset

, 
\begin_inset Formula $f\left(v_{2}\right)=f_{k+1}$
\end_inset

, and 
\begin_inset Formula $f\left(v_{3}\right)=f_{k-1}$
\end_inset

, then we have
\begin_inset Formula 
\begin{equation}
\left(\boldsymbol{L}\boldsymbol{f}\right)\left(v_{i}\right)_{1}=-\left[f_{k+1}-2f_{k}+f_{k-1}\right].
\end{equation}

\end_inset

We recall that the second order derivative can be approximated by
\begin_inset Formula 
\begin{equation}
\begin{aligned}f^{''}\left(x\right) & =\frac{\frac{f\left(x+\Delta x\right)-f\left(x\right)}{\Delta x}-\frac{f\left(x\right)-f\left(x-\Delta x\right)}{\Delta x}}{\Delta x}\\
 & =\frac{f\left(x+\Delta x\right)-2f\left(x\right)+f\left(x-\Delta x\right)}{\left(\Delta x\right)^{2}}.
\end{aligned}
\end{equation}

\end_inset

Hence, we observe that the graph Laplacian is the negative numerator of
 the finite difference approximation of the second derivative.
 The Laplacian matrix 
\begin_inset Formula $\boldsymbol{L}$
\end_inset

 is symmetric and positive semi-definite, and it has 
\begin_inset Formula $n$
\end_inset

 non-negative, real-valued eigenvalues 
\begin_inset Formula $0=\lambda_{1}\leq\lambda_{2}\leq\cdots\leq\lambda_{n}$
\end_inset

.
 The number of 0 eigenvalues of the Laplacian matrix 
\begin_inset Formula $\boldsymbol{L}$
\end_inset

 is the number of connected components, because each connected component
 forms a block in the Laplacian matrix that only has edges within itself,
 and each block is the Laplacian for a small connected component and it
 has one zero eigenvalue, so the number of zeros is the number of blocks
 is the number of connected components.
\end_layout

\begin_layout Subsection
Graph Laplacian and Heat Equation
\end_layout

\begin_layout Standard
We can get more intuition about the basic objects in Graph learning by using
 the concept of conductance .
 As we saw in the previous section, the Laplacian is closely related to
 a second derivative, and it is no coincidence that the continuous version
 of the Laplacian appears in the heat equation.
 Let 
\begin_inset Formula $f$
\end_inset

 be some real-valued function defined over the nodes of the graph.
 If we think of 
\begin_inset Formula $f$
\end_inset

 as a hat distribution, then we can think of 
\begin_inset Formula $\boldsymbol{L}\boldsymbol{f}$
\end_inset

 roughly as the flux induced at each of the nodes by that distribution over
 the graph, based on the graph structure.
 Then the form 
\begin_inset Formula $\boldsymbol{g}^{T}\boldsymbol{L}\boldsymbol{f}$
\end_inset

 represents some weighted measurement of this flux with the weights given
 by 
\begin_inset Formula $\boldsymbol{g}$
\end_inset

.
 For example, if f is a binary vector representing the boundary of some
 open set on the graph 
\begin_inset Formula $\left\langle \boldsymbol{f},\,\boldsymbol{g}\right\rangle $
\end_inset

 would be the flux produced by 
\begin_inset Formula $\boldsymbol{g}$
\end_inset

 as measured on the boundary represented by 
\begin_inset Formula $\boldsymbol{f}$
\end_inset

.
 We can even use this formalism to quickly prove a version of Stokes' Theorem
 specialized to the graph setting.
 In particular, we can see that the quadratic form 
\begin_inset Formula $\boldsymbol{f}^{T}\boldsymbol{L}\boldsymbol{f}$
\end_inset

 can be thought of as a standard metric for the 
\begin_inset Quotes eld
\end_inset

total flux
\begin_inset Quotes erd
\end_inset

 induced over the graph by some initial heat distribution 
\begin_inset Formula $\boldsymbol{f}$
\end_inset

.
 In other words, 
\begin_inset Formula $\boldsymbol{f}^{T}\boldsymbol{L}\boldsymbol{f}$
\end_inset

 is a measure of the smoothness of the function 
\begin_inset Formula $\boldsymbol{f}$
\end_inset

 relative to the structure of the graph defined by 
\begin_inset Formula $L.$
\end_inset


\end_layout

\begin_layout Subsection
Label Propagation
\end_layout

\begin_layout Standard
Given the graph 
\begin_inset Formula $G\left(V,\,E,\,W\right)$
\end_inset

, and an initial state of some real-valued function 
\begin_inset Formula $\boldsymbol{f}\,:\,V\rightarrow\mathbb{R}$
\end_inset

 defined over the vertices of the graph, we may be interested in propagating
 this function over the graph at time steps 
\begin_inset Formula $t=0,\,1,\,2,\,\cdots,\,T$
\end_inset

 that is consistent with the graph structure.
 In the simplest possible case, we might be provided with the initial state
 
\begin_inset Formula $\boldsymbol{f}_{0}$
\end_inset

 and wish to see how these values propagate over the graph, as in a heat
 diffusion.
 While very simple, even scenario could be useful in settings such as fraud
 detection over a graph of accounts that are connected by their transactions.
 For example, we may have received knowledge that a small number of vertices
 in the graph are fraudulent, and we want to perform a quick analysis to
 see to what degree other vertices in the graph might be exposed to the
 risk emanating from these fraudulent accounts.
 
\end_layout

\begin_layout Standard
We proceed by deriving an update equation starting with two basic assumptions:
 First, we want our function 
\begin_inset Formula $\boldsymbol{f}_{t+1}$
\end_inset

 at each time step to be 
\begin_inset Quotes eld
\end_inset

close
\begin_inset Quotes erd
\end_inset

 to 
\begin_inset Formula $\boldsymbol{f}_{t}$
\end_inset

.
 Second, we want to impose the condition that 
\begin_inset Formula $\boldsymbol{f}_{t+1}$
\end_inset

 be a 
\begin_inset Quotes eld
\end_inset

smooth
\begin_inset Quotes erd
\end_inset

 function over the graph, in the sense of minimal total flux described above.
 Thus we seek to minimize
\begin_inset Formula 
\begin{equation}
C\left(\boldsymbol{f}_{t+1}\Bigl|\boldsymbol{f}_{t}\right)=\left(\boldsymbol{f}_{t+1}-\boldsymbol{f}_{t}\right){}^{T}\left(\boldsymbol{f}_{t+1}-\boldsymbol{f}_{t}\right)+\alpha\boldsymbol{f}_{t+1}^{T}\boldsymbol{L}\boldsymbol{f}_{t+1},
\end{equation}

\end_inset

where 
\begin_inset Formula $\boldsymbol{L}$
\end_inset

 is the non-normalized Laplacian, and 
\begin_inset Formula $\alpha$
\end_inset

 is a constant measuring the conductivity of the graph.
 Taking the derivative and setting it to zero yields
\begin_inset Formula 
\begin{equation}
\boldsymbol{f}_{t+1}=\boldsymbol{f}_{t}-\alpha\boldsymbol{L}\boldsymbol{f}_{t}.
\end{equation}

\end_inset

As an example, if we were to begin with the vector of initial values 
\begin_inset Formula $\hat{\boldsymbol{Y}}^{0}=e_{1}$
\end_inset

 representing a unit point density, the above recurrence relation would
 show how that density propagates across the graph.
 We could alternatively consider the update equation to be a discretization
 of a system evolving in continuous-time.
 Note that the above equation can be viewed as a discrete difference equation
\begin_inset Formula 
\begin{equation}
\boldsymbol{f}_{t+1}-\boldsymbol{f}_{t}=\delta\boldsymbol{f}_{t}=-\alpha\boldsymbol{L}\boldsymbol{f}_{t}.
\end{equation}

\end_inset

Taking the continuous limit of this diffusion yields
\begin_inset Formula 
\begin{equation}
\frac{d\boldsymbol{f}_{t}}{dt}=-\alpha\boldsymbol{L}\boldsymbol{f}_{t},\qquad\boldsymbol{f}_{t}(0)=\boldsymbol{f}_{0}.
\end{equation}

\end_inset

The solution to this ODE is given by
\begin_inset Formula 
\begin{equation}
\boldsymbol{f}_{t}=e^{-\alpha\boldsymbol{L}t}\boldsymbol{f}_{0}.
\end{equation}

\end_inset

We can now use this continuous system in our discretization in order to
 derive a discrete update formula
\begin_inset Formula 
\begin{equation}
\boldsymbol{f}_{t}=e^{-\alpha\boldsymbol{L}t}\boldsymbol{f}_{0}.
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
This system differs from the one described by the original difference equation
 in several ways.
 First, there is a closed-form solution for the value of each node at any
 point in time, rather than a recurrence relation, which should make the
 time-complexity much more efficient.
 But it turns out that what is gained in time efficiency is more than lost
 inamuch higher space complexity, and thus is not practical in applications
 with very large graphs.
 Consider the case where our graph contains 
\begin_inset Formula $100000$
\end_inset

 vertices, and all vertices are connected by a single path, with the degree
 of each node exactly equal to 
\begin_inset Formula $2$
\end_inset

.
 In this case the regular Laplacian matrix is extremely sparse, as only
 
\begin_inset Formula $0.00001$
\end_inset

 of the entries are non-zero.
 Using data structures designed specifically for extremely sparse matrices,
 storing this matrix and performing basic computations would be quite easy.
 On the other hand, the matrix 
\begin_inset Formula $e^{-\alpha\boldsymbol{L}t}$
\end_inset

 has strictly positive entries for all t, and therefore would require 
\begin_inset Formula $100000$
\end_inset

 times as much memory to store.
\end_layout

\begin_layout Standard
The underlying reason for this discrepancy is that whereas the continuous
 Laplacian establishes a dependency among all vertices within a given connected
 component of the graph, the discrete Laplacian 
\begin_inset Formula $\boldsymbol{L}\left(G\right)$
\end_inset

 only propagates to immediate neighbors, as any vertices 
\begin_inset Formula $v_{i}$
\end_inset

 and 
\begin_inset Formula $v_{j}$
\end_inset

, which are not connected by an edge will have value zero.
 To gain more intuition about this discrepancy, we recall that the matrix
 exponential is defined by its Taylor series
\begin_inset Formula 
\begin{equation}
e^{-\alpha\boldsymbol{L}t}=\sum_{\{i=0\}}^{\infty}\frac{\left(-\alpha\boldsymbol{L}t\right)^{i}}{i!}.
\end{equation}

\end_inset

Replacing our initial condition with 
\begin_inset Formula $\boldsymbol{f}_{t}$
\end_inset

 and final state with 
\begin_inset Formula $\boldsymbol{f}_{t+1}$
\end_inset

 and taking the first order approximation to this series gives
\begin_inset Formula 
\begin{equation}
e^{-\alpha\boldsymbol{L}t}\approx\boldsymbol{1}-\boldsymbol{L}.
\end{equation}

\end_inset

Plugging this approximation into our update formula for the continuous case
 yields
\begin_inset Formula 
\begin{equation}
\boldsymbol{f}_{t+1}=e^{-\alpha\boldsymbol{L}}\boldsymbol{f}_{t}\approx(\boldsymbol{1}-\boldsymbol{L})\boldsymbol{f}_{t}=\boldsymbol{f}_{t}-\alpha\boldsymbol{L}\boldsymbol{f}_{t},
\end{equation}

\end_inset

which matches exactly our formula in the discrete case.
 If we were to take the second order approximation, each step in our iteration
 would take into account not only immediate neighbours, but also neighbours
 of neighbours and so on as more terms from the Taylor series are kept.
 This in turn would reduce the total number of iteration needed to simulate
 the evolution o the system, so that we have achieved some of the time-efficienc
y of the continuous model.
 However, we need to pay for this more favourable time-complexity by computing
 and storing the matrix 
\begin_inset Formula $\boldsymbol{L}^{2}$
\end_inset

.
 Even if 
\begin_inset Formula $\boldsymbol{L}$
\end_inset

 is sparse, 
\begin_inset Formula $\boldsymbol{L}^{2}$
\end_inset

will have exponentially more non-zero entries, and will thus be much larger,
 and will also take much longer to compute.
 Therefore, if we want to optimize label propagation in practice, we may
 want to take as many term of the Taylor expansion as we can without exceeded
 our memory.
 
\end_layout

\begin_layout Subsection
Sustained Effects of Initial State
\end_layout

\begin_layout Standard
Now suppose that we wish to maintain the effects of our initial function
 over time.
 Then we may add a term to the objective function of the form 
\begin_inset Formula $\boldsymbol{f}_{t+1}^{T}\boldsymbol{f}_{0}.$
\end_inset

 This inner product measures the similarity between the updated function
 
\begin_inset Formula $\boldsymbol{f}_{t+1}$
\end_inset

 and the initial state vector 
\begin_inset Formula $\boldsymbol{f}_{0}.$
\end_inset

 For this problem, we will also replace the non-normalized Laplacian 
\begin_inset Formula $L$
\end_inset

 with the normalized Laplacian 
\begin_inset Formula $\hat{\boldsymbol{L}}$
\end_inset

, which will become useful when deriving the steady state solution below.
 Thus our objective function now has the form
\begin_inset Formula 
\begin{equation}
C\left(\boldsymbol{f}_{t+1}\Bigl|\boldsymbol{f}_{t}\right)=\left(\boldsymbol{f}_{t+1}-\boldsymbol{f}_{t}\right)^{T}\left(\boldsymbol{f}_{t+1}-\boldsymbol{f}_{t}\right)+\boldsymbol{f}_{t+1}^{T}\left(\boldsymbol{I}-\alpha\hat{\boldsymbol{L}}\right)\boldsymbol{f}_{t+1}+\beta\boldsymbol{f}_{t+1}^{T}\boldsymbol{f}_{0}.
\end{equation}

\end_inset

Straightforward calculus and linear algebra give the solution
\begin_inset Formula 
\begin{equation}
\boldsymbol{f}_{t+1}=\alpha\hat{\boldsymbol{L}}\boldsymbol{f}_{t}+\beta\boldsymbol{f}_{0}.
\end{equation}

\end_inset

The update equation derived hence agrees with that of chapter 11 of 
\begin_inset CommandInset citation
LatexCommand citet
key "BengioDelalleauRoux2006"

\end_inset

.
 Following 
\begin_inset CommandInset citation
LatexCommand citet
key "BengioDelalleauRoux2006"

\end_inset

, this recurrence relation can equivalently write the entire recurrence
 relation in terms of 
\begin_inset Formula $\hat{\boldsymbol{Y}}^{0}$
\end_inset


\begin_inset Formula 
\begin{equation}
\boldsymbol{f}_{t+1}=\left(\alpha\hat{\boldsymbol{L}}\right)^{t}\boldsymbol{f}_{0}+\beta\sum_{i=0}^{t}\left(\alpha\hat{\boldsymbol{L}}\right)^{i}\boldsymbol{f}_{0}.
\end{equation}

\end_inset

Please note that in order to write this equation in absolute form, reducing
 the number of computations, we need to pay for this by having to store
 powers of the matrix 
\begin_inset Formula $\boldsymbol{L}$
\end_inset

.
 It can be easily shown that the normalized Laplacian has an operator norm
 less than unity, 
\begin_inset Formula $\left\Vert \hat{\boldsymbol{L}}\right\Vert <1$
\end_inset

.
 Therefore, the power series 
\begin_inset Formula $\sum_{i=0}^{t}\left(\alpha\hat{\boldsymbol{L}}\right)^{i}$
\end_inset

 converges, and is equal to 
\begin_inset Formula $\left(\boldsymbol{I}-\alpha\hat{\boldsymbol{L}}\right)^{-1}$
\end_inset

.
 In addition, this implies that 
\begin_inset Formula $\lim_{t\to\infty}\left(\alpha\hat{\boldsymbol{L}}\right)^{t}=0$
\end_inset

.
 Thus 
\begin_inset Formula 
\begin{equation}
\boldsymbol{f}_{\infty}=\beta\left(\boldsymbol{I}-\alpha\hat{\boldsymbol{L}}\right)^{-1}.\boldsymbol{f}_{0}.
\end{equation}

\end_inset

We will see in the sequel that the inverse or generalized inverse of the
 Laplacian and closely related matrices play a key role for online learning
 over graphs.
 The above is a linear system of equations which can be solved iteratively,
 as explained in section 2.
\end_layout

\begin_layout Subsection
Regression on Graphs
\end_layout

\begin_layout Standard
Now that we have introduced the basic building blocks for graphical learning,
 we can begin to analyse graphical learning algorithms that are used in
 practice.
 Like in the previous sections, we begin by defining some cost function
 for our function 
\begin_inset Formula $f$
\end_inset

, and then we proceed to derive an update equation that minimizes that cost
 function.
 The sequel deals with algorithms that are actually used by machine learning
 practitioners.
 In this setting, we are given some set of labels some set of labels 
\begin_inset Formula $Y$
\end_inset

 defined over the 
\begin_inset Formula $V$
\end_inset

, and we are asked to make predictions of those labels, 
\begin_inset Formula $\hat{\boldsymbol{Y}}$
\end_inset

.
 
\end_layout

\begin_layout Standard
Given a labelling 
\begin_inset Formula $\boldsymbol{Y}$
\end_inset

, we wish to find a set of predicted labels 
\begin_inset Formula $\hat{\boldsymbol{Y}}$
\end_inset

 that closely approximate the true labels, with some additional constraints
 that are familiar from the previous section.
 consistency with the initial labelling can be measured by
\begin_inset Formula 
\begin{equation}
\sum_{i=1}^{l}\left(\hat{y}_{i}-y_{i}\right)^{2}=\left\Vert \hat{\boldsymbol{Y}}_{l}-\boldsymbol{Y}_{l}\right\Vert ^{2}.\label{eq:1.37}
\end{equation}

\end_inset

By following the smoothness assumption, if two points 
\begin_inset Formula $x_{1}$
\end_inset

 and 
\begin_inset Formula $x_{2}$
\end_inset

 in a high-density region are close, then so should be the corresponding
 outputs 
\begin_inset Formula $y_{1}$
\end_inset

 and 
\begin_inset Formula $y_{2}$
\end_inset

, we consider a penalty term of the form
\begin_inset Formula 
\begin{equation}
\frac{1}{2}\sum_{i,j=1}^{n}\boldsymbol{W}_{ij}\left(\hat{y}_{i}-\hat{y}_{j}\right)^{2}.\label{eq:1.38}
\end{equation}

\end_inset

This means we penalize rapid changes in 
\begin_inset Formula $\hat{\boldsymbol{Y}}$
\end_inset

 between points that are close, which is given by the similarity matrix
 
\begin_inset Formula $\boldsymbol{W}$
\end_inset

.
 However, if there is noise in the available labels, it may be beneficial
 to allow the algorithm to relabel the labelled data.
 This could also help generalization in a noise-free setting where, for
 instance, a positive sample had been drawn from a region of space mainly
 filled with negative samples.
 
\end_layout

\begin_layout Standard
Based on this observation, 
\begin_inset CommandInset citation
LatexCommand citet
key "BelkinMatveevaNiyogi2004"

\end_inset

 proposed a more general cost criterion involving a trade-off between formula
 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:1.37"

\end_inset

 and formula 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:1.38"

\end_inset

, which is shown in algorithm 
\begin_inset CommandInset ref
LatexCommand ref
reference "alg:1.1"

\end_inset

 and algorithm 
\begin_inset CommandInset ref
LatexCommand ref
reference "alg:1.2"

\end_inset

.
 The paper assumed that 
\begin_inset Formula $G\left(V,\,E,\,W\right)$
\end_inset

 is connected and that the vertices of the graph are numbered, and only
 partial information, 
\begin_inset Formula $f\left(\boldsymbol{x}_{i}\right)=y_{i},\:1\leq i\leq k$
\end_inset

, is given.
 The labels can potentially be noisy.
 They also allow data points to have multiplicities, i.e.
 each vertex of the graph may appear more than once with same or different
 
\begin_inset Formula $y$
\end_inset

 value.
 They precondition the data by mean subtracting first.
 That is we take
\begin_inset Formula 
\begin{equation}
\tilde{\boldsymbol{y}}=\left(y_{1}-\bar{y},\,y_{2}-\bar{y},\,\cdots,\,y_{k}-\bar{y}\right),
\end{equation}

\end_inset

where
\begin_inset Formula $\bar{y}=\frac{1}{k}\sum y_{i}$
\end_inset

.
 
\end_layout

\begin_layout Standard
In the algorithm 
\begin_inset CommandInset ref
LatexCommand ref
reference "alg:1.1"

\end_inset

, 
\begin_inset Formula $\boldsymbol{S}$
\end_inset

 is a smoothness matrix, e.g.
 
\begin_inset Formula $S=L$
\end_inset

 or 
\begin_inset Formula $S=L^{p}$
\end_inset

, 
\begin_inset Formula $p\in\mathbb{N}$
\end_inset

.
 The condition 
\begin_inset Formula $\sum f_{i}=0$
\end_inset

 is needed to make the algorithm stable.
 The solution to the quadratic problem above is not hard to obtain by standard
 linear algebra considerations.
 We have the objective function 
\begin_inset Formula $\frac{1}{k}\sum_{i}\left(f_{i}-\tilde{y}_{i}\right)^{2}+\gamma\boldsymbol{f}^{T}\boldsymbol{S}\boldsymbol{f}$
\end_inset

 and constraint 
\begin_inset Formula $\sum f_{i}-0$
\end_inset

.
 Therefore, the Lagrangian is 
\begin_inset Formula 
\begin{equation}
\begin{aligned}\mathcal{L} & =\frac{1}{k}\sum_{i}\left(f_{i}-\tilde{y}_{i}\right)^{2}+\gamma\boldsymbol{f}^{T}\boldsymbol{S}\boldsymbol{f}-\mu\left(\sum f_{i}-0\right)\\
 & =\frac{1}{k}\boldsymbol{f}^{T}\boldsymbol{f}-\frac{2}{k}\boldsymbol{f}^{T}\tilde{\boldsymbol{y}}+\frac{1}{k}\tilde{\boldsymbol{y}}^{T}\tilde{\boldsymbol{y}}+\gamma\boldsymbol{f}^{T}\boldsymbol{S}\boldsymbol{f}-\mu\boldsymbol{f}^{T}\boldsymbol{1}\\
 & =\boldsymbol{f}^{T}\left(\frac{1}{k}\boldsymbol{I}+\gamma\boldsymbol{S}\right)\boldsymbol{f}-\frac{2}{k}\boldsymbol{f}^{T}\left(\tilde{\boldsymbol{y}}+\mu\boldsymbol{1}\right)+\frac{1}{k}\tilde{\boldsymbol{y}}^{T}\tilde{\boldsymbol{y}}.
\end{aligned}
\end{equation}

\end_inset

Then by taking 
\begin_inset Formula $\frac{\partial\mathcal{L}}{\partial\boldsymbol{f}}=0$
\end_inset

, we have 
\begin_inset Formula 
\begin{equation}
\tilde{\boldsymbol{f}}=\left(\boldsymbol{I}+k\gamma\boldsymbol{S}\right)^{-1}\left(\tilde{\boldsymbol{y}}+\mu\boldsymbol{1}\right).
\end{equation}

\end_inset

Here, 
\begin_inset Formula $\mu$
\end_inset

 is chosen so that the resulting vector 
\begin_inset Formula $\boldsymbol{f}$
\end_inset

 is orthogonal to 
\begin_inset Formula $\boldsymbol{1}$
\end_inset

.
 Denote by 
\begin_inset Formula $s\left(\boldsymbol{f}\right)$
\end_inset

 the functional
\begin_inset Formula 
\begin{equation}
s\,:\,\boldsymbol{f}\rightarrow\sum_{i}f_{i}.
\end{equation}

\end_inset

Since 
\begin_inset Formula $s$
\end_inset

 is linear, we obtain
\begin_inset Formula 
\begin{equation}
0=s\left(\tilde{\boldsymbol{f}}\right)=s\left(\left(\boldsymbol{I}+k\gamma\boldsymbol{S}\right)^{-1}\tilde{\boldsymbol{y}}\right)+s\left(\left(\boldsymbol{I}+k\gamma\boldsymbol{S}\right)^{-1}\boldsymbol{1}\right).
\end{equation}

\end_inset

Therefore we can write
\begin_inset Formula 
\begin{equation}
\mu=-\frac{s\left(\left(\boldsymbol{I}+k\gamma\boldsymbol{S}\right)^{-1}\tilde{\boldsymbol{y}}\right)}{s\left(\left(\boldsymbol{I}+k\gamma\boldsymbol{S}\right)^{-1}\boldsymbol{1}\right)}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float algorithm
wide false
sideways false
status open

\begin_layout Plain Layout
The objective is to minimize the square loss function plus the smoothness
 penalty.
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\tilde{\boldsymbol{f}}=\argmin{}_{\substack{\boldsymbol{f}=\left(f_{1},\,\cdots,\,f_{n}\right)\\
\sum f_{i}=0
}
}\frac{1}{k}\sum_{i}\left(f_{i}-\tilde{y}_{i}\right)^{2}+\gamma\boldsymbol{f}^{T}\boldsymbol{S}\boldsymbol{f}$
\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Tickhonov Regularization with Parameter 
\begin_inset Formula $\gamma\in\mathbb{R}$
\end_inset


\begin_inset CommandInset label
LatexCommand label
name "alg:1.1"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset

 
\end_layout

\begin_layout Standard
In the algorithm 
\begin_inset CommandInset ref
LatexCommand ref
reference "alg:1.2"

\end_inset

, 
\begin_inset Formula $\boldsymbol{S}$
\end_inset

 is a smoothness matrix, e.g.
 
\begin_inset Formula $S=L$
\end_inset

 or 
\begin_inset Formula $S=L^{p}$
\end_inset

, 
\begin_inset Formula $p\in\mathbb{N}$
\end_inset

.
 However, here we are not allowing multiple vertices in the sample.
 We partition 
\begin_inset Formula $\boldsymbol{S}$
\end_inset

 as 
\begin_inset Formula 
\begin{equation}
\begin{aligned}\boldsymbol{S} & =\left[\begin{array}{cc}
\boldsymbol{S}_{1} & \boldsymbol{S}_{2}\\
\boldsymbol{S}_{2}^{T} & \boldsymbol{S}_{3}
\end{array}\right]\end{aligned}
,
\end{equation}

\end_inset

where 
\begin_inset Formula $\boldsymbol{S}_{1}$
\end_inset

 is a 
\begin_inset Formula $k\times k$
\end_inset

 matrix, 
\begin_inset Formula $\boldsymbol{S}_{2}$
\end_inset

 is a 
\begin_inset Formula $k\times\left(n-k\right)$
\end_inset

 matrix, and 
\begin_inset Formula $\boldsymbol{S}_{3}$
\end_inset

 is a 
\begin_inset Formula $\left(n-k\right)\times\left(n-k\right)$
\end_inset

 matrix.
 Let 
\begin_inset Formula $\tilde{\boldsymbol{f}}$
\end_inset

 be the values of 
\begin_inset Formula $\boldsymbol{f}$
\end_inset

, where the function is unknown, 
\begin_inset Formula $\tilde{\boldsymbol{f}}=\left(f_{k+1},\,\cdots,\,f_{n}\right)$
\end_inset

.
 By applying the similar Lagrangian multiplier method, we have 
\begin_inset Formula 
\begin{equation}
\tilde{\boldsymbol{f}}=\boldsymbol{S}_{3}^{-1}\boldsymbol{S}_{2}^{T}\left(\left(\tilde{y}_{1},\,\cdots,\,\tilde{y}_{k}\right)^{T}+\mu\boldsymbol{1}\right),
\end{equation}

\end_inset

and 
\begin_inset Formula 
\begin{equation}
\mu=-\frac{s\left(\boldsymbol{S}_{3}^{-1}\boldsymbol{S}_{2}^{T}\tilde{\boldsymbol{y}}\right)}{s\left(\boldsymbol{S}_{3}^{-1}\boldsymbol{S}_{2}^{T}\boldsymbol{1}\right)}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float algorithm
wide false
sideways false
status open

\begin_layout Plain Layout
Here we assume that the values 
\begin_inset Formula $y_{1},\,y_{2},\,\cdots,\,y_{k}$
\end_inset

 have no noise.
\end_layout

\begin_layout Plain Layout
Thus the optimization problem is to find a function of maximum smoothness
 satisfying 
\begin_inset Formula $f\left(\boldsymbol{x}_{i}\right)=\tilde{y}_{i},\:1\leq i\leq k$
\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\tilde{\boldsymbol{f}}=\argmin{}_{\substack{\boldsymbol{f}=\left(\tilde{y}_{1},\,\tilde{y}_{2},\,\cdots,\,\tilde{y_{k}},\,f_{k+1},\,\cdots,\,f_{n}\right)\\
\sum f_{i}=0
}
}\boldsymbol{f}^{T}\boldsymbol{S}\boldsymbol{f}$
\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Interpolated Regularization without Parameter
\begin_inset CommandInset label
LatexCommand label
name "alg:1.2"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
We now investigate generalization bounds for this graph semi-supervised
 learning regularization through algorithmic stability.
 
\begin_inset CommandInset citation
LatexCommand citet
key "BelkinMatveevaNiyogi2004"

\end_inset

 defined the the empirical error 
\begin_inset Formula $R_{k}\left(f\right)$
\end_inset

, which is a measure of how well we do on the training set, and the generalizati
on error 
\begin_inset Formula $R\left(f\right)$
\end_inset

, which is the expectation of how well we do on all labelled or unlabelled
 points, in the following way
\begin_inset Formula 
\begin{equation}
\begin{aligned}R_{k}\left(f\right) & =\frac{1}{k}\sum_{1}^{k}\left(f\left(\boldsymbol{x}_{i}\right)-y_{i}\right)^{2}\\
R\left(f\right) & =\mathbb{E}_{\mu}\left[\left(f\left(\boldsymbol{x}\right)-y\left(\boldsymbol{x}\right)\right)^{2}\right],
\end{aligned}
\end{equation}

\end_inset

where 
\begin_inset Formula $f_{T}$
\end_inset

 maps a given set of examples 
\begin_inset Formula $T$
\end_inset

 to 
\begin_inset Formula $\mathbb{R}$
\end_inset

, i.e.
 
\begin_inset Formula $f_{T}\,:\,V\rightarrow\mathbb{R}$
\end_inset

, the expectation is taken over an underlying distribution 
\begin_inset Formula $\mu$
\end_inset

.
 In theorem 5, they showed that for data samples of size 
\begin_inset Formula $k\geq4$
\end_inset

 with multiplicity of at most 
\begin_inset Formula $t$
\end_inset

, 
\begin_inset Formula $\gamma$
\end_inset

-regularization using the smoothness functional 
\begin_inset Formula $S$
\end_inset

 is a 
\begin_inset Formula $\left(\frac{3M\sqrt{tk}}{\left(k\gamma\lambda_{1}-t\right)^{2}}+\frac{4M}{k\gamma\lambda_{1}-t}\right)$
\end_inset

-stable algorithm, assuming that the denominator 
\begin_inset Formula $k\gamma\lambda_{1}-t$
\end_inset

 is positive.
 
\begin_inset CommandInset citation
LatexCommand citet
key "BousquetElisseeff2001"

\end_inset

 showed that for a 
\begin_inset Formula $\beta$
\end_inset

-stable algorithm 
\begin_inset Formula $T\rightarrow f_{T}$
\end_inset

 we have
\begin_inset Formula 
\begin{equation}
\mathbb{P}\left(\left|R_{k}\left(f_{T}\right)-R\left(f_{T}\right)\right|>\epsilon+\beta\right)\leq2\exp\left(-\frac{k\epsilon^{2}}{2\left(k\beta+K+M\right)^{2}}\right),\qquad\forall\epsilon>0.
\end{equation}

\end_inset

Therefore, by following the derivation in the theorem 11.1 in 
\begin_inset CommandInset citation
LatexCommand citet
key "MohriRosTalw2012"

\end_inset

, we have with probability 
\begin_inset Formula $1-\delta$
\end_inset

 
\end_layout

\begin_layout Standard
*****need to summarize argument****
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\begin{aligned}\left|R_{k}\left(f_{T}\right)-R\left(f_{T}\right)\right| & <\epsilon+\beta\end{aligned}
\label{eq:2.14}
\end{equation}

\end_inset

where 
\begin_inset Formula $\delta=2\exp\left(-\frac{k\epsilon^{2}}{2\left(k\beta+K+M\right)^{2}}\right)$
\end_inset

.
 We then solve for 
\begin_inset Formula $\epsilon$
\end_inset

 in this expression for 
\begin_inset Formula $\delta$
\end_inset

 and get 
\begin_inset Formula 
\begin{equation}
\epsilon=\sqrt{\frac{2\ln\frac{2}{\delta}}{k}}\left(k\beta+K+M\right),
\end{equation}

\end_inset

then plug into inequality 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:2.14"

\end_inset

 and rearrange terms, then, with probability 
\begin_inset Formula $1-\delta$
\end_inset

, we have
\begin_inset Formula 
\begin{equation}
\left|R_{k}\left(f_{T}\right)-R\left(f_{T}\right)\right|\leq\beta+\sqrt{\frac{2\ln\frac{2}{\delta}}{k}}\left(k\beta+K+M\right),
\end{equation}

\end_inset

where 
\begin_inset Formula 
\begin{equation}
\beta=\left(\frac{3M\sqrt{tk}}{\left(k\gamma\lambda_{1}-t\right)^{2}}+\frac{4M}{k\gamma\lambda_{1}-t}\right).
\end{equation}

\end_inset

This is the theorem 1 of 
\begin_inset CommandInset citation
LatexCommand citet
key "BelkinMatveevaNiyogi2004"

\end_inset

.
\end_layout

\begin_layout Standard
****discuss learning bounds************
\end_layout

\begin_layout Section
Semi-Supervised Learning and Regularization over Graphs
\end_layout

\begin_layout Standard
\begin_inset Formula $\boldsymbol{Semi-supervised}$
\end_inset

 
\begin_inset Formula $\boldsymbol{learning}$
\end_inset

 is halfway between supervised and unsupervised learning.
 In addition to unlabelled data, the algorithm is provided with some supervision
 information, but not necessarily for all examples.
 Often, this information will be the targets associated with some of the
 examples.
 In this case, the data set 
\begin_inset Formula $\boldsymbol{X}=\left\{ x_{1},\,x_{2},\,\cdots,\,x_{n}\right\} $
\end_inset

 can be divided into two parts: the points 
\begin_inset Formula $\boldsymbol{X}_{l}=\left\{ x_{1},\,x_{2},\,\cdots,\,x_{l}\right\} $
\end_inset

, for which labels 
\begin_inset Formula $\boldsymbol{Y}_{l}=\left(y_{1},\,y_{2},\,\cdots,\,y_{l}\right)$
\end_inset

 are provided, and the points 
\begin_inset Formula $\boldsymbol{X}_{u}=\left\{ x_{l+1},\,x_{l+2},\,\cdots,\,x_{l+u}\right\} $
\end_inset

, the labels of which are not known.
\end_layout

\begin_layout Standard
Building upon the previous ideas introduced in this section, we now describe
 a setting for semi-supervised learning over a weighted graph.
 We start with vertices 
\begin_inset Formula $1,\,2,\,\cdots,\,l$
\end_inset

 labelled 
\begin_inset Formula $l$
\end_inset

 with their known label
\begin_inset Formula $1$
\end_inset

 or 
\begin_inset Formula $−1$
\end_inset

 and nodes 
\begin_inset Formula $l+1,\,\cdots,\,n$
\end_inset

 labelled with 
\begin_inset Formula $0$
\end_inset

.
 Estimated labels on both labelled and unlabelled data are denoted by 
\begin_inset Formula $\hat{\boldsymbol{Y}}=\left(\hat{\boldsymbol{Y}}_{l},\,\hat{\boldsymbol{Y}}_{u}\right)$
\end_inset

, where where 
\begin_inset Formula $\hat{\boldsymbol{Y}}_{l}$
\end_inset

 may be allowed to differ from the given labels 
\begin_inset Formula $\boldsymbol{Y}_{l}=\left(y_{1},\,y_{2},\,\cdots,\,y_{l}\right)$
\end_inset

.
 At each step a vertex 
\begin_inset Formula $i$
\end_inset

 receives a contribution from its neighbours 
\begin_inset Formula $j$
\end_inset

 and an additional small contribution given by its initial value.
 In addition, we wish to take into consideration that the correct labels
 
\begin_inset Formula $\boldsymbol{Y}_{l}$
\end_inset

 are known for some subset of vertices.
 
\end_layout

\begin_layout Standard
The new semi-supervised learning objective – that we wish to minimize the
 difference between predicted labels and those that are known in the labelled
 set – can be reflected in terms of the cost function 
\begin_inset Formula $||S\hat{Y_{v}}-SY_{v}||_{2}^{2}$
\end_inset

, where 
\begin_inset Formula $S=I_{[l]}$
\end_inset

 is a selection matrix that is equal to the identity for the first 
\begin_inset Formula $l$
\end_inset

 rows and columns, and equal to 0 everywhere else.
 In addition, since we no longer have access to the true labels for every
 vertex, we add a regularization term, so that we can deal with degenerate
 situations, such as connected components of the graph that have no labelled
 vertices.
 Thus we seek to minimize the objective function 
\begin_inset CommandInset citation
LatexCommand citet
key "ZhouBousquetLalWS2004"

\end_inset

 
\begin_inset Formula 
\begin{equation}
C(\hat{Y)}=||S\hat{Y}-SY||_{2}^{2}+\mu\hat{Y}L_{w}\hat{Y}+\epsilon\mu||\hat{Y}||_{2}^{2}.
\end{equation}

\end_inset

We can use straightforward calculus techniques to minimize this expression
\begin_inset Formula 
\begin{equation}
\frac{\partial C(\hat{Y})}{\partial\hat{Y}}=2S(\hat{Y}-Y)+2\mu L\hat{Y}+2\epsilon\mu I\hat{Y}=0.
\end{equation}

\end_inset


\begin_inset CommandInset citation
LatexCommand citet
key "BengioDelalleauRoux2006"

\end_inset

 proposed a solution to this system of linear equations based on the Jacobi
 iterative method for linear systems.
 The Jacobi iteration algorithm gives a solution to the linear system 
\begin_inset Formula 
\begin{equation}
Mx=b.
\end{equation}

\end_inset

By expanding the system as 
\begin_inset Formula $\sum_{j=1}^{n}m_{ij}x_{j}=b_{j}.$
\end_inset

 Suppose we wish to solve for 
\begin_inset Formula $x_{j}$
\end_inset

 while assuming that all other values of x remain fixed.
 We decompose the sum as
\begin_inset Formula 
\begin{equation}
\sum_{j=1}^{n}m_{ij}x_{j}=m_{ij}x_{j}+\sum_{k\neq j}^{n}m_{ij}x_{j}=b_{i}\to x_{j}=b_{i}-\frac{\sum_{k\neq j}^{n}m_{ij}x_{j}}{m_{ij}}.
\end{equation}

\end_inset

We can write this in matrix form as 
\begin_inset Formula 
\begin{equation}
x^{(k)}=D^{-1}Rx^{k-1}+D^{-1}b,
\end{equation}

\end_inset

where D is the diagonal component of M, and R is the remainder, (i.e.
 D+R=M).
 In our case, 
\begin_inset Formula $M=S+\mu L+\epsilon\mu I$
\end_inset

, and 
\begin_inset Formula $b=SY$
\end_inset

.
 The Jacobi method is particularly well suited for the graphical setting,
 since 
\begin_inset Formula $M=S+\mu D+\epsilon\mu I$
\end_inset

, and 
\begin_inset Formula $R=\mu W.$
\end_inset

 A sufficient condition for the convergence of the Jacobi method is when
 the matrix M is strictly diangonally dominany, meaning that the absolute
 value of each diagonal entry is greater than any other entry in the same
 row or column.
 Since the diagonal entries of the Laplacian are by definition positive
 and at least as great as any entry in the same row or column, adding the
 positive diagonal matrix 
\begin_inset Formula $S+\epsilon\mu I$
\end_inset

 to 
\begin_inset Formula $\mu L$
\end_inset

 makes 
\begin_inset Formula $M=S+\mu L+\epsilon\mu I$
\end_inset

 strictly diagonally dominant, as desired.
\end_layout

\begin_layout Standard
\begin_inset Float algorithm
wide false
sideways false
status open

\begin_layout Plain Layout
Compute weight matrix 
\begin_inset Formula $\boldsymbol{W}$
\end_inset

 from formula 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:1.5"

\end_inset

 such that 
\begin_inset Formula $\boldsymbol{W}_{ii}=0$
\end_inset


\end_layout

\begin_layout Plain Layout
Compute the diagonal degree matrix 
\begin_inset Formula $\boldsymbol{D}$
\end_inset

 by 
\begin_inset Formula $\boldsymbol{D}_{ii}=\sum_{j}\boldsymbol{W}_{ij}$
\end_inset

 
\end_layout

\begin_layout Plain Layout
Choose a parameter 
\begin_inset Formula $\alpha\in\left(0,\,1\right)$
\end_inset

 and a small 
\begin_inset Formula $\epsilon>0$
\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\mu=\frac{\alpha}{1-\alpha}\in\left(0,\,+\infty\right)$
\end_inset


\end_layout

\begin_layout Plain Layout
Compute the diagonal matrix 
\begin_inset Formula $\boldsymbol{A}$
\end_inset

 by 
\begin_inset Formula $\boldsymbol{A}_{ii}=\boldsymbol{I}_{l}\left(i\right)+\mu\boldsymbol{D}_{ii}+\mu\epsilon$
\end_inset


\end_layout

\begin_layout Plain Layout
Initialize 
\begin_inset Formula $\hat{\boldsymbol{Y}}^{\left(0\right)}=\left(y_{1},\,y_{2},\,\cdots,\,y_{l},\,0,\,0,\,\cdots,\,0\right)$
\end_inset


\end_layout

\begin_layout Plain Layout
Iterate
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\qquad$
\end_inset


\begin_inset Formula $\hat{\boldsymbol{Y}}^{\left(t+1\right)}=\boldsymbol{A}^{-1}\left(\mu\boldsymbol{W}\hat{\boldsymbol{Y}}^{\left(t\right)}+\hat{\boldsymbol{Y}}^{\left(0\right)}\right)$
\end_inset


\end_layout

\begin_layout Plain Layout
until convergence to 
\begin_inset Formula $\hat{\boldsymbol{Y}}^{\left(\infty\right)}$
\end_inset


\end_layout

\begin_layout Plain Layout
Label point 
\begin_inset Formula $v_{i}$
\end_inset

 by the sign of 
\begin_inset Formula $\hat{y}_{i}^{\left(\infty\right)}$
\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Jacobi Iterative Label Propagation Algorithm
\begin_inset CommandInset label
LatexCommand label
name "alg:1.3"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Section
Projection Algorithm for Graph Online Learning
\end_layout

\begin_layout Standard
A different approach was proposed by 
\begin_inset CommandInset citation
LatexCommand citet
key "HerbsterPontilWainer2005"

\end_inset

 using projection and and the concept of Reprducing Kernel Hilbert Spaces.
\end_layout

\begin_layout Subsection
Pseudo-Inverse of Graph Laplacian and Kernel of Hilbert
\end_layout

\begin_layout Standard
A key object in this approach is the pseudo-inverse of the graph Laplacian
 
\begin_inset Formula $\boldsymbol{L}^{+}$
\end_inset

 can be thought of as the reproducing kernel of a particular Hilbert space
 
\begin_inset Formula $H$
\end_inset

 of real-valued functions over the vertices of the graph
\begin_inset Formula 
\begin{equation}
\boldsymbol{f}\,:V\rightarrow\mathbb{R}^{n}
\end{equation}

\end_inset

equipped with the inner product
\begin_inset Formula 
\begin{equation}
\left\langle \boldsymbol{f},\,\boldsymbol{g}\right\rangle =\boldsymbol{f}^{T}\boldsymbol{L}\boldsymbol{g}.
\end{equation}

\end_inset

Over the entire Vector space of real-valued functions on the graph, 
\begin_inset Formula $\left\langle \boldsymbol{f},\,\boldsymbol{g}\right\rangle $
\end_inset

 is only a semi-inner product, and therefore induces the semi-norm 
\begin_inset Formula $\left\Vert \boldsymbol{g}\right\Vert $
\end_inset

.
 If 
\begin_inset Formula $\boldsymbol{f}^{*}$
\end_inset

 is an eigenvector of 
\begin_inset Formula $\boldsymbol{L}$
\end_inset

 corresponding to an eigenvalue of 
\begin_inset Formula $0$
\end_inset

, then 
\begin_inset Formula $\left\langle \boldsymbol{f}^{*},\,\boldsymbol{f}^{*}\right\rangle =0$
\end_inset

.
 These eigenvectors with eigenvalues of zero are precisely the eigenvectors
 that are piecewise constant.
 We can see this by noting that 
\begin_inset Formula 
\begin{equation}
\begin{aligned}\left\Vert \boldsymbol{g}\right\Vert ^{2} & =\sum_{\left(i,\,j\right)\in E\left(G\right)}(\boldsymbol{g}_{i}-\boldsymbol{g}_{j})^{2}\end{aligned}
.
\end{equation}

\end_inset

This is because 
\begin_inset Formula 
\begin{equation}
\begin{aligned}\left\Vert \boldsymbol{g}\right\Vert ^{2} & =\left\langle \boldsymbol{g},\,\boldsymbol{g}\right\rangle \\
 & =\sum_{i,\,j=1}^{n}\boldsymbol{g}_{i}\boldsymbol{L}_{ij}\boldsymbol{g}_{j}\\
 & =\sum_{i=j}^{n}\boldsymbol{g}_{i}\boldsymbol{L}_{ij}\boldsymbol{g}_{j}+\sum_{i\neq j}^{n}\boldsymbol{g}_{i}\boldsymbol{L}_{ij}\boldsymbol{g}_{j}\\
 & =\sum_{i=1}^{n}\boldsymbol{g}_{i}^{2}\boldsymbol{D}_{ii}+\sum_{i\neq j}^{n}\boldsymbol{g}_{i}\boldsymbol{A}_{ij}\boldsymbol{g}_{j}\\
 & =\sum_{i,\,j=1}^{n}\boldsymbol{g}_{i}{}_{i}^{2}\boldsymbol{A}_{ij}+\sum_{i\neq j}^{n}\boldsymbol{g}_{i}\boldsymbol{L}_{ij}\boldsymbol{g}_{j}\\
 & =\sum_{i,j\in E\left(G\right)}^{n}\boldsymbol{g}_{i}^{2}-2\boldsymbol{g}_{i}\boldsymbol{g}_{j}\\
 & =\sum_{\left(i,\,j\right)\in E\left(G\right)}(\boldsymbol{g}_{i}-\boldsymbol{g}_{j})^{2}.
\end{aligned}
\end{equation}

\end_inset

Please notice that the diagonal component of 
\begin_inset Formula $\boldsymbol{L}$
\end_inset

 is 
\begin_inset Formula $\boldsymbol{D}$
\end_inset

, the off-diagonal component of 
\begin_inset Formula $\boldsymbol{L}$
\end_inset

 is 
\begin_inset Formula $-\boldsymbol{A}$
\end_inset

, 
\begin_inset Formula $\boldsymbol{D}_{ii}=\sum_{j=1}^{n}\boldsymbol{A}_{ij}$
\end_inset

, and in the proof we pick up a factor of 
\begin_inset Formula $2$
\end_inset

 from the second summation since 
\begin_inset Formula $E$
\end_inset

 is the set of unordered pairs in the edge set, and therefore each edge
 gets counted twice.
\end_layout

\begin_layout Standard
As we can see, the sum vanishes if 
\begin_inset Formula $\boldsymbol{g}_{i}=\boldsymbol{g}_{j}$
\end_inset

 for all vertices connected by an edge, i.e.
 when g is piecewise constant on each connected component.
 Therefore we must restrict our functions to be in the subspace of 
\begin_inset Formula $H$
\end_inset

 spanned by the eigenvectors that have non-zero eigenvalues.
 Restricted to this subspace, 
\begin_inset Formula $\left\langle \boldsymbol{f},\,\boldsymbol{g}\right\rangle $
\end_inset

 is indeed an inner product, and therefore induces a true norm 
\begin_inset Formula $\left\Vert \boldsymbol{g}\right\Vert $
\end_inset

.
 Since 
\begin_inset Formula $H$
\end_inset

 is a Hilbert space, we can use the Riesz Representation theorem to conclude
 that the evaluation functional of 
\begin_inset Formula $\boldsymbol{f}$
\end_inset

 at any vertex, 
\begin_inset Formula $\boldsymbol{f}\left(v_{i}\right)$
\end_inset

, being a linear functional 
\begin_inset Formula $\left(\boldsymbol{f}+\boldsymbol{g}\right)\left(v_{i}\right)=\left(\boldsymbol{f}+\boldsymbol{g}\right)_{i}\left(v\right)=\boldsymbol{f}_{i}\left(v\right)+\boldsymbol{g}_{i}\left(v\right)=\boldsymbol{f}\left(v_{i}\right)+\boldsymbol{g}\left(v_{i}\right)$
\end_inset

, can be represented as an inner product 
\begin_inset Formula 
\begin{equation}
\boldsymbol{f}\left(v_{i}\right)=\left\langle \boldsymbol{K}_{i},\,\boldsymbol{f}\right\rangle =\boldsymbol{K}_{i}\boldsymbol{L}\boldsymbol{f}.
\end{equation}

\end_inset

The pseudo-inverse 
\begin_inset Formula $\boldsymbol{L}^{+}$
\end_inset

 satisfies this property, which can be verified by noting that 
\begin_inset Formula 
\begin{equation}
\boldsymbol{f}=\boldsymbol{f}_{i}=\boldsymbol{L}_{i}^{+}\boldsymbol{L}\boldsymbol{f}_{i}.
\end{equation}

\end_inset

Thus 
\begin_inset Formula $\boldsymbol{L}^{+}$
\end_inset

 is the reproducing kernel of 
\begin_inset Formula $H$
\end_inset

.
\end_layout

\begin_layout Subsection
Online learning over graphs
\end_layout

\begin_layout Standard
As shown in the previous section, the reproducing kernel of the Hilbert
 space defined over the graph is given by the pseudoinverse of the Laplacian
 matrix, 
\begin_inset Formula $\boldsymbol{L}^{+}.$
\end_inset

 Ths suggests that we can apply the above projection algorithm, with its
 associated guarantees, by letting the input vector 
\begin_inset Formula $x_{t}$
\end_inset

be replaced by the t-th row of 
\begin_inset Formula $\boldsymbol{L}^{+}.$
\end_inset

 As before, our weight vector 
\begin_inset Formula $\boldsymbol{g}$
\end_inset

 reoresents a real-valued function on our hilbert space.
 But in the graph setting, this Hilbert space and its reproducing kernel
 
\begin_inset Formula $\boldsymbol{L}^{+}$
\end_inset

and the vector 
\begin_inset Formula $\boldsymbol{g}$
\end_inset

 admit a specific interpretation.
\end_layout

\begin_layout Standard
We can get more intuition about the basic objects such as 
\begin_inset Formula $\left\Vert \boldsymbol{g}\right\Vert $
\end_inset

 and 
\begin_inset Formula $\boldsymbol{L}^{+}$
\end_inset

 by using the concept of conductance (it is no coincidence that the continuous
 version of the Laplacian appears in the heat equation).
 If we think of f as some distribution over the nodes, we can think of 
\begin_inset Formula $\boldsymbol{L}\boldsymbol{f}$
\end_inset

 roughly as the flux induced at each of the nodes by that distribution over
 the graph.
 Then the form 
\begin_inset Formula $\boldsymbol{g}^{T}\boldsymbol{L}\boldsymbol{f}$
\end_inset

 represents some weighted measurement of this flux with the weights given
 by 
\begin_inset Formula $\boldsymbol{g}$
\end_inset

.
 For example, if f is a binary vector representing the boundary of some
 open set on the graph 
\begin_inset Formula $\left\langle \boldsymbol{f},\,\boldsymbol{g}\right\rangle $
\end_inset

 would be the flux produced by 
\begin_inset Formula $\boldsymbol{g}$
\end_inset

 as measured on the boundary represented by 
\begin_inset Formula $\boldsymbol{f}$
\end_inset

.
 We can even use this formalism to quickly prove a version of Stokes' Theorem
 on graphs.
\end_layout

\begin_layout Standard
\begin_inset Note Comment
status open

\begin_layout Plain Layout
Let 
\begin_inset Formula $G\left(V,\,E,\,W\right)$
\end_inset

 be a graph imbued with the topology generated by the open neighbourhoods
 
\begin_inset Formula $N_{i}=\left\{ v_{j}\,:\,\left(i,\,j\right)\in E\right\} $
\end_inset

.
 Suppose we have some subset 
\begin_inset Formula $U\left(G\right)$
\end_inset

 of vertices set 
\begin_inset Formula $V\left(G\right)$
\end_inset

, i.e.
 
\begin_inset Formula $U\left(G\right)\subset V\left(G\right)$
\end_inset

.
 Then we can define the boundary of 
\begin_inset Formula $U\left(G\right)$
\end_inset

 as the set of all vertices contained in the closure of 
\begin_inset Formula $U\left(G\right)$
\end_inset

 but not contained in 
\begin_inset Formula $U\left(G\right)$
\end_inset

 itself 
\begin_inset Formula $\partial U\left(G\right)=\left\{ v_{j}\,:\,v_{j}\in\bar{U}\left(G\right)-U\left(G\right)\right\} $
\end_inset

.
 Now suppose our vertices 
\begin_inset Formula $V\left(G\right)$
\end_inset

 are ordered such that 
\begin_inset Formula $v_{1},\,\cdots,\,v_{k}\in U\left(G\right)$
\end_inset

, 
\begin_inset Formula $v_{k+1},\,\cdots,\,v_{l}\in\partial U\left(G\right)$
\end_inset

, and 
\begin_inset Formula $v_{l+1},\,\cdots,\,v_{n}\in V\left(G\right)-\bar{U}\left(G\right)$
\end_inset

.
 Now let 
\begin_inset Formula $\boldsymbol{1}_{U}$
\end_inset

 be a binary vector with all 
\begin_inset Formula $1$
\end_inset

 for the first 
\begin_inset Formula $k$
\end_inset

 entries, and 
\begin_inset Formula $0$
\end_inset

 elsewhere, and let 
\begin_inset Formula $\boldsymbol{1}_{\partial U}$
\end_inset

 be the binary vector with all 
\begin_inset Formula $1$
\end_inset

 for entries 
\begin_inset Formula $k+1$
\end_inset

 through 
\begin_inset Formula $l$
\end_inset

, and 
\begin_inset Formula $0$
\end_inset

 elsewhere.
 Then according to the above interpretation of the norm, 
\begin_inset Formula 
\begin{equation}
\boldsymbol{F}_{U}\coloneqq\boldsymbol{1}_{U}\boldsymbol{L}\boldsymbol{g}
\end{equation}

\end_inset

 is the total flux induced by distribution g over the interior of 
\begin_inset Formula $U\left(G\right)$
\end_inset

, while
\begin_inset Formula 
\begin{equation}
\boldsymbol{F}_{\partial U}\coloneqq\boldsymbol{1}_{\partial U}\boldsymbol{L}\boldsymbol{g}
\end{equation}

\end_inset

is the total flux induced by 
\begin_inset Formula $\boldsymbol{g}$
\end_inset

 as measured over the boundary of 
\begin_inset Formula $U\left(G\right)$
\end_inset

.
 Our claim is that 
\begin_inset Formula 
\begin{equation}
\boldsymbol{F}_{U}=-\boldsymbol{F}_{\partial U}.\label{eq:3.9}
\end{equation}

\end_inset


\end_layout

\begin_layout Plain Layout
We now give an informal proof.
 Let us denote by 
\begin_inset Formula $d_{i}$
\end_inset

 the degree of vertex 
\begin_inset Formula $i$
\end_inset

, 
\begin_inset Formula $d_{i}^{int}$
\end_inset

 by the number of edges connecting vertex 
\begin_inset Formula $i$
\end_inset

 to points in 
\begin_inset Formula $U\left(G\right)$
\end_inset

, and 
\begin_inset Formula $d_{i}^{ext}$
\end_inset

 by the number of edges connecting vertex 
\begin_inset Formula $i$
\end_inset

 to points not in 
\begin_inset Formula $U\left(G\right)$
\end_inset

, so that 
\begin_inset Formula 
\begin{equation}
d_{i}=d_{i}^{int}+d_{i}^{ext}.
\end{equation}

\end_inset

In particular, for any 
\begin_inset Formula $v_{i}\in U$
\end_inset

, 
\begin_inset Formula $d_{i}^{ext}$
\end_inset

 is the number of edges connecting 
\begin_inset Formula $v_{i}$
\end_inset

 to points in 
\begin_inset Formula $\partial U\left(G\right)$
\end_inset

, since by definition all other vertices are not connected to 
\begin_inset Formula $v_{i}$
\end_inset

.
 Then for the left hand side of formula 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:3.9"

\end_inset

, note that the only contribution to the 
\begin_inset Formula $i^{\text{th}}$
\end_inset

 entry is given by the first 
\begin_inset Formula $k$
\end_inset

 columns of 
\begin_inset Formula $\boldsymbol{L}$
\end_inset

, and a straightforward calculation shows that 
\begin_inset Formula 
\begin{equation}
\boldsymbol{1}_{U}\boldsymbol{L}\boldsymbol{g}=\sum_{i\in U}(d_{i}-d_{i}^{int})=\sum_{i\in U}d^{ext}.
\end{equation}

\end_inset

For the right hand side of formula 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:3.9"

\end_inset

, we can see that the only contribution to the sum is from entries 
\begin_inset Formula $k+1$
\end_inset

 to 
\begin_inset Formula $l$
\end_inset

, which another straightforward calculation shows that
\begin_inset Formula 
\begin{equation}
\boldsymbol{1}_{\partial U}\boldsymbol{L}\boldsymbol{g}=\sum_{i\in\partial U}d_{i}^{ext}.
\end{equation}

\end_inset

Thus
\begin_inset Formula 
\begin{equation}
\boldsymbol{1}_{U}\boldsymbol{L}\boldsymbol{g}=-\boldsymbol{1}_{\partial U}\boldsymbol{L}\boldsymbol{g},\qquad\forall\boldsymbol{g}\in H.
\end{equation}

\end_inset

Despite the simplicity of the proof and the fact that there is a very general
 form of Stoke's theorem over smooth manifolds, this graphical version of
 Stokes' theorem has not been seen by the authors, and could potentially
 be leveraged for new graphical algorithms.
 
\end_layout

\end_inset


\end_layout

\begin_layout Standard
To gain intuition about 
\begin_inset Formula $L^{+}$
\end_inset

, we turn back to the representation theorem, namely that 
\begin_inset Formula $\boldsymbol{f}\left(v_{i}\right)=\boldsymbol{L}_{i}^{+}\boldsymbol{L}\boldsymbol{f}$
\end_inset

.
 In the heat analogy, this could be interpreted as saying that the heat
 at each vertex can be expressed in terms of, or derived from, the flux
 through every vertex.
 So in this heat analogy, 
\begin_inset Formula $\boldsymbol{L}$
\end_inset

 sends a heat distribution 
\begin_inset Formula $\boldsymbol{f}$
\end_inset

 over each node to a flux through each vertex.
 Conversely, 
\begin_inset Formula $\boldsymbol{L}_{i}^{+}$
\end_inset

 sends some definition of fluxes over the graph back to some heat distribution
 that would have induced it.
 This relationship is shown in figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:3.1"

\end_inset

.
 
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename 3-1.png
	lyxscale 70
	scale 60

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Bidirectional Mapping Between Heat and Flux
\begin_inset CommandInset label
LatexCommand label
name "fig:3.1"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset

 Intuitively, if the graph is disconnected this distribution should not
 be unique, since a constant added to the distribution in each of the connected
 components will not affect the flux induced through the graph.
 Mathematically, this is affirmed by noting that
\begin_inset Formula 
\begin{equation}
\boldsymbol{L}^{+}=\boldsymbol{L}^{-1}
\end{equation}

\end_inset

 if and only if all eigenvalues of L are non-zero which, is exactly when
 the graph is connected.
\end_layout

\begin_layout Standard
Let 
\begin_inset Formula $g^{+}$
\end_inset

 be the positively labeled vertices, and let 
\begin_inset Formula $g^{-}$
\end_inset

be the negatively labeled vertices.
 Let 
\begin_inset Formula $|g|^{+}=n^{+},|g^{-}|=n^{-},$
\end_inset

 and assume WLOG that 
\begin_inset Formula $n^{+}>=n^{-}.$
\end_inset


\end_layout

\begin_layout Standard
Claim: 
\begin_inset Formula $||U||^{2}\leq\partial(g^{+},g^{-})*(1+\frac{n_{+}}{n_{-}})^{2}$
\end_inset


\end_layout

\begin_layout Standard
Proof.
 To show this, we consider the vector 
\begin_inset Formula $f$
\end_inset

, with 
\begin_inset Formula $f_{i}=1$
\end_inset

 if 
\begin_inset Formula $g_{i}=1$
\end_inset

, and 
\begin_inset Formula $f_{i}=-\frac{n^{+}}{n^{-}}$
\end_inset

 if 
\begin_inset Formula $g_{i}=-1.$
\end_inset

Now suppose we order our vertices so that the first 
\begin_inset Formula $n^{+}$
\end_inset

vertices are those in 
\begin_inset Formula $g^{+},$
\end_inset

 while the last 
\begin_inset Formula $n_{-}$
\end_inset

vertices are those in 
\begin_inset Formula $g^{-}.$
\end_inset

 Now consider the Laplacian 
\begin_inset Formula $L$
\end_inset

 of this matrix with the vertices ordered in such a way.
 We are interested in the sums of the entries in the off-diagonal blocks,
 the sums of whose entries are each the negative of the number of intra-partitio
n edges.
 We will denote this sum by S, as opposed to the sub-matrix 
\series bold
S.
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\begin{aligned}L & =\left[\begin{array}{cc}
\boldsymbol{A} & -\boldsymbol{S}\\
\boldsymbol{-S}^{T} & \boldsymbol{B}
\end{array}\right]\end{aligned}
,
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
Note that the sum of entries in the left partition of the matrix for any
 given row is always the negative of the sum to the right of the partition.
 Using this and the fact that the sum of entries in both 
\series bold
S
\series default
 and 
\begin_inset Formula $\boldsymbol{\boldsymbol{S}^{T}}$
\end_inset

 are both equal to S, we find after some algebra that
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
||f||^{2}=<f,f>=f^{T}Lf=(1+\frac{n^{+}}{n^{-}})^{2}S
\]

\end_inset


\end_layout

\begin_layout Standard
Finally, combined with the bound on the power mean of the input patterns
 in terms of the graph diameter, we acheive a bound on the number of mistakes.
\end_layout

\begin_layout Subsection
Drawbacks with the above approach
\end_layout

\begin_layout Standard
Large diameter gaphs
\end_layout

\begin_layout Standard
With that in mind, we can introduce the mathematical framework for the online
 projection algorithm of 
\begin_inset CommandInset citation
LatexCommand citet
key "HerbsterPontilWainer2005"

\end_inset

.
 Herbster seeks to minimize
\end_layout

\begin_layout Standard
–projection description (tom)
\end_layout

\begin_layout Standard
–learning bounds (Paul)
\end_layout

\begin_layout Standard
Extension of the herbster paper – use different laplacians simulataneously
 by considering them each as different kernels in some set of kernels, and
 then applying the objective function for multiple kernels (need to flesh
 out this idea)
\end_layout

\begin_layout Standard
\begin_inset Newpage clearpage
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
renewcommand
\backslash
refname{Bibliography}
\end_layout

\end_inset


\begin_inset Note Comment
status open

\begin_layout Plain Layout
Leave this inside the comment:
\begin_inset CommandInset bibtex
LatexCommand bibtex
bibfiles "bibliography"
options "plain"

\end_inset


\end_layout

\begin_layout Plain Layout
LyX still provides its citation dialogs, but does not export bibtex commands
 to LaTeX.
 Bibliography database has to be loaded twice: in the document preamble
 for biblatex, and through the above button for LyX support.
\end_layout

\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
nocite{*}
\end_layout

\begin_layout Plain Layout


\backslash
printbibliography
\end_layout

\begin_layout Plain Layout


\backslash
addcontentsline{toc}{chapter}{Bibliography}
\end_layout

\end_inset


\end_layout

\end_body
\end_document
